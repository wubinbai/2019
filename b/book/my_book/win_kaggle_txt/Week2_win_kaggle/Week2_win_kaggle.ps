%!PS-Adobe-3.0
%%Title: Week2_win_kaggle.txt
%%For: wb
%%Creator: VIM - Vi IMproved 8.0 (2016 Sep 12)
%%CreationDate: Mon Dec 23 18:52:00 2019
%%DocumentData: Clean8Bit
%%Orientation: Portrait
%%Pages: (atend)
%%PageOrder: Ascend
%%BoundingBox: 59 45 559 800
%%DocumentMedia: A4 595 842 0 () ()
%%DocumentNeededResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%DocumentSuppliedResources: procset VIM-Prolog 1.4 1
%%+ encoding VIM-latin1 1.0 0
%%Requirements: duplex collate
%%EndComments
%%BeginDefaults
%%PageResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%PageMedia: A4
%%EndDefaults
%%BeginProlog
%%BeginResource: procset VIM-Prolog
%%BeginDocument: /usr/share/vim/vim80/print/prolog.ps
%!PS-Adobe-3.0 Resource-ProcSet
%%Title: VIM-Prolog
%%Version: 1.4 1
%%EndComments
% Editing of this file is NOT RECOMMENDED.  You run a very good risk of causing
% all PostScript printing from VIM failing if you do.  PostScript is not called
% a write-only language for nothing!
/packedarray where not{userdict begin/setpacking/pop load def/currentpacking
false def end}{pop}ifelse/CP currentpacking def true setpacking
/bd{bind def}bind def/ld{load def}bd/ed{exch def}bd/d/def ld
/db{dict begin}bd/cde{currentdict end}bd
/T true d/F false d
/SO null d/sv{/SO save d}bd/re{SO restore}bd
/L2 systemdict/languagelevel 2 copy known{get exec}{pop pop 1}ifelse 2 ge d
/m/moveto ld/s/show ld /ms{m s}bd /g/setgray ld/r/setrgbcolor ld/sp{showpage}bd
/gs/gsave ld/gr/grestore ld/cp/currentpoint ld
/ul{gs UW setlinewidth cp UO add 2 copy newpath m 3 1 roll add exch lineto
stroke gr}bd
/bg{gs r cp BO add 4 -2 roll rectfill gr}bd
/sl{90 rotate 0 exch translate}bd
L2{
/sspd{mark exch{setpagedevice}stopped cleartomark}bd
/nc{1 db/NumCopies ed cde sspd}bd
/sps{3 db/Orientation ed[3 1 roll]/PageSize ed/ImagingBBox null d cde sspd}bd
/dt{2 db/Tumble ed/Duplex ed cde sspd}bd
/c{1 db/Collate ed cde sspd}bd
}{
/nc{/#copies ed}bd
/sps{statusdict/setpage get exec}bd
/dt{statusdict/settumble 2 copy known{get exec}{pop pop pop}ifelse
statusdict/setduplexmode 2 copy known{get exec}{pop pop pop}ifelse}bd
/c{pop}bd
}ifelse
/ffs{findfont exch scalefont d}bd/sf{setfont}bd
/ref{1 db findfont dup maxlength dict/NFD ed{exch dup/FID ne{exch NFD 3 1 roll
put}{pop pop}ifelse}forall/Encoding findresource dup length 256 eq{NFD/Encoding
3 -1 roll put}{pop}ifelse NFD dup/FontType get 3 ne{/CharStrings}{/CharProcs}
ifelse 2 copy known{2 copy get dup maxlength dict copy[/questiondown/space]{2
copy known{2 copy get 2 index/.notdef 3 -1 roll put pop exit}if pop}forall put
}{pop pop}ifelse dup NFD/FontName 3 -1 roll put NFD definefont pop end}bd
CP setpacking
(\004)cvn{}bd
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%BeginResource: encoding VIM-latin1
%%BeginDocument: /usr/share/vim/vim80/print/latin1.ps
%!PS-Adobe-3.0 Resource-Encoding
%%Title: VIM-latin1
%%Version: 1.0 0
%%EndComments
/VIM-latin1[
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclam /quotedbl /numbersign /dollar /percent /ampersand /quotesingle
/parenleft /parenright /asterisk /plus /comma /minus /period /slash
/zero /one /two /three /four /five /six /seven
/eight /nine /colon /semicolon /less /equal /greater /question
/at /A /B /C /D /E /F /G
/H /I /J /K /L /M /N /O
/P /Q /R /S /T /U /V /W
/X /Y /Z /bracketleft /backslash /bracketright /asciicircum /underscore
/grave /a /b /c /d /e /f /g
/h /i /j /k /l /m /n /o
/p /q /r /s /t /u /v /w
/x /y /z /braceleft /bar /braceright /asciitilde /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclamdown /cent /sterling /currency /yen /brokenbar /section
/dieresis /copyright /ordfeminine /guillemotleft /logicalnot /hyphen /registered /macron
/degree /plusminus /twosuperior /threesuperior /acute /mu /paragraph /periodcentered
/cedilla /onesuperior /ordmasculine /guillemotright /onequarter /onehalf /threequarters /questiondown
/Agrave /Aacute /Acircumflex /Atilde /Adieresis /Aring /AE /Ccedilla
/Egrave /Eacute /Ecircumflex /Edieresis /Igrave /Iacute /Icircumflex /Idieresis
/Eth /Ntilde /Ograve /Oacute /Ocircumflex /Otilde /Odieresis /multiply
/Oslash /Ugrave /Uacute /Ucircumflex /Udieresis /Yacute /Thorn /germandbls
/agrave /aacute /acircumflex /atilde /adieresis /aring /ae /ccedilla
/egrave /eacute /ecircumflex /edieresis /igrave /iacute /icircumflex /idieresis
/eth /ntilde /ograve /oacute /ocircumflex /otilde /odieresis /divide
/oslash /ugrave /uacute /ucircumflex /udieresis /yacute /thorn /ydieresis]
/Encoding defineresource pop
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%EndProlog
%%BeginSetup
595 842 0 sps
1 nc
T F dt
T c
%%IncludeResource: font Courier
/_F0 /VIM-latin1 /Courier ref
/F0 13 /_F0 ffs
%%IncludeResource: font Courier-Bold
/_F1 /VIM-latin1 /Courier-Bold ref
/F1 13 /_F1 ffs
%%IncludeResource: font Courier-Oblique
/_F2 /VIM-latin1 /Courier-Oblique ref
/F2 13 /_F2 ffs
%%IncludeResource: font Courier-BoldOblique
/_F3 /VIM-latin1 /Courier-BoldOblique ref
/F3 13 /_F3 ffs
/UO -1.3 d
/UW 0.65 d
/BO -3.25 d
%%EndSetup
%%Page: 1 1
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                      Page 1)59.5 790.15 ms
F0 sf
([MUSIC] Hi, in this lesson we)59.5 764.15 ms
(will talk about the very first steps a good data scientist)59.5 751.15 ms
(takes when he is given a new data set. Mainly, exploratory data )59.5 738.15 ms
(analysis or)59.5 725.15 ms
(EDA in short. By the end of this lesson, you will know,)59.5 712.15 ms
(what are the most important things from data understanding and e)59.5 699.15 ms
(xploration)59.5 686.15 ms
(prospective we need to pay attention to. This knowledge is requi)59.5 673.15 ms
(red)59.5 660.15 ms
(to build good models and achieve high places on the leader board)59.5 647.15 ms
(. We will first discuss what exploratory)59.5 634.15 ms
(data analysis is and why we need it. We will then go through imp)59.5 621.15 ms
(ortant)59.5 608.15 ms
(parts of EDA process and see examples of what we)59.5 595.15 ms
(can discover during EDA. Next we will take a look at the tools)59.5 582.15 ms
(we have to perform exploration. What plots to draw and)59.5 569.15 ms
(what functions from pandas and matplotlib libraries can be usefu)59.5 556.15 ms
(l for us. We will also briefly discuss a very)59.5 543.15 ms
(basic data set cleaning process that is convenient to perform)59.5 530.15 ms
(while exploring the data. And finally we'll go through)59.5 517.15 ms
(exploration process for the Springleaf competition)59.5 504.15 ms
(hosted on Kaggle some time ago. In this video we'll start talkin)59.5 491.15 ms
(g)59.5 478.15 ms
(about Exploratory Data Analysis. What is Exploratory Data Analys)59.5 465.15 ms
(is? It's basically a process of looking)59.5 452.15 ms
(into the data, understanding it and getting comfortable with it.)59.5 439.15 ms
( Getting comfortable with a task,)59.5 426.15 ms
(probably always the first thing you do. To solve a problem,)59.5 413.15 ms
(you need to understand a problem, and to know what you)59.5 400.15 ms
(are given to solve it. In data science, complete data)59.5 387.15 ms
(understanding is required to generate powerful features and)59.5 374.15 ms
(to build accurate models. In fact while you explore the data,)59.5 361.15 ms
(you build an intuition about it. And when the data is intuitive )59.5 348.15 ms
(for you, you can generate hypothesis)59.5 335.15 ms
(about possible new features and eventually find some insights in)59.5 322.15 ms
( the data)59.5 309.15 ms
(which in turn can lead to a better score. We will see the exampl)59.5 296.15 ms
(e of what EDA)59.5 283.15 ms
(can give us later in this lesson. Well, one may argue that)59.5 270.15 ms
(there is another way to go. Read the data from the hard drive,)59.5 257.15 ms
(never look at it and feed the classifier immediately.They use)59.5 244.15 ms
(some pretty advanced modeling techniques, like mixing, stacking,)59.5 231.15 ms
( and eventually get)59.5 218.15 ms
(a pretty good score on the leaderboard. Although this approach s)59.5 205.15 ms
(ometimes works, it cannot take you to the very)59.5 192.15 ms
(top positions and let you win. Top solutions always use advanced)59.5 179.15 ms
( and)59.5 166.15 ms
(aggressive modeling. But usually they have)59.5 153.15 ms
(something more than that. They incorporated insights from the da)59.5 140.15 ms
(ta,)59.5 127.15 ms
(and to find those insights,)59.5 114.15 ms
(they did a careful EDA. While we need to admit the raw)59.5 101.15 ms
(computations where all you can do is modeling and EDA will not h)59.5 88.15 ms
(elp)59.5 75.15 ms
(you to build a better model. It is usually the case when)59.5 62.15 ms
(the data is anonymized, encrypted, pre-processed, and obfuscated)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 2 2
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                      Page 2)59.5 790.15 ms
F0 sf
(. But look it will any way need to perform)59.5 764.15 ms
(EDA to realize that this is the case and you better spend more t)59.5 751.15 ms
(ime on modeling and)59.5 738.15 ms
(make a server busy for a month. One of the main EDA)59.5 725.15 ms
(tools is Visualization. When we visualize the data,)59.5 712.15 ms
(we immediately see the patterns. And with this, ask ourselves,)59.5 699.15 ms
(what are those patterns? Why do we see them? And finally, how do)59.5 686.15 ms
( we use those)59.5 673.15 ms
(patters to build a better model? It also can be another way arou)59.5 660.15 ms
(nd. Maybe we come up with a particular)59.5 647.15 ms
(hypothesis about the data. What do we do? We test it by making a)59.5 634.15 ms
( visualization. In one of the next videos, we'll talk about the )59.5 621.15 ms
(main visualization)59.5 608.15 ms
(tools we can use for exploration. Just as a motivation example,)59.5 595.15 ms
(I want to tell you about the competition, alexander D'yakonov, a)59.5 582.15 ms
( former top one)59.5 569.15 ms
(at Kagel took part some time ago. The interesting thing about th)59.5 556.15 ms
(is)59.5 543.15 ms
(competition is that you do not need to do any modeling,)59.5 530.15 ms
(if you understood your data well. In that competition,)59.5 517.15 ms
(the objective was to predict whether a person will use the promo)59.5 504.15 ms
(that a company offers him. So each role correspond to a particul)59.5 491.15 ms
(ar)59.5 478.15 ms
(promo received by a person. There are features that)59.5 465.15 ms
(describe the person, for example his age, sex,)59.5 452.15 ms
(is he married or not and so on. And there are features that desc)59.5 439.15 ms
(ribe)59.5 426.15 ms
(the promo, the target is 0 or 1, will he use the promo or not. B)59.5 413.15 ms
(ut, among all the features there)59.5 400.15 ms
(were two especially interesting. The first one is, the number of)59.5 387.15 ms
(promos sent by the person before. And the second is the number o)59.5 374.15 ms
(f)59.5 361.15 ms
(promos the person had to use before. So let's take a particular )59.5 348.15 ms
(user,)59.5 335.15 ms
(say with index 13, and sort the rows by number)59.5 322.15 ms
(of promos sent column. And now let's take a look)59.5 309.15 ms
(at the difference at column the number of used promos)59.5 296.15 ms
(between two consecutive rows. It is written here in diff column.)59.5 283.15 ms
( And look, the values in diff column in)59.5 270.15 ms
(most cases equal the target values. And in fact, there is no mag)59.5 257.15 ms
(ic. Just think about)59.5 244.15 ms
(the meaning of the columns. For example, for the second row we s)59.5 231.15 ms
(ee)59.5 218.15 ms
(that the person used one promo already but he was sent only one )59.5 205.15 ms
(before that time. And that is why we know that he used the)59.5 192.15 ms
(first promo and thus we have an answer for the first row. In gen)59.5 179.15 ms
(eral, if before the current)59.5 166.15 ms
(promo the person used n promos and before the next promo he used)59.5 153.15 ms
( that, we know that he used n + 1 promos then we)59.5 140.15 ms
(realize that he used the current promo. And so the answer is 1. )59.5 127.15 ms
(If we know that he used n)59.5 114.15 ms
(promos before the next promo, exactly as before the current prom)59.5 101.15 ms
(o, then obviously he did not use)59.5 88.15 ms
(the current promo and the answer is 0. Well, it's not clear what)59.5 75.15 ms
(to do with the last row for every user, or when we have missing )59.5 62.15 ms
(rows,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 3 3
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                      Page 3)59.5 790.15 ms
F0 sf
(but you see the point. We didn't even run the classifier,)59.5 764.15 ms
(and we have 80% accuracy already. This would not be possible if )59.5 751.15 ms
(we didn't)59.5 738.15 ms
(do an EDA and didn't look into the data. Also as a remark, I sho)59.5 725.15 ms
(uld say)59.5 712.15 ms
(that the presented method works because of mistake made by)59.5 699.15 ms
(the organizers during data preparation. These mistakes are calle)59.5 686.15 ms
(d leaks, and in competitions we are usually)59.5 673.15 ms
(allowed to exploit them. We'll see more of these)59.5 660.15 ms
(examples later in this course. So in this video we discussed the)59.5 647.15 ms
( main)59.5 634.15 ms
(reasons for performing an EDA. That is to get comfortable with t)59.5 621.15 ms
(he data)59.5 608.15 ms
(and to find insights in magic features. We also saw an example w)59.5 595.15 ms
(here EDA and the data understanding was)59.5 582.15 ms
(important to get a better score. And finally, the point to take )59.5 569.15 ms
(away. When you start a competition,)59.5 556.15 ms
(you better start with EDA, and not with hardcore modelling. We'v)59.5 543.15 ms
(e had a lot of things to)59.5 530.15 ms
(talk about in this lesson. So let´s move to the next video. [MUS)59.5 517.15 ms
(IC]In this video, we'll go through and break down several import)59.5 504.15 ms
(ant steps namely, the first, getting domain knowledge step, seco)59.5 491.15 ms
(nd, checking if data is intuitive, and finally, understanding ho)59.5 478.15 ms
(w the data was generated. So let's start with the first step, ge)59.5 465.15 ms
(tting the domain knowledge. If we take a look at the computation)59.5 452.15 ms
(s hosted in the Kaggle, well, you'll notice, they are rather div)59.5 439.15 ms
(erse. Sometimes, we need to detect threats on three dimensional )59.5 426.15 ms
(body scans, or predict real estate price, or classify satellite )59.5 413.15 ms
(images. Computation can be on a very specific topic which we kno)59.5 400.15 ms
(w almost nothing about, that is, we don't have a domain knowledg)59.5 387.15 ms
(e. Usually, we don't need to go too deep inside the field but it)59.5 374.15 ms
('s preferable to understand what our aim is, what data we have, )59.5 361.15 ms
(and how people usually tackle this kind of problems to build a b)59.5 348.15 ms
(aseline. So, our first step should probably be searching for the)59.5 335.15 ms
( topic, Googling within Wikipedia, and making sure we understand)59.5 322.15 ms
( the data. For example, let's say we start a new computation in )59.5 309.15 ms
(which we need to predict advertisers cost. Our first step is to )59.5 296.15 ms
(realize that the computation is about web advertisement. By look)59.5 283.15 ms
(ing and searching for the column names, using any search engine,)59.5 270.15 ms
( we understand that the data was exported from Google AdWords sy)59.5 257.15 ms
(stem. And after reading several articles about Google AdWords, w)59.5 244.15 ms
(e get the meaning of the columns. We now know that impressions c)59.5 231.15 ms
(olumn contained a number of times a particular ad appeared befor)59.5 218.15 ms
(e users, and clicks column is how many times the ad was clicked )59.5 205.15 ms
(by the users, and of course, the number of clicks should be less)59.5 192.15 ms
( or equal than the number of impression. In this video, we'll no)59.5 179.15 ms
(t go much further into the details about this data set, but you )59.5 166.15 ms
(can open the supplementary reading material for a more detailed )59.5 153.15 ms
(exploration. After we've learned some domain knowledge, it is ne)59.5 140.15 ms
(cessary to check if the values in the data set are intuitive, an)59.5 127.15 ms
(d agree with our domain knowledge. For example, if there is a co)59.5 114.15 ms
(lumn with age data, we should expect the values rarely to be lar)59.5 101.15 ms
(ger than 100. And for sure, no one ever lived more than 200 year)59.5 88.15 ms
(s. So, the values should be smaller than 200. But for some reaso)59.5 75.15 ms
(n, we find this super huge value 336. Most probably, is just a t)59.5 62.15 ms
(ypo but it should be 36 or 33, and the best we can do is manuall)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 4 4
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                      Page 4)59.5 790.15 ms
F0 sf
(y correct it. But the other possibility is that its not a human )59.5 764.15 ms
(age, but some alien's age for which it's totally normal to live )59.5 751.15 ms
(more than 300 years. To check that, we should probably read the )59.5 738.15 ms
(data description one more time, ask on forums. Maybe the data is)59.5 725.15 ms
( totally correct, and then we just misinterpret it. Now, take a )59.5 712.15 ms
(look at our Google AdWords data set. We understood that the valu)59.5 699.15 ms
(es in the clicks variable should be less or equal than the value)59.5 686.15 ms
(s in impressions column. And in our case, in the first row, we s)59.5 673.15 ms
(ee zero impressions and three clicker. That sounds like a bug, r)59.5 660.15 ms
(ight? In fact, it probably is, but differently to the example of)59.5 647.15 ms
( person's age, it could be rather a regular error made by either)59.5 634.15 ms
( data exporting script or another kind of algorithm. That is, th)59.5 621.15 ms
(e errors were made not at random, but there is some kind of logi)59.5 608.15 ms
(c why there is an error in that particular place. It means that )59.5 595.15 ms
(these mistakes can be used to get a better score. For example, i)59.5 582.15 ms
(n our case, we could create a new feature, is_incorrect, and mar)59.5 569.15 ms
(k all the rows that have errors. Probably, our models will find )59.5 556.15 ms
(this feature helpful. It is also very important to understand ho)59.5 543.15 ms
(w the data was generated. What was the algorithm for sampling ob)59.5 530.15 ms
(jects from the database? Maybe, the host sample get objects at r)59.5 517.15 ms
(andom, or they over-sample the particular class, that is, they g)59.5 504.15 ms
(enerated more examples of that class. For example, to make the d)59.5 491.15 ms
(ata set more class balanced. In fact, only if you know how the d)59.5 478.15 ms
(ata was generated, you can set up a proper validation scheme for)59.5 465.15 ms
( models. Coming down for the correct validation pipeline is cruc)59.5 452.15 ms
(ial, and we will discuss it later in this course. So, what can w)59.5 439.15 ms
(e possibly find out about generation processes? For example, we )59.5 426.15 ms
(could find out the train and test set were generated with differ)59.5 413.15 ms
(ent algorithms. And if the test set is different to the train se)59.5 400.15 ms
(t, we cannot use part of the train set as a validation set, beca)59.5 387.15 ms
(use this part will not be representative of test set. And so, we)59.5 374.15 ms
( cannot evaluate our models using it. So once again, to set up a)59.5 361.15 ms
( correct validation, we need to know underlying data generation )59.5 348.15 ms
(processes. In the ad computation, we've discussed before, that a)59.5 335.15 ms
(ll the symptoms of different train test sampling. Improving the )59.5 322.15 ms
(model on validation set didn't result into improved public leade)59.5 309.15 ms
(r-board score. And more, the leader-board score was unexpectedly)59.5 296.15 ms
( higher than the validation score. I was also visualizing variou)59.5 283.15 ms
(s things while trying to understand what's happening, and every )59.5 270.15 ms
(time, the plots for the train set were much different to the tes)59.5 257.15 ms
(t set plots. This also could not happen if the train and test se)59.5 244.15 ms
(t were similar. And finally, it was suspicious that although the)59.5 231.15 ms
( train period was more than ten times larger than the test perio)59.5 218.15 ms
(d, the train set had much fewer rows. it was not straight forwar)59.5 205.15 ms
(d, but this triangle on the left figure was the clue for me, and)59.5 192.15 ms
( the puzzle was solved. I've adjusted the train set to match tes)59.5 179.15 ms
(t set. The validation score became reliable, and the modeling co)59.5 166.15 ms
(uld be commenced. You can find the entire task description and i)59.5 153.15 ms
(nvestigation in the written materials. So, in this video, we've )59.5 140.15 ms
(discussed several important exploratory steps. First, we need to)59.5 127.15 ms
( get domain knowledge about the task as it helps to better under)59.5 114.15 ms
(stand the problem and the data. Next, we need to check if the da)59.5 101.15 ms
(ta is intuitive, and agrees with our domain knowledge. And final)59.5 88.15 ms
(ly, it is necessary to understand how the data was generated by )59.5 75.15 ms
(organizers because otherwise, we cannot establish a proper valid)59.5 62.15 ms
(ation for our models.[SOUND] In the previous video,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 5 5
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                      Page 5)59.5 790.15 ms
F0 sf
(we were working with the data for which we had a nice descriptio)59.5 764.15 ms
(n. That is,)59.5 751.15 ms
(we knew what the features were, and the data was given us as the)59.5 738.15 ms
(se)59.5 725.15 ms
(without severe modifications. But, it's not always the case. The)59.5 712.15 ms
( data can be anonymized,)59.5 699.15 ms
(and obfuscated. In this video, we'll first discuss)59.5 686.15 ms
(what is anonymized data, and why organizers decide to)59.5 673.15 ms
(anonymize their data. And next, we will see what we)59.5 660.15 ms
(as competitors can do about it. Sometimes we can decode the data)59.5 647.15 ms
(, or if we can not we can try to guess,)59.5 634.15 ms
(what is the type of feature. So, let's get to the discussion. So)59.5 621.15 ms
(metimes the organizers really want)59.5 608.15 ms
(some information to be reviewed. So, they make an effort to)59.5 595.15 ms
(export competition data, in a way one couldn't get)59.5 582.15 ms
(while you're out of it. Yet all the features are preserved, and )59.5 569.15 ms
(machinery model will be)59.5 556.15 ms
(able to do it's job. For example, if a company wants)59.5 543.15 ms
(someone to classify its document, but doesn't want to reveal)59.5 530.15 ms
(the document's content. It can replace all the word occurrences)59.5 517.15 ms
(with hash values of those words, like in the example you see her)59.5 504.15 ms
(e. In fact, it will not change a thing for)59.5 491.15 ms
(a model based on bags of words. I will refer to Anonymized)59.5 478.15 ms
(data as to any data which organizers intentionally changed. Alth)59.5 465.15 ms
(ough it is not completely correct,)59.5 452.15 ms
(I will use this wording for any type of changes. In computations)59.5 439.15 ms
( with tabular data, companies can try to hide)59.5 426.15 ms
(information each column stores. Take a look at this data set. Fi)59.5 413.15 ms
(rst, we don't have any)59.5 400.15 ms
(meaningful names for the features. The names are replaced with s)59.5 387.15 ms
(ome dummies,)59.5 374.15 ms
(and we see some hash like values)59.5 361.15 ms
(in columns x1 and x6. Most likely, organizers decided)59.5 348.15 ms
(to hash some sensitive data. There are several things we can do)59.5 335.15 ms
(while exploring the data in this case. First, we can try to deco)59.5 322.15 ms
(de or de-anonymize the data,)59.5 309.15 ms
(in a legal way of course. That is, we can try to guess)59.5 296.15 ms
(true meaning of the features. Sometimes de-anonymization)59.5 283.15 ms
(is not possible, but what we almost surely can do,)59.5 270.15 ms
(is to guess the type of the features, separating them into numer)59.5 257.15 ms
(ic,)59.5 244.15 ms
(categorical, and so on. Then, we can try to find how)59.5 231.15 ms
(features relate to each other. That can be a specific relation)59.5 218.15 ms
(between a pair of features, or we can try to figure out if)59.5 205.15 ms
(the features are grouped in some way. In this video we will conc)59.5 192.15 ms
(entrate)59.5 179.15 ms
(on the first problem. In the next video we will discuss)59.5 166.15 ms
(visualization tools, that we can use both for exploring individu)59.5 153.15 ms
(al)59.5 140.15 ms
(features, and feature relations. Let's now get to an example)59.5 127.15 ms
(how it was possible to decode the meaning of the feature in one)59.5 114.15 ms
(local competition I took part. I want to tell you about)59.5 101.15 ms
(a competition I took part. It was a local competition, and)59.5 88.15 ms
(organizers literally didn't give competitors any information)59.5 75.15 ms
(about a dataset. They just put the link to download data on)59.5 62.15 ms
(the competition page, and nothing else. Let's read the data firs)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 6 6
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                      Page 6)59.5 790.15 ms
F0 sf
(t, and basically what we see here is)59.5 764.15 ms
(that the data is anonymized. The column names are like x somethi)59.5 751.15 ms
(ng, and the values are hashes, and)59.5 738.15 ms
(then the rest are numeric in here. But, well we don't know)59.5 725.15 ms
(what they mean at all, and basically we don't)59.5 712.15 ms
(what we are to predict. We only know that it is)59.5 699.15 ms
(a multi-class classification task, and we have four labels. So, )59.5 686.15 ms
(as long as we don't)59.5 673.15 ms
(know what the data is, we can probably build a quick baseline. L)59.5 660.15 ms
(et's import Random Forest Classifier. Yeah, of course we need to)59.5 647.15 ms
( drop)59.5 634.15 ms
(target label from our data frame, as it is included in there. We)59.5 621.15 ms
('ll fill null values with minus 999, and let's encode all the ca)59.5 608.15 ms
(tegorical features, that we can find by looking at the types. Pr)59.5 595.15 ms
(operty of our data frame. We will encode them with Label Encoder)59.5 582.15 ms
(,)59.5 569.15 ms
(and it is easier to do with)59.5 556.15 ms
(function factorize from Pandas. Let's feed to)59.5 543.15 ms
(Random Forest Classifier on our data. And let's plot the feature)59.5 530.15 ms
( importance's,)59.5 517.15 ms
(and what we see here is that feature)59.5 504.15 ms
(X8 looks like an interesting one. We should probably investigate)59.5 491.15 ms
(it a little bit deeper. If we take the feature X8, and print it')59.5 478.15 ms
(s mean, and estimate the value. They turn out to be quite close )59.5 465.15 ms
(to 0,)59.5 452.15 ms
(and 1 respectively, and it looks like this feature was)59.5 439.15 ms
(tendered skilled by the organizers. And we don't see here exactl)59.5 426.15 ms
(y 0,)59.5 413.15 ms
(and exactly 1, because probably training test was)59.5 400.15 ms
(concatenated when on the latest scale. If we concatenate trainin)59.5 387.15 ms
(g test,)59.5 374.15 ms
(then the mean will be exactly 0, and the std will be exactly 1. )59.5 361.15 ms
(Okay, so let's also see are there any)59.5 348.15 ms
(other repeated values in these features? We can do it with a val)59.5 335.15 ms
(ue counts function. Let's print first 15)59.5 322.15 ms
(rows of value counts out. And we can see that there)59.5 309.15 ms
(are a lot of repeated values, they repeated a thousand times. Al)59.5 296.15 ms
(l right, so we now know that)59.5 283.15 ms
(this feature was standard scaled. Probably, we can try to scale )59.5 270.15 ms
(it back. The original feature was multiplied by)59.5 257.15 ms
(a number, and was shifted by a number. All we need to do is to f)59.5 244.15 ms
(ind the shooting)59.5 231.15 ms
(parameter, and the scaling parameter. But how do we do that,)59.5 218.15 ms
(and it is really possible? Let's take unique values of the featu)59.5 205.15 ms
(re,)59.5 192.15 ms
(and sort them. And let's print the difference)59.5 179.15 ms
(between two consecutive numbers, in this sorted array. And look,)59.5 166.15 ms
( it looks like the values)59.5 153.15 ms
(are the same all the time. The distance between two consecutive)59.5 140.15 ms
(unique values in this feature, was the same in the original data)59.5 127.15 ms
( to. It was probably not 0.043 something, it was who knows,)59.5 114.15 ms
(it could be 9 or 11 or 11.7, but it was the same between all the)59.5 101.15 ms
( pairs,)59.5 88.15 ms
(so assume that it was 1 because, well,)59.5 75.15 ms
(1 looks like a natural choice. Let's divide our feature by)59.5 62.15 ms
(this number 0.043 something, and if we do it, yes, we see that)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 7 7
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                      Page 7)59.5 790.15 ms
F0 sf
(the differences become rather close to 1, they are not 1,)59.5 764.15 ms
(only because of some numeric errors. So yes, if we divide our fe)59.5 751.15 ms
(ature by)59.5 738.15 ms
(this value, this is what you get. All right, so what else do we )59.5 725.15 ms
(see here. We see that each number,)59.5 712.15 ms
(it ends with the same values. Each positive number ends)59.5 699.15 ms
(with this kind of value, and each negative with this, look. It l)59.5 686.15 ms
(ooks like this fractional)59.5 673.15 ms
(part was a part of the shifting parameter,)59.5 660.15 ms
(let's just subtract it. And in fact if we subtract it,)59.5 647.15 ms
(the data looks like an integers, actually. Like it was integer d)59.5 634.15 ms
(ata, but)59.5 621.15 ms
(again because of numeric errors, we see some weird numbers in he)59.5 608.15 ms
(re. Let's round the numbers,)59.5 595.15 ms
(and that is what we get. This is actually on the first ten rows,)59.5 582.15 ms
(not the whole feature. Okay, so what's next? What did we do so f)59.5 569.15 ms
(ar? We found the scaling parameter,)59.5 556.15 ms
(probably we were right, because the numbers became integers,)59.5 543.15 ms
(and it's a good sign. We could be not right, because who knows,)59.5 530.15 ms
(the scaling parameter could be 10 or 2 or again 11 and)59.5 517.15 ms
(still the numbers will be integers. But, 1 looks like a good mat)59.5 504.15 ms
(ch. It couldn't be as random, I guess. But, how can we find)59.5 491.15 ms
(the shifting parameter? We found only fractional part,)59.5 478.15 ms
(can we find the other, and can we find the integer part, I mean?)59.5 465.15 ms
( It's actually a hard question, because)59.5 452.15 ms
(while you have a bunch of numbers in here, and you can probably )59.5 439.15 ms
(build a hypothesis. It could be something, and the regular)59.5 426.15 ms
(values for this something is like that, and we could probably sc)59.5 413.15 ms
(ale it,)59.5 400.15 ms
(shift it by this number. But it could be only an approximation,)59.5 387.15 ms
(and not a hypothesis, and so our journey could)59.5 374.15 ms
(really end up in here. But I was really lucky, and I will show i)59.5 361.15 ms
(t to you,)59.5 348.15 ms
(so if you take your x8. I mean our feature, and)59.5 335.15 ms
(print value counts, what we will see, we will this number 11, 17)59.5 322.15 ms
(, 18, something. And then if we scroll down)59.5 309.15 ms
(we will see this, -1968, and it definitely looks like)59.5 296.15 ms
(year a of birth, right? Immediately I have a hypothesis, that th)59.5 283.15 ms
(is could be a text box where)59.5 270.15 ms
(a person should enter his year of birth. And while most of the p)59.5 257.15 ms
(eople really)59.5 244.15 ms
(enter their year of birth, but one person entered zero. Or syste)59.5 231.15 ms
(m automatically entered 0,)59.5 218.15 ms
(when something wrong happened. And wow, that isn't the key. If w)59.5 205.15 ms
(e assume the value was originally 0, then the shifting parameter)59.5 192.15 ms
( is)59.5 179.15 ms
(exactly 9068, let's try it. Let's add 9068 to our data,)59.5 166.15 ms
(and see the values. Again we will use value counts function,)59.5 153.15 ms
(and we will sort sorted values. This is the minimum of the value)59.5 140.15 ms
(s,)59.5 127.15 ms
(and in fact you see the minimum is 0, and all the values are not)59.5 114.15 ms
( negative,)59.5 101.15 ms
(and it looks really plausible. Take a look, 999,)59.5 88.15 ms
(it's probably what people love to enter when they're asked to en)59.5 75.15 ms
(ter something,)59.5 62.15 ms
(or this, 1899. It could be a default value for)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 8 8
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                      Page 8)59.5 790.15 ms
F0 sf
(this textbook, it occurred so many times. And then we see some w)59.5 764.15 ms
(eird values in here. People just put them at random. And then, w)59.5 751.15 ms
(e see some kind of)59.5 738.15 ms
(distribution over the dates. That are plausible for)59.5 725.15 ms
(people who live now, like 1980. Well maybe 1938, I'm not sure ab)59.5 712.15 ms
(out this, and yes of course we see some)59.5 699.15 ms
(days from the future, but for sure it looks like a year of birth)59.5 686.15 ms
(, right? Well the question, how can we use)59.5 673.15 ms
(this information for the competition? Well again for linear mode)59.5 660.15 ms
(ls, you probably could make a new feature)59.5 647.15 ms
(like age group, or something like that. But In this particular c)59.5 634.15 ms
(ompetition, it was no way to use this for,)59.5 621.15 ms
(to use this knowledge. But, it was really fun to investigate. I )59.5 608.15 ms
(hope you liked the example,)59.5 595.15 ms
(but usually is really hard to recognize anything sensible like)59.5 582.15 ms
(a year of birth anonymous features. The best we can do is to)59.5 569.15 ms
(recognize the type of the feature. Is it categorical, numeric,)59.5 556.15 ms
(text, or something else? Last week we saw that each data)59.5 543.15 ms
(type should be treated differently, and more treatment depends)59.5 530.15 ms
(on the model we want to use. That is why to make a stronger mode)59.5 517.15 ms
(l, we)59.5 504.15 ms
(should know what data we are working with. Even though we cannot)59.5 491.15 ms
( understand)59.5 478.15 ms
(what the features are about, we should at least detect the types)59.5 465.15 ms
(of variables in the data. Take a look at this example, we don't)59.5 452.15 ms
(have any meaningful companies, but still we can deduce what)59.5 439.15 ms
(the feature types are. So, x1 looks like text or)59.5 426.15 ms
(physical recorded, x2 and x3 are binary, x4 is numeric,)59.5 413.15 ms
(x5 is either categorical or numeric. And more, if it's numeric i)59.5 400.15 ms
(t could)59.5 387.15 ms
(be something like event calendars, because the values are intege)59.5 374.15 ms
(rs. When the number of columns in data)59.5 361.15 ms
(set is small, like in our example, we can just bring the table, )59.5 348.15 ms
(and)59.5 335.15 ms
(manually sort the types out. But, what if there are thousand)59.5 322.15 ms
(of features in the data set? Very useful functions to)59.5 309.15 ms
(facilitate our exploration, function d types from pandas guesses)59.5 296.15 ms
( the)59.5 283.15 ms
(types for each column in the data frame. Usually it groups all t)59.5 270.15 ms
(he columns)59.5 257.15 ms
(into three categories, flawed, integer, and)59.5 244.15 ms
(so called object type. If dtype function assigned)59.5 231.15 ms
(flawed type to a feature, this feature is most likely to be nume)59.5 218.15 ms
(ric. Integer typed features can be either)59.5 205.15 ms
(binary encoded with a zero or one. Event counters, or even categ)59.5 192.15 ms
(orical,)59.5 179.15 ms
(encoded with the label encoder. Sometimes this function)59.5 166.15 ms
(returns a type named object. And it's the most problematic,)59.5 153.15 ms
(it can be anything, even an irregular numeric feature with)59.5 140.15 ms
(missing values filled with some text. Try it on your data, and a)59.5 127.15 ms
(lso check out a)59.5 114.15 ms
(very similar in full function from Pandas. To deal with object t)59.5 101.15 ms
(ypes, it is useful to)59.5 88.15 ms
(print the data and literally look at it. It is useful to check u)59.5 75.15 ms
(nique)59.5 62.15 ms
(values with value counts function, and nulls location with)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 9 9
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                      Page 9)59.5 790.15 ms
F0 sf
(isnull function at times. In this lesson, we were discussing two)59.5 764.15 ms
(things we can do with anonymized features. We saw that sometimes)59.5 751.15 ms
(, it's possible to decode features,)59.5 738.15 ms
(find out what this feature really means. It doesn't matter if we)59.5 725.15 ms
( understand)59.5 712.15 ms
(the meaning of the features or not, we should guess the feature )59.5 699.15 ms
(types,)59.5 686.15 ms
(in order to pre-process features accordingly to the type we have)59.5 673.15 ms
(,)59.5 660.15 ms
(and selected model class. In the next video,)59.5 647.15 ms
(we'll see a lot of colorful plots, and talk about visualization,)59.5 634.15 ms
( and)59.5 621.15 ms
(other tools for exploratory data analysis. [SOUND]In the previou)59.5 608.15 ms
(s video, we've tried to decode anonymized features and guess the)59.5 595.15 ms
(ir types. In fact, we want to do more. We want to generate new f)59.5 582.15 ms
(eatures and to find insights in a data. And in this lesson, we w)59.5 569.15 ms
(ill talk about various visualizations that can help us with it. )59.5 556.15 ms
(We will first to see what plots we can draw to explore individua)59.5 543.15 ms
(l features, and then we will get to exploration of feature relat)59.5 530.15 ms
(ions. We'll explore pairs first and then we'll try to find featu)59.5 517.15 ms
(re groups in a dataset. First, there is no recipe how you find i)59.5 504.15 ms
(nteresting things in the data. You should just spend some time l)59.5 491.15 ms
(ooking closely at the data table, printing it, and examining. If)59.5 478.15 ms
( we found something interesting, we then can take a closer look.)59.5 465.15 ms
( So, EDA is kind of an art, but we have a bunch of tools for it )59.5 452.15 ms
(which we'll discuss right now. The first, we can build histogram)59.5 439.15 ms
(s. Histograms split feature edge into bins and show how many poi)59.5 426.15 ms
(nts fall into each bin. Note that histograms may be misleading i)59.5 413.15 ms
(n some cases, so try to overwrite its number of bins when using )59.5 400.15 ms
(it. Also, know that it aggregates in the data, so we cannot see,)59.5 387.15 ms
( for example, if all the values are unique or there are a lot of)59.5 374.15 ms
( repeated values. Let's see in other example. The first thing th)59.5 361.15 ms
(at I want to illustrate here is that histograms can confuse. Loo)59.5 348.15 ms
(king at this histogram, we could probably think that there are a)59.5 335.15 ms
( lot of zero values in this feature. But in fact, if we take log)59.5 322.15 ms
(arithm of the values and build histogram again, we'll clearly se)59.5 309.15 ms
(e that distribution is non-degenerate and there are many more di)59.5 296.15 ms
(stinct values than one. So my point is never make a conclusion b)59.5 283.15 ms
(ased on a single plot. If you have a hypothesis, try to make sev)59.5 270.15 ms
(eral different plots to prove it. The second interesting thing h)59.5 257.15 ms
(ere is that peak. What is it? It turns out that the peak is loca)59.5 244.15 ms
(ted exactly at the mean value of this feature. Seems like organi)59.5 231.15 ms
(zers filled the missing values with the mean values for us. So, )59.5 218.15 ms
(now we understand that values were originally missing. How can w)59.5 205.15 ms
(e use this information? We can replace the missing values we fou)59.5 192.15 ms
(nd with not numbers, nulls again. For example, [inaudible] has a)59.5 179.15 ms
( special algorithm that can fill missing values on its own and s)59.5 166.15 ms
(o, maybe [inaudible] will benefit from explicit missing values. )59.5 153.15 ms
(Or we can fill the missing values with something other than feat)59.5 140.15 ms
(ure mean, for example, with -999. Or we can generate a new featu)59.5 127.15 ms
(re which will indicate that the value was missing. This can be p)59.5 114.15 ms
(articularly useful for linear models. We can also build the plot)59.5 101.15 ms
( where on X axis, we have a row index, and on the Y axis, we hav)59.5 88.15 ms
(e feature values. It is convenient not to connect points with li)59.5 75.15 ms
(ne segments but only draw them with circles. Now, if we observe )59.5 62.15 ms
(horizontal lines on this kind of plot, we understand there are a)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 10 10
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 10)59.5 790.15 ms
F0 sf
( lot of repeated values in this feature. Also, note the randomne)59.5 764.15 ms
(ss over the indices. That is, we see some horizontal patterns bu)59.5 751.15 ms
(t no vertical ones. It means that the data is properly shuffled.)59.5 738.15 ms
( We can also color code the points according to their labels. He)59.5 725.15 ms
(re, we see that the feature is quite good as it presumably gives)59.5 712.15 ms
( a nice class separation. And also, we clearly see that the data)59.5 699.15 ms
( is not shuffled here. It is, in fact, sorted by class label. It)59.5 686.15 ms
( is useful to examine statistics with Pandas' describe function.)59.5 673.15 ms
( You can see examples of its output on the screenshot. It gives )59.5 660.15 ms
(you information about mean, standard deviation, and several perc)59.5 647.15 ms
(entiles of the feature distribution. Of course, you can manually)59.5 634.15 ms
( compute those statistics. In Pandas' nan type, you can find fun)59.5 621.15 ms
(ctions named by statistics they compute. Mean for mean value, va)59.5 608.15 ms
(r for variance, and so on, but it's really convenient to have th)59.5 595.15 ms
(em all in once. And finally, as we already discussed in the prev)59.5 582.15 ms
(ious video, there is value_counts function to examine the number)59.5 569.15 ms
( of occurrences of distinct feature values, and a function is nu)59.5 556.15 ms
(ll, which helps to find the missing values in the data. For exam)59.5 543.15 ms
(ple, you can visualize nulls patterns in the data as on the pict)59.5 530.15 ms
(ure you see. So, here's the full list of functions we've discuss)59.5 517.15 ms
(ed. Make sure you remember each of them. To this end, we've disc)59.5 504.15 ms
(ussed visualizations for individual features. And now, let's get)59.5 491.15 ms
( to the next topic of our discussion, exploration of feature rel)59.5 478.15 ms
(ations. It turns out that sometimes, it's hard to make conclusio)59.5 465.15 ms
(ns looking at one feature at a time. So let's look at the pairs.)59.5 452.15 ms
( The best two here is a scatter plot. With it, we can draw one s)59.5 439.15 ms
(equence of values versus another one. And usually, we plot one f)59.5 426.15 ms
(eature versus another feature. So each point on the figure corre)59.5 413.15 ms
(spond to an object with the feature values shown by points posit)59.5 400.15 ms
(ion. If it's a classification task, it's convenient to color cod)59.5 387.15 ms
(e the points with their labels like on this picture. The color i)59.5 374.15 ms
(ndicates the class of the object. For regression, the heat map l)59.5 361.15 ms
(ight coloring can be used, too. Or alternatively, the target val)59.5 348.15 ms
(ue can be visualized by point size. We can effectively use scatt)59.5 335.15 ms
(er plots to check if the data distribution in the train and test)59.5 322.15 ms
( sets are the same. In this example, the red points correspond t)59.5 309.15 ms
(o class zero, and the blue points to class one. And on top of re)59.5 296.15 ms
(d and blue points, we see gray points. They correspond to test s)59.5 283.15 ms
(et. We don't have labels for the test set, that is why they are )59.5 270.15 ms
(gray. And we clearly see that the red points are mixed with part)59.5 257.15 ms
( of the gray ones, and that that is good actually. But other gra)59.5 244.15 ms
(y points are located in the region where we don't have any train)59.5 231.15 ms
(ing data, and that is bad. If you see some kind of discrepancy b)59.5 218.15 ms
(etween colored and gray points distribution, you should probably)59.5 205.15 ms
( stop and think if you're doing it right. It can be just a bug i)59.5 192.15 ms
(n the code, or it can be completely overfitted feature, or somet)59.5 179.15 ms
(hing else that is for sure not healthy. Now, take a look at this)59.5 166.15 ms
( scatter plot. Say, we plot feature X1 versus feature X2. What c)59.5 153.15 ms
(an we say about their relation? The right answer is X2 is less o)59.5 140.15 ms
(r equal than one_minus_X1. Just realize that the equation for th)59.5 127.15 ms
(e diagonal line is X1 + X2 = 1, and for all the points below the)59.5 114.15 ms
( line, X2 is less or equal than one_minus_X1. So, suppose we fou)59.5 101.15 ms
(nd this relation between two features, how do we use this fact? )59.5 88.15 ms
(Of course, it depends, but at least there are some obvious featu)59.5 75.15 ms
(res to generate. For tree-based models, we can create a new feat)59.5 62.15 ms
(ures like the difference or ratio between X1 and X2. Now, take a)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 11 11
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 11)59.5 790.15 ms
F0 sf
( look at this scatter plot. It's hard to say what is the true re)59.5 764.15 ms
(lation between the features, but after all, our goal is not to d)59.5 751.15 ms
(ecode the data here but to generate new features and get a bette)59.5 738.15 ms
(r score. And this plot gives us an idea how to generate the feat)59.5 725.15 ms
(ures out of these two features. We see several triangles on the )59.5 712.15 ms
(picture, so we could probably make a feature to each triangle a )59.5 699.15 ms
(given point belongs, and hope that this feature will help. When )59.5 686.15 ms
(you have a small number of features, you can plot all the pairwi)59.5 673.15 ms
(se scatter plots at once using scatter metrics function from Pan)59.5 660.15 ms
(das. It's pretty handy. It's also nice to have histogram and sca)59.5 647.15 ms
(tter plot before the eyes at the same time as scatter plot gives)59.5 634.15 ms
( you very vague information about densities, while histograms do)59.5 621.15 ms
( not show feature interactions. We can also compute some kind of)59.5 608.15 ms
( distance between the columns of our feature table and store the)59.5 595.15 ms
(m into a matrix of size number of features by a number of featur)59.5 582.15 ms
(es. For example, we can compute correlation between the counts. )59.5 569.15 ms
(It's the most common type of matrices people build, correlation )59.5 556.15 ms
(metric. But we can compute other things than correlation. For ex)59.5 543.15 ms
(ample, how many times one feature is larger than the other? I me)59.5 530.15 ms
(an, how many rows are there such that the value of the first fea)59.5 517.15 ms
(ture is larger than the value of the second one? Or another exam)59.5 504.15 ms
(ple, we can compute how many distinct combinations the features )59.5 491.15 ms
(have in the dataset. With such custom functions, we should build)59.5 478.15 ms
( the metrics manually, and we can use matshow function from Matp)59.5 465.15 ms
(lotlib to visualize it like on the slide you see. If the metrics)59.5 452.15 ms
( looks like a total mess like in here, we can run some kind of c)59.5 439.15 ms
(lustering like K-means clustering on the rows and columns of thi)59.5 426.15 ms
(s matrix and reorder the features. This one looks better, isn't )59.5 413.15 ms
(it? We actually came to the last topic of our discussion, featur)59.5 400.15 ms
(e groups. And it's what we see here. There are groups of very si)59.5 387.15 ms
(milar features, and usually, it's a good idea to generate new fe)59.5 374.15 ms
(atures based on the groups. Again, it depends, but maybe some st)59.5 361.15 ms
(atistics could collated over the group will work fine as feature)59.5 348.15 ms
(s. Another visualization that helps to find feature groups is th)59.5 335.15 ms
(e following: We calculate the statistics of each feature, for ex)59.5 322.15 ms
(ample, mean value, and then plot it against column index. This p)59.5 309.15 ms
(lot can look quite random if the columns are shuffled. So, what )59.5 296.15 ms
(if we sorted the columns based on this statistic? Feature and me)59.5 283.15 ms
(an, in this case. It looks like it worked out. We clearly see th)59.5 270.15 ms
(e groups here. So, now we can take a closer look to each group a)59.5 257.15 ms
(nd use the imagination to generate new features. And here is a l)59.5 244.15 ms
(ist of all the functions we've just discussed. Pause the video a)59.5 231.15 ms
(nd check if you remember the examples we saw. So, finally in thi)59.5 218.15 ms
(s video, we we're talking about the tools and functions that hel)59.5 205.15 ms
(p us with data exploration. For example, to explore features one)59.5 192.15 ms
( by one, we can use histograms, plots, and we can also examine s)59.5 179.15 ms
(tatistics. To explore a relation between the features, the best )59.5 166.15 ms
(tool is a scatter plot. Scatter metrics combines several scatter)59.5 153.15 ms
( plots and histograms on one figure. Correlation plot is useful )59.5 140.15 ms
(to understand how similar the features are. And if we reorder th)59.5 127.15 ms
(e columns and rows of the correlation metrics, we'll probably fi)59.5 114.15 ms
(nd feature groups. And feature groups was the last topic we disc)59.5 101.15 ms
(ussed in this lesson. We also saw a plot of sorted feature stati)59.5 88.15 ms
(stics and how it can reveal as feature groups. Well, of course, )59.5 75.15 ms
(we've discussed only a fraction of helpful plots there are. With)59.5 62.15 ms
( practice, you will develop and find your own tools further expl)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 12 12
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 12)59.5 790.15 ms
F0 sf
(oration.[MUSIC] Hi, in this video we will discuss)59.5 764.15 ms
(a little bit of dataset cleaning and see how to check if dataset)59.5 751.15 ms
( is shuffled. It is important to understand that)59.5 738.15 ms
(the competition data can be only apart of the data organizers ha)59.5 725.15 ms
(ve. The organizers could give us)59.5 712.15 ms
(a fraction of objects they have or a fraction of features. And t)59.5 699.15 ms
(hat is why we can have)59.5 686.15 ms
(some issues with the data. For example, we can encounter a featu)59.5 673.15 ms
(re)59.5 660.15 ms
(which takes the same value for every object in both train and te)59.5 647.15 ms
(st set. This could be due to)59.5 634.15 ms
(the sampling procedure. For example, the future is a year, and t)59.5 621.15 ms
(he organizers exported)59.5 608.15 ms
(us only one year of data. So in the original data)59.5 595.15 ms
(that the organizers have, this future is not constant, but)59.5 582.15 ms
(in the competition data it is constant. And obviously, it is not)59.5 569.15 ms
( useful for)59.5 556.15 ms
(the models and just occupy some memory. So we are about to remov)59.5 543.15 ms
(e)59.5 530.15 ms
(such constant features. In this example data set)59.5 517.15 ms
(feature of zero is constant. It can be the case that the feature)59.5 504.15 ms
(is constant on the train set but how is different values on the )59.5 491.15 ms
(test set. Again, it is better to remove such)59.5 478.15 ms
(features completely since it is constant during training. In our)59.5 465.15 ms
( dataset feature is f1. What is the problem, actually? For examp)59.5 452.15 ms
(le, my new model can assign)59.5 439.15 ms
(some weight to this future, so this future will be a part of the)59.5 426.15 ms
(prediction formula, and this formula will be completely unreliab)59.5 413.15 ms
(le for the objects)59.5 400.15 ms
(with the new values of that feature. For example, for)59.5 387.15 ms
(the last row in our data set. J row, even if categorical feature)59.5 374.15 ms
( is not)59.5 361.15 ms
(constant on the train path but there were values that present on)59.5 348.15 ms
(ly in the test data,)59.5 335.15 ms
(we need to handle this situation properly. We need to decide,)59.5 322.15 ms
(do these new values matter much or not? For example, we can simu)59.5 309.15 ms
(late this)59.5 296.15 ms
(situation with a validation set and compare the quality of the p)59.5 283.15 ms
(redictions)59.5 270.15 ms
(on the objects with the syn feature values and)59.5 257.15 ms
(objects with the new feature values. Maybe we will decide to rem)59.5 244.15 ms
(ove)59.5 231.15 ms
(the feature or maybe we will decide to create a separate model f)59.5 218.15 ms
(or)59.5 205.15 ms
(the object with a new feature values. Sometimes there are duplic)59.5 192.15 ms
(ated)59.5 179.15 ms
(numerical features that these two columns are completely identic)59.5 166.15 ms
(al. In our example data set,)59.5 153.15 ms
(these columns f2 and f3. Obviously, we should leave only one of)59.5 140.15 ms
(those two features since the other one will not give any new inf)59.5 127.15 ms
(ormation to the)59.5 114.15 ms
(model and will only slow down training. From a number of feature)59.5 101.15 ms
(s, it's easy)59.5 88.15 ms
(to check if two columns are the same. We just can compare them e)59.5 75.15 ms
(lement wise. We can also have duplicated)59.5 62.15 ms
(categorical features. The problem is that the features)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 13 13
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 13)59.5 790.15 ms
F0 sf
(can be identical but their levels have different names. That is )59.5 764.15 ms
(it can be possible to rename)59.5 751.15 ms
(levels of one of the features and two columns will become identi)59.5 738.15 ms
(cal. For example features f4 and f5. If we rename levels of the )59.5 725.15 ms
(feature f5, C to A, A to B, and B to C. The result will look)59.5 712.15 ms
(exactly as feature f4. But how do we find such)59.5 699.15 ms
(duplicated features? Fortunately, it's quite easy, it will take )59.5 686.15 ms
(us only one more)59.5 673.15 ms
(line of code to find them. We need to label and code all)59.5 660.15 ms
(the categorical features first, and then compare them as if they)59.5 647.15 ms
( were numbers. The most important part)59.5 634.15 ms
(here is label encoding. We need to do it right. We need to encod)59.5 621.15 ms
(e the features)59.5 608.15 ms
(from top to bottom so that the first unique value we see gets)59.5 595.15 ms
(label 1, the second gets 2 and so on. For example for feature f4)59.5 582.15 ms
(, we will encode A with 1,)59.5 569.15 ms
(B with 2 and C with 3. Now feature f5 will encode)59.5 556.15 ms
(it differently C will be 1, A will be 2 and B will be 3. But aft)59.5 543.15 ms
(er such encodings columns f4 and f5 turn out to be identical and)59.5 530.15 ms
(we can remove one of them. Another important thing)59.5 517.15 ms
(to check is if there are any duplicated rows in the train and)59.5 504.15 ms
(test. Is to write a lot of duplicated rows)59.5 491.15 ms
(that also have different target, it can be a sign the competitio)59.5 478.15 ms
(n)59.5 465.15 ms
(will be more like a roulette, and our validation will be differe)59.5 452.15 ms
(nt)59.5 439.15 ms
(to public leader board score, and private standing will be rathe)59.5 426.15 ms
(r random. Another possibility, duplicated rows)59.5 413.15 ms
(can just be the result of a mistake. There was a competition whe)59.5 400.15 ms
(re)59.5 387.15 ms
(one row was repeated 100,000 times in the training data set. I'm)59.5 374.15 ms
( not sure if it was intentional or)59.5 361.15 ms
(not, but it was necessary to remove those duplicated rows to)59.5 348.15 ms
(have a high score on the test set. Anyway, it's better to explai)59.5 335.15 ms
(n it to ourselves why do we)59.5 322.15 ms
(observe such duplicated rows? This is a part of data)59.5 309.15 ms
(understanding in fact. We should also check if train and)59.5 296.15 ms
(test have common rows. Sometimes it can tell us something)59.5 283.15 ms
(about data set generation process. And again we should probably )59.5 270.15 ms
(think what)59.5 257.15 ms
(could be the reason for those duplicates? Another thing we can d)59.5 244.15 ms
(o,)59.5 231.15 ms
(we can set labels manually for the test rows that)59.5 218.15 ms
(are present in the train set. Finally, it is very useful to chec)59.5 205.15 ms
(k)59.5 192.15 ms
(that the data set is shuffled, because if it is not then, there )59.5 179.15 ms
(is)59.5 166.15 ms
(a high chance to find data leakage. We'll have a special topic a)59.5 153.15 ms
(bout)59.5 140.15 ms
(date leakages later, but for now we'll just discuss)59.5 127.15 ms
(that the data is shuffled. What we can do is we can plug a featu)59.5 114.15 ms
(re or)59.5 101.15 ms
(target vector versus row index. We can optionally smooth)59.5 88.15 ms
(the values using running average. On this slide rolling target)59.5 75.15 ms
(value from pairs competition is plotted while mean target value)59.5 62.15 ms
(is shown with dashed blue line. If the data was shuffled properl)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 14 14
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 14)59.5 790.15 ms
F0 sf
(y we)59.5 764.15 ms
(would expect some kind of oscillation of the target values aroun)59.5 751.15 ms
(d)59.5 738.15 ms
(the mean target value. But in this case, it looks like)59.5 725.15 ms
(the end of the train set is much different to the start,)59.5 712.15 ms
(and we have some patterns. Maybe the information from this parti)59.5 699.15 ms
(cular)59.5 686.15 ms
(plot will not advance our model. But once again,)59.5 673.15 ms
(we should find an explanation for all extraordinary things we ob)59.5 660.15 ms
(serve. Maybe eventually, we will find something)59.5 647.15 ms
(that will lead us to the first place. Finally, I want to encoura)59.5 634.15 ms
(ge)59.5 621.15 ms
(you one more time to visualize every possible)59.5 608.15 ms
(thing in a dataset. Visualizations will lead)59.5 595.15 ms
(you to magic features. So this is the last slide for this lesson)59.5 582.15 ms
(. Hope you've learned something new and)59.5 569.15 ms
(excited about it. Here's a whole list of)59.5 556.15 ms
(topics we've discussed. You can pause this video and try to reme)59.5 543.15 ms
(mber what we were)59.5 530.15 ms
(talking about and where. See you later. [MUSIC][MUSIC] So in thi)59.5 517.15 ms
(s video,)59.5 504.15 ms
(I will go through Springleaf data, it was a competition on Kaggl)59.5 491.15 ms
(e. In that competition,)59.5 478.15 ms
(the competitors were to predict whether a client will respond to)59.5 465.15 ms
(direct mail offer provided by Springleaf. So presumably,)59.5 452.15 ms
(we'll have some features about client, some features about offer)59.5 439.15 ms
(, and we'll)59.5 426.15 ms
(need to predict 1 if he will respond and 0 if he will not, so le)59.5 413.15 ms
(t's start. We'll first import some libraries in here,)59.5 400.15 ms
(define some functions, it's not very interesting. And finally, l)59.5 387.15 ms
(et's load the data and train our test one, and)59.5 374.15 ms
(do a little bit of data overview. So the first thing we want to )59.5 361.15 ms
(know about)59.5 348.15 ms
(our data is the shapes of data tables, so let's bring the train )59.5 335.15 ms
(shape,)59.5 322.15 ms
(and test that test shape. What we see here, we have one)59.5 309.15 ms
(150,000 objects, both in train and test sets, and about 2000)59.5 296.15 ms
(features in both train and test. And what we see more than,)59.5 283.15 ms
(we have one more feature in train, and as humans, just target ca)59.5 270.15 ms
(n)59.5 257.15 ms
(continue to move the train. So we should just keep it in mind an)59.5 244.15 ms
(d)59.5 231.15 ms
(be careful, and drop this column when we feed our models. So let)59.5 218.15 ms
('s examine training and test, so let's use this function had)59.5 205.15 ms
(to print several rows of both. We see here we have ID column, an)59.5 192.15 ms
(d)59.5 179.15 ms
(what's interesting here is that I see in training we have values)59.5 166.15 ms
( 2, 4, 5,)59.5 153.15 ms
(7, and in test we have 1, 3, 6, 9. And it seems like they)59.5 140.15 ms
(are not overlapping, and I suppose the generation)59.5 127.15 ms
(process was as following. So the organizers created a huge)59.5 114.15 ms
(data set with 300,000 rules, and then they sampled at random,)59.5 101.15 ms
(rows for the train and for the test. And that is basically how w)59.5 88.15 ms
(e)59.5 75.15 ms
(get this train and test, and we have this column IG, it is row)59.5 62.15 ms
(index in this original huge file. Then we have something categor)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 15 15
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 15)59.5 790.15 ms
F0 sf
(ical,)59.5 764.15 ms
(then something numeric, numeric again, categorical, then)59.5 751.15 ms
(something that can be numeric or binary. But you see has decimal)59.5 738.15 ms
( part,)59.5 725.15 ms
(so I don't know why, then some very strange values in here,)59.5 712.15 ms
(and again, something categorical. And actually,)59.5 699.15 ms
(we have a lot of in between, and yeah, we have target as the las)59.5 686.15 ms
(t column)59.5 673.15 ms
(of the train set, so let's move on. Probably another thing we wa)59.5 660.15 ms
(nt to)59.5 647.15 ms
(check is whether we have not a numbers in our data set, like non)59.5 634.15 ms
(ce values,)59.5 621.15 ms
(and we can do it in several ways. And one way we, let's compute )59.5 608.15 ms
(how many NaNs are there for)59.5 595.15 ms
(each object, for each row. So this is actually what we do here, )59.5 582.15 ms
(and we print only the values for)59.5 569.15 ms
(the first 15 rows. And so the row 0 has 25 NaNs, row 1 has 19 Na)59.5 556.15 ms
(N,, and so on, but what's interesting here,)59.5 543.15 ms
(six rows have 24 NaNs. It doesn't look like we got it in random,)59.5 530.15 ms
( it's really unlikely to)59.5 517.15 ms
(have these at random. So my hypothesis could be that)59.5 504.15 ms
(the row order has some structure, so the rows are not shuffled, )59.5 491.15 ms
(and)59.5 478.15 ms
(that is why we have this kind of pattern. And that means that we)59.5 465.15 ms
(probably could use row index as another feature for)59.5 452.15 ms
(our classifier, so that is it. And the same, we can do with colu)59.5 439.15 ms
(mns, so for each column, let's compute how)59.5 426.15 ms
(many NaNs are there in each column. And we see that ID has 0 NaN)59.5 413.15 ms
(s,)59.5 400.15 ms
(then some 0s, and then we see that a lot of)59.5 387.15 ms
(columns have the same 56 NaNs. And that is again something reall)59.5 374.15 ms
(y)59.5 361.15 ms
(strange, so either every column will have 56 NaNs, and so it's n)59.5 348.15 ms
(ot magic,)59.5 335.15 ms
(it's probably just how the things go. But if we know that there )59.5 322.15 ms
(are a lot)59.5 309.15 ms
(of columns, and every column have more different number of NaNs,)59.5 296.15 ms
( then it's)59.5 283.15 ms
(really unlikely to have a lot of columns nearer to each other in)59.5 270.15 ms
( the data)59.5 257.15 ms
(set with the same number of NaNs. So probably, our hypothesis co)59.5 244.15 ms
(uld)59.5 231.15 ms
(be here that the column order is not random, so)59.5 218.15 ms
(we could probably investigate this. So we have about 2,000)59.5 205.15 ms
(columns in this data, and it's a really huge number of columns. )59.5 192.15 ms
(And it's really hard to work)59.5 179.15 ms
(with this data set, and basically we don't have any names,)59.5 166.15 ms
(so the data is only mice. As I told you,)59.5 153.15 ms
(the first thing we can do is to determine the types of the data,)59.5 140.15 ms
(so we will do it here. So we're first going to continue train an)59.5 127.15 ms
(d)59.5 114.15 ms
(test on a huge data frame like the organizers had,)59.5 101.15 ms
(it will have 300,000 rows. And then we'll first use)59.5 88.15 ms
(a unique function to determine how many unique)59.5 75.15 ms
(values each column has. And basically here we bring)59.5 62.15 ms
(several values of what we found, and it seems like there are fiv)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 16 16
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 16)59.5 790.15 ms
F0 sf
(e columns)59.5 764.15 ms
(that have only one unique number. So we can drop the,)59.5 751.15 ms
(basically what we have here, we just find them in this line,)59.5 738.15 ms
(and then we drop them. So next we want to remove)59.5 725.15 ms
(duplicated features, but first, for convenience,)59.5 712.15 ms
(fill not a numbers with something that we can find easily later,)59.5 699.15 ms
( and)59.5 686.15 ms
(then we do the following. So we create another data frame of siz)59.5 673.15 ms
(e, of a similar shape as the training set. What we do we take)59.5 660.15 ms
(a column from train set, we apply a label encoder,)59.5 647.15 ms
(as we discussed in a previous video, and we basically store it)59.5 634.15 ms
(in this new train set. So basically we get another)59.5 621.15 ms
(data frame which is train, but label encoded train set. And havi)59.5 608.15 ms
(ng this data frame,)59.5 595.15 ms
(we can easily find duplicated features, we just start iterating)59.5 582.15 ms
(the features with two iterators. Basically, one is fixed and the)59.5 569.15 ms
( second one)59.5 556.15 ms
(goes from the next feature to the end. Then we try to compare th)59.5 543.15 ms
(e columns, the)59.5 530.15 ms
(two columns that we're standing at, right. And if they are eleme)59.5 517.15 ms
(nt wise the same,)59.5 504.15 ms
(then we have duplicated columns, and basically that is how we fi)59.5 491.15 ms
(ll up)59.5 478.15 ms
(this dictionary of duplicated columns. We see it here, so)59.5 465.15 ms
(we found that variable 9 is duplicated for input 8, and)59.5 452.15 ms
(variable 18 again is duplicated for variable 8, and so on, and s)59.5 439.15 ms
(o we have)59.5 426.15 ms
(really a lot of duplicates in here. So this loop, it took some t)59.5 413.15 ms
(ime,)59.5 400.15 ms
(so I prefer to dump the results to disk, so)59.5 387.15 ms
(we can easily restore them. So I do it here, and)59.5 374.15 ms
(then I basically drop those columns that we found from)59.5 361.15 ms
(the train test data frame. So yeah, in the second video, we will)59.5 348.15 ms
( go through some features and do some work to data set. [MUSIC]S)59.5 335.15 ms
(o, let's continue exploration. We wanted to determine the types )59.5 322.15 ms
(of variables, and to do that we will first use this nunique func)59.5 309.15 ms
(tion to determine how many unique values again our feature have.)59.5 296.15 ms
( And we use this dropna=False to make sure this function compute)59.5 283.15 ms
(s and accounts for nons. Otherwise, it will not count nun as uni)59.5 270.15 ms
(que value. It will just unhit them. So, what we see here that ID)59.5 257.15 ms
( has a lot of unique values again and then we have not so huge v)59.5 244.15 ms
(alues in this series, right? So I have 150,000 elements but 6,00)59.5 231.15 ms
(0 unique elements. 25,000, it's not that a huge number, right? S)59.5 218.15 ms
(o, let's aggregate this information and do the histogram of the )59.5 205.15 ms
(values from above. And it's not histogram of these exact values )59.5 192.15 ms
(but but it's normalized values. So, we divide each value by the )59.5 179.15 ms
(number of rows in the tree. It's the maximum value of unique val)59.5 166.15 ms
(ues we could possibly have. So what we see here that there are a)59.5 153.15 ms
( lot of features that have a few unique values and there are sev)59.5 140.15 ms
(eral that have a lot, but not so much, not as much as these. So )59.5 127.15 ms
(these features have almost in every row unique value. So, let's )59.5 114.15 ms
(actually explore these. So, ID essentially is having a lot of un)59.5 101.15 ms
(ique values. No problem with that. But what is this? So what we )59.5 88.15 ms
(actually see here, they are integers. They are huge numbers but )59.5 75.15 ms
(they're integers. Well, I would expect a real, nunique variable )59.5 62.15 ms
(with real values to have a lot of unique values, not integer typ)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 17 17
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 17)59.5 790.15 ms
F0 sf
(e variable. So, what could be our guess what these variables rep)59.5 764.15 ms
(resent? Basically, it can be a counter again. But what else it c)59.5 751.15 ms
(ould be? It could be a time in let's say milliseconds or nanosec)59.5 738.15 ms
(onds or something like that. And we have a lot of unique values )59.5 725.15 ms
(and no overlapping between the values because it's really unlike)59.5 712.15 ms
(ly to have two events or two rows in our data set having the sam)59.5 699.15 ms
(e time, let's say it's time of creation and so on, because the t)59.5 686.15 ms
(ime precision is quite good. So yeah, that could be our guess. S)59.5 673.15 ms
(o next, let's explore this group of features. Again with some ma)59.5 660.15 ms
(nipulations, I found them and these are presented in this table.)59.5 647.15 ms
( So, what's interesting about this? Actually, if you take a look)59.5 634.15 ms
( at the names. So the first one is 541. And the second one is 54)59.5 621.15 ms
(3. Okay. And then we have 1,081 and 1,082, so you see they are s)59.5 608.15 ms
(tanding really close to each other. It's really unlikely that ha)59.5 595.15 ms
(lf of the row, if the column order was random, if the columns we)59.5 582.15 ms
(re shuffled. So, probably the columns are grouped together accor)59.5 569.15 ms
(ding to something and we could explore this something. And what')59.5 556.15 ms
(s more interesting, if we take a look at the values correspondin)59.5 543.15 ms
(g to one row, then we'll find that'll say this value is equal to)59.5 530.15 ms
( this value. And this value is equal to this value and this valu)59.5 517.15 ms
(e, and this is basically the same value that we had in here. So,)59.5 504.15 ms
( we have five features out of four of this having the same value)59.5 491.15 ms
(. And if you examine other objects, some of them will have the s)59.5 478.15 ms
(ame thing happening and some will not. So, you see it could be s)59.5 465.15 ms
(omething that is really essential to the objects and it could be)59.5 452.15 ms
( a nice feature that separates the objects from each other. And,)59.5 439.15 ms
( it's something that we should really investigate and where we s)59.5 426.15 ms
(hould really do some feature engineering. So, for say [inaudible)59.5 413.15 ms
(] , it will be really hard to find those patterns. I mean, it ca)59.5 400.15 ms
(nnot find. Well, it will struggle to find that two features are )59.5 387.15 ms
(equal or five features are equal. So, if we create or say featur)59.5 374.15 ms
(e that will calculate how many features out of these, how many f)59.5 361.15 ms
(eatures we have have the same value say for the object zero wher)59.5 348.15 ms
(e we'll have the value five in this feature and something for ot)59.5 335.15 ms
(her rows, then probably this feature could be discriminative. An)59.5 322.15 ms
(d then we can create other features, say we set it to one if the)59.5 309.15 ms
( values in this column, this and this and this and this are the )59.5 296.15 ms
(same and zero to otherwise, and so on. And basically, if you go )59.5 283.15 ms
(through these rows, you will find that the patterns are differen)59.5 270.15 ms
(t and sometimes the values are the same in different columns. So)59.5 257.15 ms
( for example, for this row, we see that this value is equal to t)59.5 244.15 ms
(his value. And this value is different to previous ones but its )59.5 231.15 ms
(equal to this one. And it's really fascinating, isn't it? And if)59.5 218.15 ms
( it actually will work and improve the model, I will be happy. A)59.5 205.15 ms
(nd another thing we see here is some strange values and they loo)59.5 192.15 ms
(k like nons. I mean, it's something that a human typed in or a m)59.5 179.15 ms
(achine just autofilled. So, let's go further. Oh, yeah. And the )59.5 166.15 ms
(last thing is just try to pick one variable from this group and )59.5 153.15 ms
(see what values does it have. So, let's pick variable 15 and her)59.5 140.15 ms
(e's its values. And minus 999 is probably how we've filled in th)59.5 127.15 ms
(e nons. And yeah, we have 56 of them and all other values are no)59.5 114.15 ms
(n-negative, so probably it's counters. I mean, how many events h)59.5 101.15 ms
(appened in, I don't know, in the month or something like that. O)59.5 88.15 ms
(kay. And finally, let's filter the columns and then separate col)59.5 75.15 ms
(umns into categorical and numeric. And it's really easy to do us)59.5 62.15 ms
(ing this function select_dtypes. Basically, all the columns that)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 18 18
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 18)59.5 790.15 ms
F0 sf
( will have objects type, if you would use a function dtypes. We )59.5 764.15 ms
(think of them as categorical variables. And otherwise, if they a)59.5 751.15 ms
(re assigned type integer or float or something like that, or num)59.5 738.15 ms
(eric type then we will think of these columns as numeric columns)59.5 725.15 ms
(. So, we can go through the features one-by-one as actually I di)59.5 712.15 ms
(d during the competition. Well, we have 2,000 features in this d)59.5 699.15 ms
(ata set and it is unbearable to go through a feature one-by-one.)59.5 686.15 ms
( I've stopped at about 250 features. And you can find in my note)59.5 673.15 ms
(book and reading materials if you're interested. It's a little b)59.5 660.15 ms
(it messy but you can see it. So, What we will do here, just seve)59.5 647.15 ms
(ral examples of what I was trying to investigate in data set, le)59.5 634.15 ms
(t's do the following. Let's take the number of columns, we compu)59.5 621.15 ms
(ted them previously. So, we'll now work with only the first 42 c)59.5 608.15 ms
(olumns and we'll create such metrics. And it looks like correlat)59.5 595.15 ms
(ion matrices and all of that type of matrices like when we have )59.5 582.15 ms
(the features along the y axis, features along the x axis. Basica)59.5 569.15 ms
(lly, well, it's really huge. Yeah. And in this case, what we'll )59.5 556.15 ms
(have as the values is the number or the fraction of elements of )59.5 543.15 ms
(one feature that are greater than elements of the second feature)59.5 530.15 ms
(. So, for example, this cell shows that all variables or all val)59.5 517.15 ms
(ues in variable 50 are less than values and variable ID, which i)59.5 504.15 ms
(s expected. So, yeah. And it's opposite in here. So, if we see o)59.5 491.15 ms
(ne in here it means that variable 45, for example, is always gre)59.5 478.15 ms
(ater than variable 24. And, while we expect this metrics to be s)59.5 465.15 ms
(omehow random, if the count order was random. But, in here we se)59.5 452.15 ms
(e, for example, these kind of square. It means that every second)59.5 439.15 ms
( feature is greater, not to the second but let's say i+1 feature)59.5 426.15 ms
( is greater than the feature i. And, well it could be that this )59.5 413.15 ms
(information is about, for example, counters in different periods)59.5 400.15 ms
( of time. So, for example, the first feature is how many events )59.5 387.15 ms
(happened in the first month. The second feature is how many even)59.5 374.15 ms
(ts happened in the first two month and so kind of cumulative val)59.5 361.15 ms
(ues. And, that is why one feature is always greater than the oth)59.5 348.15 ms
(er. And basically, what information we can extract from this kin)59.5 335.15 ms
(d of metrics is that we have this group and we can generate new )59.5 322.15 ms
(features and these features could be, for example, the differenc)59.5 309.15 ms
(e between two consecutive features. That is how we will extract,)59.5 296.15 ms
( for example, the number of events in each month. So, we'll go f)59.5 283.15 ms
(rom cumulative values back to normal values. And, well linear mo)59.5 270.15 ms
(dels, say, neural networks, they could do it themselves but tree)59.5 257.15 ms
(-based algorithms they could not. So, it could be really helpful)59.5 244.15 ms
(. So, in attached to non-book in the reading materials you will )59.5 231.15 ms
(see that a lot of these kind of patterns. So, we have one in her)59.5 218.15 ms
(e, one in here. The patterns, well, this is also a pattern, isn')59.5 205.15 ms
(t it? And now we will just go through several variables that are)59.5 192.15 ms
( different. So, for example, variable two and variable three are)59.5 179.15 ms
( interesting. If you build a histogram of them, you will see som)59.5 166.15 ms
(ething like that. And, the most interesting part here are these )59.5 153.15 ms
(spikes. And you see, again, they're not random. There's somethin)59.5 140.15 ms
(g in there. So, if we take this variable two and build there, we)59.5 127.15 ms
(ll, use this value count's function, we'll have value and how ma)59.5 114.15 ms
(ny times it occurs in this variable. We will see that the values)59.5 101.15 ms
(, the top values, are 12, 24, 36, 60 and so on. So, they can be )59.5 88.15 ms
(divided by 12 and well probably, this variable is somehow connec)59.5 75.15 ms
(ted to time, isn't it? To hours. Well, and what can we do? We wa)59.5 62.15 ms
(nt to generate features so we will generate feature like the val)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 19 19
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 19)59.5 790.15 ms
F0 sf
(ue of these variable modular 12 or, for example, value of this v)59.5 764.15 ms
(ariable integer division by 12. So, this could really help. In o)59.5 751.15 ms
(ther competition, you could build a variable and see something l)59.5 738.15 ms
(ike that again. And what happened in there, the organizers actua)59.5 725.15 ms
(lly had quantized data. So, they only had data that in our case )59.5 712.15 ms
(could be divided by 12. Say 12, 24 and so on. But, they wanted t)59.5 699.15 ms
(o kind of obfuscate the data probably and they added some noise.)59.5 686.15 ms
( And, that is why if you plot an histogram, you will still see t)59.5 673.15 ms
(he spikes but you will also see something in between the spikes.)59.5 660.15 ms
( And so, again, these features in that competition they work qui)59.5 647.15 ms
(te well and you could dequantize the values and it could really )59.5 634.15 ms
(help. And the same is happening with variable 3 basically, 0, 12)59.5 621.15 ms
(, 24 and so on. And variable 4, I don't have any plot for variab)59.5 608.15 ms
(le 4 itself in here but actually we do the same thing. So, we ta)59.5 595.15 ms
(ke variable 4, we create a new feature variable 4 modulus 50. An)59.5 582.15 ms
(d now, we plot this kind of histogram. What you see here is ligh)59.5 569.15 ms
(t green, there are actually two histograms in there. The first o)59.5 556.15 ms
(ne for object from the class 0 and the second one for the object)59.5 543.15 ms
(s from class 1. And one is depicted with light green and the sec)59.5 530.15 ms
(ond one is with dark green. And, you see these other values. And)59.5 517.15 ms
(, you see only difference in these bar, but, you see the differe)59.5 504.15 ms
(nce. So, it means that these new feature variable 4 modulus 50 c)59.5 491.15 ms
(an be really discriminative when it takes the value 0. So, one c)59.5 478.15 ms
(ould say that this is kind of, well, I don't know how to say tha)59.5 465.15 ms
(t., I mean, certain people would never do that. Like, why do we )59.5 452.15 ms
(want to take away modular 50? But, you see sometimes this can re)59.5 439.15 ms
(ally help. Probably because organizers prepare the data that way)59.5 426.15 ms
(. So, let's get through categorical features. We have actually n)59.5 413.15 ms
(ot a lot of them. We have some labels in here, some binary varia)59.5 400.15 ms
(bles. I don't know what is this, this is probably is some proble)59.5 387.15 ms
(ms with the encoding I have. And then, we have some time variabl)59.5 374.15 ms
(es. This is actually not a time. Time. Not a time. Not a time. T)59.5 361.15 ms
(his is time. Whoa, this is interesting. This looks like cities, )59.5 348.15 ms
(right? Or towns, I mean, city names. And, if you remember what f)59.5 335.15 ms
(eatures we can generate from geolocation, it's the place to gene)59.5 322.15 ms
(rate it. And, then again, it was some time, some labels and once)59.5 309.15 ms
( again, it's the states. Isn't it? So, again, we can generate so)59.5 296.15 ms
(me geographic features. But particularly interesting, the featur)59.5 283.15 ms
(es are the date. Dates that we had in here. And basically, these)59.5 270.15 ms
( are all the columns that I found having the data information. S)59.5 257.15 ms
(o, it was one of the best features for this competition actually)59.5 244.15 ms
(. You could do the following, you could do a scatter plot betwee)59.5 231.15 ms
(n two date features to particular date features and found that t)59.5 218.15 ms
(hey have some relation, and, one is always greater than another.)59.5 205.15 ms
( It means that probably these are dates of some events and one e)59.5 192.15 ms
(vent is happening always after the first one. So, we can extract)59.5 179.15 ms
( different features like the difference between these two dates.)59.5 166.15 ms
( And in this competition, it really helped a lot. So, be sure to)59.5 153.15 ms
( do exploratory data analysis and extract all the powerful featu)59.5 140.15 ms
(res like that. Otherwise, if you don't want to look into the dat)59.5 127.15 ms
(a, you will not find something like that. And, it's really inter)59.5 114.15 ms
(esting. So, thank you for listening.Hi, everyone. In this video,)59.5 101.15 ms
( I will tell you about the specifics of Numerai Competition that)59.5 88.15 ms
( was held throughout year 2016. Note that Numerai organizers cha)59.5 75.15 ms
(nged the format in 2017. So, the findings I'm going to read will)59.5 62.15 ms
( not work on new data. Let's state the problem. Participants wer)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 20 20
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 20)59.5 790.15 ms
F0 sf
(e solving a binary classification task on a data set with 21 ano)59.5 764.15 ms
(nymized numeric features. Unusual part is that both train and te)59.5 751.15 ms
(st data sets have been updating every week. Data sets were also )59.5 738.15 ms
(shuffled column-wise. So it was like a new task every week. Pret)59.5 725.15 ms
(ty challenging. As it turned out, this competition had a data le)59.5 712.15 ms
(ak. Organizers did not disclose any information about the nature)59.5 699.15 ms
( of data set. But allegedly, it was some time series data with t)59.5 686.15 ms
(arget variable highly dependent on transitions between time poin)59.5 673.15 ms
(ts. Think of something like predicting price change in stock mar)59.5 660.15 ms
(ket here. Means that, if we knew true order or had timestamp var)59.5 647.15 ms
(iable, we could easily get nearly perfect score. And therefore, )59.5 634.15 ms
(we had to somehow reconstruct this order. Of course, approximate)59.5 621.15 ms
(ly. But even a rough approximation was giving a huge advantage o)59.5 608.15 ms
(ver other participants. The first and most important step is to )59.5 595.15 ms
(find a nearest neighbor for every point in a data set, and add a)59.5 582.15 ms
(ll 21 features from that neighbor to original point. Simple logi)59.5 569.15 ms
(stic regression of those 42 features, 21 from original, and 21 f)59.5 556.15 ms
(rom neighboring points, allowed to get into top 10 on the leader)59.5 543.15 ms
( board. Of course, we can get better scores with some Hardcore E)59.5 530.15 ms
(DA. Let's start exploring correlation metrics of new 21 features)59.5 517.15 ms
(. If group features with highest correlation coefficient next to)59.5 504.15 ms
( each other, we'll get a right picture. This picture can help us)59.5 491.15 ms
( in two different ways. First, we can actually fix some column o)59.5 478.15 ms
(rder. So, weekly column shuffling won't affect our models. And s)59.5 465.15 ms
(econd, we can clearly notice seven groups with three highly corr)59.5 452.15 ms
(elated features in each of them. So, the data actually has some )59.5 439.15 ms
(non-trivial structure. Now, let's remember that we get new data )59.5 426.15 ms
(sets every week. What is more? Each week, train data sets have t)59.5 413.15 ms
(he same number of points. We can assume that there is some conne)59.5 400.15 ms
(ction between consecutive data sets. This is a little strange be)59.5 387.15 ms
(cause we already have a time series. So, what's the connection b)59.5 374.15 ms
(etween the data from different weeks? Well, if we find nearest n)59.5 361.15 ms
(eighbors from every point in current data set from previous data)59.5 348.15 ms
( set, and plot distance distributions, we can notice that first )59.5 335.15 ms
(neighbor is much, much closer than the second. So, we indeed hav)59.5 322.15 ms
(e some connection between consecutive data sets. And it looks li)59.5 309.15 ms
(ke we can build a bijective mapping between them. But let's not )59.5 296.15 ms
(quickly jump into conclusions and do more exploration. Okay. We )59.5 283.15 ms
(found a nearest neighbor in previous data set. What if we examin)59.5 270.15 ms
(e the distances between the neighboring objects at the level of )59.5 257.15 ms
(individual features? We clearly have three different groups of s)59.5 244.15 ms
(even features. Now remember, the sorted correlation matrix? It t)59.5 231.15 ms
(urns out that each of three highly correlated features belong to)59.5 218.15 ms
( a different group. A perfect match. And if we multiply seven fe)59.5 205.15 ms
(atures from the first group by three, and seven features from th)59.5 192.15 ms
(e second group by two in the original data set, recalculate near)59.5 179.15 ms
(est neighbor-based features within the data sets, and re-train o)59.5 166.15 ms
(ur models, we'll get a nice improvement. So, after this magic mu)59.5 153.15 ms
(ltiplications, of course, I'd tried other constants, our true or)59.5 140.15 ms
(der approximation became a little better. Great. Now, let's move)59.5 127.15 ms
( to the true relation. New data, weekly updates, all of it was a)59.5 114.15 ms
( lie. Remember, how we were calculating neighbors between consec)59.5 101.15 ms
(utive data sets? Well, we can forget about consecutiveness. Calc)59.5 88.15 ms
(ulate neighbors between current data set, and the data set from )59.5 75.15 ms
(two weeks ago or two months ago. No matter what, we will be gett)59.5 62.15 ms
(ing pretty much the same distances. Why? The simplest answer is )59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 21 21
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 21)59.5 790.15 ms
F0 sf
(that the data actually didn't change. And every week, we were ge)59.5 764.15 ms
(tting the same data, plus a little bit of noise. And thus, we co)59.5 751.15 ms
(uld find nearest neighbor in each of previous data sets, and ave)59.5 738.15 ms
(rage them all, successfully reducing the variance of added noise)59.5 725.15 ms
(. After averaging, true order approximation became even better. )59.5 712.15 ms
(I have to say that a little bit of test data actually did change)59.5 699.15 ms
( from time to time. But nonetheless, most of the roles migrated )59.5 686.15 ms
(from week to week. Because of that, it was possible to probe the)59.5 673.15 ms
( whole public leader board which helped even further, and so on,)59.5 660.15 ms
( and so on. Of course, there are more details regarding that com)59.5 647.15 ms
(petition, but they aren't very interesting. I wanted to focus on)59.5 634.15 ms
( the process of reverse engineering. Anyway, I hope you like thi)59.5 621.15 ms
(s kind of detective story and realized how important exploratory)59.5 608.15 ms
( data analysis could be. Thank you for your attention and always)59.5 595.15 ms
( pay respect to EDA.This isn't the rare case in competitions whe)59.5 582.15 ms
(n you see people jumping down on leaderboard after revealing pri)59.5 569.15 ms
(vate results. So, we ask ourselves, what is happening out there?)59.5 556.15 ms
( There are two main reasons for these jumps. First, competitors )59.5 543.15 ms
(could ignore the validation and select the submission which scor)59.5 530.15 ms
(ed best against the public leaderboard. Second, is that sometime)59.5 517.15 ms
(s competitions have no consistent public/private data split or t)59.5 504.15 ms
(hey have too little data in either public or private leaderboard)59.5 491.15 ms
(. Well, we as participants, can't influence competitions organiz)59.5 478.15 ms
(ation. We can certainly make sure that we select our most approp)59.5 465.15 ms
(riate submission to be evaluated by private leaderboard. So, the)59.5 452.15 ms
( broad goal of next videos is to provide you a systematic way to)59.5 439.15 ms
( set up validation in a competition, and tackle most common vali)59.5 426.15 ms
(dation problems. Let's quickly overview of the content of the ne)59.5 413.15 ms
(xt videos. First, in this video, we will understand the concept )59.5 400.15 ms
(of validation and overfitting. In the second video, we will iden)59.5 387.15 ms
(tify the number of splits that should be done to establish stabl)59.5 374.15 ms
(e validation. In the third video, we will go through most freque)59.5 361.15 ms
(nt methods which are used to make train/test split in competitio)59.5 348.15 ms
(ns. In the last video, we will discuss most often validation pro)59.5 335.15 ms
(blems. Now, let me start to explain the concept for validation f)59.5 322.15 ms
(or those who may never heard of it. In the nutshell, we want to )59.5 309.15 ms
(check if the model gives expected results on the unseen data. Fo)59.5 296.15 ms
(r example, if you've worked in a healthcare company which goal i)59.5 283.15 ms
(s to improve life of patients, we could be given the task of pre)59.5 270.15 ms
(dicting if a patient will be diagnosed a particular disease in t)59.5 257.15 ms
(he near future. Here, we need to be sure that the model we train)59.5 244.15 ms
( will be applicable in the future. And not just applicable, we n)59.5 231.15 ms
(eed to be sure about what quality this model will have depending)59.5 218.15 ms
( on the number of mistakes the model make. And on the predictive)59.5 205.15 ms
( probability of a patient having this particular disease, we may)59.5 192.15 ms
( want to decide to run special medical tests for the patient to )59.5 179.15 ms
(clarify the diagnosis. So, we need to correctly understand the q)59.5 166.15 ms
(uality of our model. But, this quality can differ on train data )59.5 153.15 ms
(from the past and on the unseen test data from the future. The m)59.5 140.15 ms
(odel could just memorize all patients from the train data and be)59.5 127.15 ms
( completely useless on the test data because we don't want this )59.5 114.15 ms
(to happen. We need to check the quality of the model with the da)59.5 101.15 ms
(ta we have and these checks are the validation. So, usually, we )59.5 88.15 ms
(divide data we have into two parts, train part and validation pa)59.5 75.15 ms
(rt. We fit our model on the train part and check its quality on )59.5 62.15 ms
(the validation part. Beside that, in the last example, our model)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 22 22
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 22)59.5 790.15 ms
F0 sf
( will be checked against the unseen data in the future and actua)59.5 764.15 ms
(lly these data can differ from the data we have. So we should be)59.5 751.15 ms
( ready for this. In competitions, we usually have the similar si)59.5 738.15 ms
(tuation. The organizers of a competition give us the data in two)59.5 725.15 ms
( chunks. First, train data with all target values. And second, t)59.5 712.15 ms
(est data without target values. As in the previous example, we s)59.5 699.15 ms
(hould split the data with labels into train and validation parts)59.5 686.15 ms
(. Furthermore, to ensure the competition spirit, the organizers )59.5 673.15 ms
(split the test data into the public test set and the private tes)59.5 660.15 ms
(t set. When we sent our submissions to the platform, we see the )59.5 647.15 ms
(scores for the public test set while the scores for the private )59.5 634.15 ms
(test set are released only after the end of the competition. Thi)59.5 621.15 ms
(s also ensures that we don't need the test set or in terms of a )59.5 608.15 ms
(model do not overfit. Let me draw you an analogy with the diseas)59.5 595.15 ms
(e projection, if we already divided our data into train and vali)59.5 582.15 ms
(dation parts. And now, we are repeatedly checking our model agai)59.5 569.15 ms
(nst the validation set, some models, just by chance, will have b)59.5 556.15 ms
(etter scores than the others. If we continue to select best mode)59.5 543.15 ms
(ls, modify them, and again select the best from them, we will se)59.5 530.15 ms
(e constant improvements in the score. But that doesn't mean we w)59.5 517.15 ms
(ill see these improvements on the test data from the future. By )59.5 504.15 ms
(repeating this over and over, we could just achieve the validati)59.5 491.15 ms
(on set or in terms of a competition, we could just cheat the pub)59.5 478.15 ms
(lic leaderboard. But again, if it overfit, the private leaderboa)59.5 465.15 ms
(rd will let us down. This is what we call overfitting in a compe)59.5 452.15 ms
(tition. Get an unrealistically good scores on the public leaderb)59.5 439.15 ms
(oard that later result in jumping down the private leaderboard. )59.5 426.15 ms
(So, we want our model to be able to capture patterns in the data)59.5 413.15 ms
( but only those patterns that generalize well between both train)59.5 400.15 ms
( and test data. Let me show you this process in terms of underfi)59.5 387.15 ms
(tting and overfitting. So, to choose the best model, we basicall)59.5 374.15 ms
(y want to avoid underfitting on the one side and overfitting on )59.5 361.15 ms
(the other. Let's understand this concept on a very simple exampl)59.5 348.15 ms
(e of a binary classification test. We will be using simple model)59.5 335.15 ms
(s defined by formulas under the pictures and visualize the resul)59.5 322.15 ms
(ts of model's predictions. Here on the left picture, we can see )59.5 309.15 ms
(that if the model is too simple, it can't capture underlined rel)59.5 296.15 ms
(ationship and we will get poor results. This is called underfitt)59.5 283.15 ms
(ing. Then, if we want our results to improve, we can increase th)59.5 270.15 ms
(e complexity of the model and that will probably find that quali)59.5 257.15 ms
(ty on the training data is going down. But on the other hand, if)59.5 244.15 ms
( we make too complicated model like on the right picture, it wil)59.5 231.15 ms
(l start describing noise in the train data that doesn't generali)59.5 218.15 ms
(ze the test data. And this will lead to a decrease of model's qu)59.5 205.15 ms
(ality. This is called overfitting. So, we want something in betw)59.5 192.15 ms
(een underfitting and overfitting here. And for the purpose of ch)59.5 179.15 ms
(oosing the most suitable model, we want to be able to evaluate o)59.5 166.15 ms
(ur results. Here, we need to make a remark, that the meaning of )59.5 153.15 ms
(overfitting in machine learning in general and the meaning of ov)59.5 140.15 ms
(erfitting competitions in particular are slightly different. In )59.5 127.15 ms
(general, we say that the model is overfitted if its quality on t)59.5 114.15 ms
(he train set is better than on the test set. But in competitions)59.5 101.15 ms
(, we often say, that the models are overfitted only in case when)59.5 88.15 ms
( quality on the test set will be worse than we have expected. Fo)59.5 75.15 ms
(r example, if you train gradient boosting decision tree in the c)59.5 62.15 ms
(ompetition is our area under a curve metric. We sometimes can ob)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 23 23
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 23)59.5 790.15 ms
F0 sf
(serve that the quality on the training data is close to one whil)59.5 764.15 ms
(e on the test data, it could be less for example, near 0.9. In g)59.5 751.15 ms
(eneral sense, the models overfitted here but while we get area u)59.5 738.15 ms
(nder curve was 0.9 on both validation and public/private test se)59.5 725.15 ms
(ts, we will not say that it is overfitted in the context of a co)59.5 712.15 ms
(mpetition. Let me illustrate this concept again in a bit differe)59.5 699.15 ms
(nt way. So, lets say for the purpose of model evaluation, we div)59.5 686.15 ms
(ided our data into two parts. Train and validation parts. Like w)59.5 673.15 ms
(e already did, we will derive model's complexity from low to hig)59.5 660.15 ms
(h and look at the models here. Note, that usually, we understand)59.5 647.15 ms
( error or loss is something which is opposite to model's quality)59.5 634.15 ms
( or score. In the figure, the dependency looks pretty reasonable)59.5 621.15 ms
(. For two simple models, we have underfitting which means higher)59.5 608.15 ms
( on both train and validation. For two complex models, we have o)59.5 595.15 ms
(verfitting which means low error on train but again high error o)59.5 582.15 ms
(n validation. In the middle, between them, if the perfect model')59.5 569.15 ms
(s complexity, it has the lowest train on the validation data and)59.5 556.15 ms
( thus we expect it to have the lowest error on the unseen test d)59.5 543.15 ms
(ata. Note, that here the training error is always better than th)59.5 530.15 ms
(e test error which implies overfitting in general sense, but doe)59.5 517.15 ms
(sn't apply in the context of competitions. Well done. In this vi)59.5 504.15 ms
(deo, we define validation, demonstrated its purpose, and interpr)59.5 491.15 ms
(eted validation in terms of underfitting and overfitting. So, on)59.5 478.15 ms
(ce again, in general, the validation helps us answer the questio)59.5 465.15 ms
(n, what will be the quality of our model on the unseeing data an)59.5 452.15 ms
(d help us select the model which will be expected to get the bes)59.5 439.15 ms
(t quality on that test data. Usually, we are trying to avoid und)59.5 426.15 ms
(erfitting on the one side that is we want our model to be expres)59.5 413.15 ms
(sive enough to capture the patterns in the data. And we are tryi)59.5 400.15 ms
(ng to avoid overfitting on the other side, and don't make too co)59.5 387.15 ms
(mplex model, because in that case, we will start to capture nois)59.5 374.15 ms
(e or patterns that doesn't generalize to the test data.[SOUND] I)59.5 361.15 ms
(n the previous video,)59.5 348.15 ms
(we understood that validation helps us select a model which will)59.5 335.15 ms
(perform best on the unseen test data. But, to use validation, we)59.5 322.15 ms
( first need)59.5 309.15 ms
(to split the data with given labels, entrain, and validation par)59.5 296.15 ms
(ts. In this video, we will discuss)59.5 283.15 ms
(different validation strategies. And answer the questions. How m)59.5 270.15 ms
(any splits should we make and what are the most often methods)59.5 257.15 ms
(to perform such splits. Loosely speaking, the main difference)59.5 244.15 ms
(between these validation strategies is the number of splits bein)59.5 231.15 ms
(g done. Here I will discuss three of them. First is holdout, sec)59.5 218.15 ms
(ond is K-fold,)59.5 205.15 ms
(and third is leave-one-out. Let's start with holdout. It's a sim)59.5 192.15 ms
(ple data split which)59.5 179.15 ms
(divides data into two parts, training data frame, and)59.5 166.15 ms
(validation data frame. And the important note here)59.5 153.15 ms
(is that in any method, holdout included, one sample can go)59.5 140.15 ms
(either to train or to validation. So the samples between train a)59.5 127.15 ms
(nd)59.5 114.15 ms
(the validation do not overlap, if they do,)59.5 101.15 ms
(we just can't trust our validation. This is sometimes the case,)59.5 88.15 ms
(when we have repeated samples in the data. And if we are,)59.5 75.15 ms
(we will get better predictions for these samples and)59.5 62.15 ms
(more optimistic all estimation overall. It is easy to see that t)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 24 24
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 24)59.5 790.15 ms
F0 sf
(hese can prevent)59.5 764.15 ms
(us from selecting best parameters for our model. For example,)59.5 751.15 ms
(over fitting is generally bad. But if we have duplicated samples)59.5 738.15 ms
(that present, and train, and test simultaneously and over feed,)59.5 725.15 ms
(validation scores can deceive us into a belief that maybe we are)59.5 712.15 ms
( moving)59.5 699.15 ms
(in the right direction. Okay, that was the quick note about)59.5 686.15 ms
(why samples between train and validation must not overlap. Back )59.5 673.15 ms
(to holdout. Here we fit our model on)59.5 660.15 ms
(the training data frame, and evaluate its quality on)59.5 647.15 ms
(the validation data frame. Using scores from this evaluation,)59.5 634.15 ms
(we select the best model. When we are ready to make a submission)59.5 621.15 ms
(, we can retrain our model on)59.5 608.15 ms
(our data with given labels. Thinking about using)59.5 595.15 ms
(holdout in the competition. It is usually a good choice,)59.5 582.15 ms
(when we have enough data. Or we are likely to get similar scores)59.5 569.15 ms
( for the same model,)59.5 556.15 ms
(if we try different splits. Great, since we understood)59.5 543.15 ms
(what holdout is, let's move onto the second validation)59.5 530.15 ms
(strategy, which is called K-fold. K-fold can be viewed as a repe)59.5 517.15 ms
(ated)59.5 504.15 ms
(holdout, because we split our data into key parts and iterate th)59.5 491.15 ms
(rough them, using)59.5 478.15 ms
(every part as a validation set only once. After this procedure,)59.5 465.15 ms
(we average scores over these K-folds. Here it is important to un)59.5 452.15 ms
(derstand)59.5 439.15 ms
(the difference between K-fold and usual holdout or bits of K-tim)59.5 426.15 ms
(es. While it is possible to average scores)59.5 413.15 ms
(they receive after K different holdouts. In this case,)59.5 400.15 ms
(some samples may never get invalidation, while others can be the)59.5 387.15 ms
(re multiple times. On the other side, the core idea of K-fold)59.5 374.15 ms
(is that we want to use every sample for validation only once. Th)59.5 361.15 ms
(is method is a good choice when we)59.5 348.15 ms
(have a minimum amount of data, and we can get either a sufficien)59.5 335.15 ms
(tly)59.5 322.15 ms
(big difference in quality, or different optimal)59.5 309.15 ms
(parameters between folds. Great, having dealt with K-fold, we ca)59.5 296.15 ms
(n move on to the third)59.5 283.15 ms
(validation strategy in our release. It is called leave-one-out. )59.5 270.15 ms
(And basically it is a special)59.5 257.15 ms
(case of Kfold when K is equal to the number)59.5 244.15 ms
(of samples in our data. This means that it will iterate)59.5 231.15 ms
(through every sample in our data. Each time usion came in a slot)59.5 218.15 ms
( minus)59.5 205.15 ms
(one object is a train subset and one object left is a test subse)59.5 192.15 ms
(t. This method can be helpful if)59.5 179.15 ms
(we have too little data and just enough model to entrain. So tha)59.5 166.15 ms
(t there, validation strategies. Holdout, K-fold and leave-one-ou)59.5 153.15 ms
(t. We usually use holdout or)59.5 140.15 ms
(K-fold on shuffle data. By shuffling data we are trying to)59.5 127.15 ms
(reproduce random trained validation split. But sometimes, especi)59.5 114.15 ms
(ally if you)59.5 101.15 ms
(do not have enough samples for some class, a random split can fa)59.5 88.15 ms
(il. Let's consider, for an example. We have binary classificatio)59.5 75.15 ms
(n tests and)59.5 62.15 ms
(a small data set with eight samples. Four of class zero, and fou)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 25 25
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 25)59.5 790.15 ms
F0 sf
(r of class one. Let's split data into four folds. Done, but noti)59.5 764.15 ms
(ce, we are not always)59.5 751.15 ms
(getting 0 and 1 in the same problem. If we'll use the second fol)59.5 738.15 ms
(d for)59.5 725.15 ms
(validation, we'll get an average value of the target in the)59.5 712.15 ms
(train of two third instead of one half. This can drastically cha)59.5 699.15 ms
(nge)59.5 686.15 ms
(predictions of our model. What we need here to handle)59.5 673.15 ms
(this problem is stratification. It is just the way to insure we')59.5 660.15 ms
(ll get similar target)59.5 647.15 ms
(distribution over different faults. If we split data into four)59.5 634.15 ms
(faults with stratification, the average of each false target)59.5 621.15 ms
(values will be equal to one half. It is easier to guess that sig)59.5 608.15 ms
(nificance)59.5 595.15 ms
(of this problem is higher, first for small data sets, like in th)59.5 582.15 ms
(is example,)59.5 569.15 ms
(second for unbalanced data sets. And for binary classification,)59.5 556.15 ms
(that could be, if target average were very close to 0 or)59.5 543.15 ms
(vice versa, very close to 1. And third, for multiclass classific)59.5 530.15 ms
(ation)59.5 517.15 ms
(tasks with huge amount of classes. For good classification data )59.5 504.15 ms
(sets, stratification split will be quite)59.5 491.15 ms
(similar to a simple shuffle split. That is, to a random split. W)59.5 478.15 ms
(ell done, in this video we have discussed)59.5 465.15 ms
(different validation strategies and reasons to use each one of t)59.5 452.15 ms
(hem. Let's summarize this all. If we have enough data, and)59.5 439.15 ms
(we're likely to get similar scores and optimal model's parameter)59.5 426.15 ms
(s for)59.5 413.15 ms
(different splits, we can go with Holdout. If on the contrary, sc)59.5 400.15 ms
(ores and)59.5 387.15 ms
(optimal parameters differ for different splits,)59.5 374.15 ms
(we can choose KFold approach. And event, if we too little data,)59.5 361.15 ms
(we can apply leave-one-out. The second big takeaway from this vi)59.5 348.15 ms
(deo)59.5 335.15 ms
(for you should be stratification. It helps make validation more )59.5 322.15 ms
(stable,)59.5 309.15 ms
(and especially useful for small and unbalanced datasets. Great. )59.5 296.15 ms
(In the next videos we will continue to)59.5 283.15 ms
(comprehend validation at it's core. [SOUND] [MUSIC]Since we alre)59.5 270.15 ms
(ady know the main strategies for validation, we can move to more)59.5 257.15 ms
( concrete examples. Let's imagine, we're solving a competition w)59.5 244.15 ms
(ith a time series prediction, namely, we are to predict a number)59.5 231.15 ms
( of customers for a shop for which they're due in next month. Ho)59.5 218.15 ms
(w should we divide the data into train and validation here? Basi)59.5 205.15 ms
(cally, we have two possibilities. Having data frame first, we ca)59.5 192.15 ms
(n take random rows in validation and second, we can make a time-)59.5 179.15 ms
(based split, take everything before some date as a train and eve)59.5 166.15 ms
(rything out there as a validation. Let's plan these two options )59.5 153.15 ms
(next. Now, when you think about features you need to generate an)59.5 140.15 ms
(d the model you need to train, how complicated these two cases a)59.5 127.15 ms
(re? In the first block, we can just interpret between the previo)59.5 114.15 ms
(us and the next value to get our predictions. Very easy, but wai)59.5 101.15 ms
(t. Do we really have future information about the number of cust)59.5 88.15 ms
(omers in the real world? Well, probably not. But does this mean )59.5 75.15 ms
(that this validation is useless? Again, it doesn't. What it does)59.5 62.15 ms
( mean really that if we make train validation split different fr)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 26 26
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 26)59.5 790.15 ms
F0 sf
(om train/test split, then we are going to create a useless model)59.5 764.15 ms
(. And here, we get to the main rule of making a reliable validat)59.5 751.15 ms
(ion. We should, if possible, set up validation to mimic train/te)59.5 738.15 ms
(st split, but that's a little later. Let's go back to our exampl)59.5 725.15 ms
(e. On the second picture, for most of test point, we have neithe)59.5 712.15 ms
(r the next value nor the previous one. Now, let's imagine we hav)59.5 699.15 ms
(e a pool of different models trained on different features, and )59.5 686.15 ms
(we selected the best model for each type of validation. Now, the)59.5 673.15 ms
( question, will these models differ? And if they will, how signi)59.5 660.15 ms
(ficantly? Well, it is certain that if you want to predict what w)59.5 647.15 ms
(ill happen a few points later, then the model which favor featur)59.5 634.15 ms
(es like previous and next target values will perform poorly. It )59.5 621.15 ms
(happens because in this case, we just don't have such observatio)59.5 608.15 ms
(ns for the test data. But we have to give the model something in)59.5 595.15 ms
( the feature value, and it probably will be not numbers or missi)59.5 582.15 ms
(ng values. How much experience that model have with these type o)59.5 569.15 ms
(f situations? Not much. The model just won't expect that and qua)59.5 556.15 ms
(lity will suffer. Now, let's remember the second case. Actually,)59.5 543.15 ms
( here we need to rely more on the time trend. And so, the featur)59.5 530.15 ms
(es, which is the model really we need here, are more like what w)59.5 517.15 ms
(as the trend in the last couple of months or weeks? So, that sho)59.5 504.15 ms
(ws that the model selected as the best model for the first type )59.5 491.15 ms
(of validation will perform poorly for the second type of validat)59.5 478.15 ms
(ion. On the opposite, the best model for the second type of vali)59.5 465.15 ms
(dation was trained to predict many points ahead, and it will not)59.5 452.15 ms
( use adjacent target values. So, to conclude this comparison, th)59.5 439.15 ms
(ese models indeed differ significantly, including the fact that )59.5 426.15 ms
(most useful features for one model are useless for another. But,)59.5 413.15 ms
( the generated features are not the only problem here. Consider )59.5 400.15 ms
(that actual train/test split is time-based, here is the question)59.5 387.15 ms
(. If we carefully generate features that are drawing attention t)59.5 374.15 ms
(o time-based patterns, we'll get a reliable validation with a ra)59.5 361.15 ms
(ndom-based split. Let me say this again in another words. If we')59.5 348.15 ms
(ll create features which are useful for a time-based split and a)59.5 335.15 ms
(re useless for a random split, will be correct to use a random s)59.5 322.15 ms
(plit to select the model? It's a tough question. Let's take a mo)59.5 309.15 ms
(ment and think about it. Okay, now let's answer this. Consider t)59.5 296.15 ms
(he case when target falls a linear train. In the first block, we)59.5 283.15 ms
( see the exact case of randomly chosen validation. In the second)59.5 270.15 ms
(, we see the same time-based split as we consider before. first,)59.5 257.15 ms
( let's notice that in general, model predictions will be close t)59.5 244.15 ms
(o targets mean value calculated using train data. So in the firs)59.5 231.15 ms
(t block, if the validation points will be closer to this mean va)59.5 218.15 ms
(lue compared to test points, we'll get a better score in validat)59.5 205.15 ms
(ion than on test. But in the second case, the validation points )59.5 192.15 ms
(are roughly as far as the test points from target mean value. An)59.5 179.15 ms
(d so, in the second case, validation score will be more similar )59.5 166.15 ms
(to the test score. Great, as we just found out, in the case of i)59.5 153.15 ms
(ncorrect validation, not only features, but the value target can)59.5 140.15 ms
( lead to unrealistic estimation of the score. Now, that example )59.5 127.15 ms
(was quite similar to what you may encounter while solving real c)59.5 114.15 ms
(ompetitions. Numerous competitions use time-based split namely: )59.5 101.15 ms
(the Rossmann Store Sales competition, the Grupo Bimbo Inventory )59.5 88.15 ms
(Demand competition and others. So, to quickly summarize this val)59.5 75.15 ms
(uable example we just have discussed, different splitting strate)59.5 62.15 ms
(gies can differ significantly, namely: in generated features, in)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 27 27
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 27)59.5 790.15 ms
F0 sf
( the way the model will rely on that features, and in some kind )59.5 764.15 ms
(of target leak. That means, to be able to find smart ideas for f)59.5 751.15 ms
(eature generation and to consistently improve our model, we abso)59.5 738.15 ms
(lutely want to identify train/test split made by organizers, inc)59.5 725.15 ms
(luding the competition, and reproduce it. Let's now categorize m)59.5 712.15 ms
(ost of these splitting strategies and competitions, and discuss )59.5 699.15 ms
(examples for them. Most splits can be united into three categori)59.5 686.15 ms
(es: a random split, a time-based split and the id-based split. L)59.5 673.15 ms
(et's start with the most basic one, the random split. Let's star)59.5 660.15 ms
(t, the most common way of making a train/test split is to split )59.5 647.15 ms
(data randomly by rows. This usually means that the rows are inde)59.5 634.15 ms
(pendent of each other. For example, we have a test of predicting)59.5 621.15 ms
( if a client will pay off alone. Each row represents a person, a)59.5 608.15 ms
(nd these rows are fairly independent of each other. Now, let's c)59.5 595.15 ms
(onsider that there is some dependency, for example, within famil)59.5 582.15 ms
(y members or people which work in the same company. If a husband)59.5 569.15 ms
( can pay a credit probably, his wife can do it too. That means i)59.5 556.15 ms
(f by some misfortune, a husband will will present in the train d)59.5 543.15 ms
(ata and his wife will present in the test data. We probably can )59.5 530.15 ms
(explore this and devise a special feature for that case. For in )59.5 517.15 ms
(such possibilities, and realizing that kind of features is reall)59.5 504.15 ms
(y interesting. More in this case and others I will mention here,)59.5 491.15 ms
( comes in the next lesson of our course. So again, that was a ra)59.5 478.15 ms
(ndom split. The second method is a time-based split. We already )59.5 465.15 ms
(discussed the unit example of the split in the beginning of this)59.5 452.15 ms
( video. In that case, we generally have everything before a part)59.5 439.15 ms
(icular date as a training data, and the rating after date as a t)59.5 426.15 ms
(est data. This can be a signal to use special approach to featur)59.5 413.15 ms
(e generation, especially to make useful features based on the ta)59.5 400.15 ms
(rget. For example, if we are to predict a number of customers fo)59.5 387.15 ms
(r the shop for each day in the next week, we can came up with so)59.5 374.15 ms
(mething like the number of customers for the same day in the pre)59.5 361.15 ms
(vious week, or the average number of customers for the past mont)59.5 348.15 ms
(h. As I mentioned before, this split is widespread enough. It wa)59.5 335.15 ms
(s used in a Rossmann store sales competition and in the Grupo Bi)59.5 322.15 ms
(mbo inventory demand competition, and in other's competitions. A)59.5 309.15 ms
( special case of validation for the time-based split is a moving)59.5 296.15 ms
( window validation. In the previous example, we can move the dat)59.5 283.15 ms
(e which divides train and validation. Successively using week af)59.5 270.15 ms
(ter week as a validation set, just like on this picture. Now, ha)59.5 257.15 ms
(ving dealt with the random and the time-based splits, let's disc)59.5 244.15 ms
(uss the ID-based split. ID can be a unique identifier of user, s)59.5 231.15 ms
(hop, or any other entity. For example, let's imagine we have to )59.5 218.15 ms
(solve a task of music recommendations for completely new users. )59.5 205.15 ms
(That means, we have different sets of users in train and test. I)59.5 192.15 ms
(f so, we probably can make a conclusion that features based on u)59.5 179.15 ms
(ser's history, for example, how many songs user listened in the )59.5 166.15 ms
(last week, will not help for completely new users. As an example)59.5 153.15 ms
( of ID-based split, I want to tell you a bit about the Caterpill)59.5 140.15 ms
(ar to pricing competition. In that competition, train/test split)59.5 127.15 ms
( was done on some category ID, namely, tube ID. There is an inte)59.5 114.15 ms
(resting case when we should employ the ID-based split, but IDs a)59.5 101.15 ms
(re hidden from us. Here, I want to mention two examples of compe)59.5 88.15 ms
(titions with hidden ID-based split. These include Intel and Mumb)59.5 75.15 ms
(aiODT Cervical Cancer Screening competition, and The Nature Cons)59.5 62.15 ms
(ervancy fisheries monitoring competition. In the first competiti)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 28 28
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 28)59.5 790.15 ms
F0 sf
(on, we had to classify patients into three classes, and for each)59.5 764.15 ms
( patient, we had several photos. Indeed, photos of one patient b)59.5 751.15 ms
(elong to the same class. Again, sets of patients from train and )59.5 738.15 ms
(test did not overlap. And we should also ensure these in the tra)59.5 725.15 ms
(ining regulations split. As another example, in The Nature Conse)59.5 712.15 ms
(rvancy fisheries monitoring competition, there were photos of fi)59.5 699.15 ms
(sh from several different fishing boats. Again, fishing boats an)59.5 686.15 ms
(d train and test did not overlap. So one could easily overfit if)59.5 673.15 ms
( you would ignore risk and make a random-based split. Because th)59.5 660.15 ms
(e IDs were not given, competitors had to derive these IDs by the)59.5 647.15 ms
(mselves. In both these competitions, it could be done by cluster)59.5 634.15 ms
(ing pictures. The easiest case was when pictures were taken just)59.5 621.15 ms
( one after another, so the images were quite similar. You can fi)59.5 608.15 ms
(nd more details of such clustering in the kernels of these compe)59.5 595.15 ms
(titions. Now, having in these two main standalone methods, we al)59.5 582.15 ms
(so need to know that they sometimes may be combined. For example)59.5 569.15 ms
(, if we have a task of predicting sales in a shop, we can choose)59.5 556.15 ms
( a split in date for each shop independently, instead of using o)59.5 543.15 ms
(ne date for every shop in the data. Or another example, if we ha)59.5 530.15 ms
(ve search queries from multiple users, is using several search e)59.5 517.15 ms
(ngines, we can split the data by a combination of user ID and se)59.5 504.15 ms
(arch engine ID. Examples of competitions with combined splits in)59.5 491.15 ms
(clude the Western Australia Rental Prices competition by Deloitt)59.5 478.15 ms
(e and their qualification phase of data science game 2017. In th)59.5 465.15 ms
(e first competition, train/test was split by a single date, but )59.5 452.15 ms
(the public/private split was made by different dates for differe)59.5 439.15 ms
(nt geographic areas. In the second competition, participants had)59.5 426.15 ms
( to predict whether a user of online music service will listen t)59.5 413.15 ms
(o the song. The train/test split was made in the following way. )59.5 400.15 ms
(For each user, the last song he listened to was placed in the te)59.5 387.15 ms
(st set, while all other songs were placed in the train set. Fine)59.5 374.15 ms
(. These were the main splitting strategies employed in the compe)59.5 361.15 ms
(titions. Again, the main idea I want you to take away from this )59.5 348.15 ms
(lesson is that your validation should always mimic train/test sp)59.5 335.15 ms
(lit made by organizers. It could be something non-trivial. For e)59.5 322.15 ms
(xample, in the Home Depot Product Search Relevance competition, )59.5 309.15 ms
(participants were asked to estimate search relevancy. In general)59.5 296.15 ms
(, data consisted of search terms and search results for those te)59.5 283.15 ms
(rms, but test set contained completely new search terms. So, we )59.5 270.15 ms
(couldn't use either a random split or a search term-based split )59.5 257.15 ms
(for validation. First split favored more complicated models, whi)59.5 244.15 ms
(ch led to overfitting while second split, conversely, to underfi)59.5 231.15 ms
(tting. So, in order to select optimal models, it was crucial to )59.5 218.15 ms
(mimic the ratio of new search terms from train/test split. Great)59.5 205.15 ms
(. This is it. We just demonstrated major data splitting strategi)59.5 192.15 ms
(es employed in competitions. Random split, time-based split, ID-)59.5 179.15 ms
(based split, and their combinations. This will help us build rel)59.5 166.15 ms
(iable validation, make a useful decisions about feature generati)59.5 153.15 ms
(on, and in the end, select models which will perform best on the)59.5 140.15 ms
( test data. As the main point of this video, remember the genera)59.5 127.15 ms
(l rule of making a reliable validation. Set up your validation t)59.5 114.15 ms
(o mimic the train/test split of the competition.[SOUND] Hi and w)59.5 101.15 ms
(elcome back. In the previous videos we discussed)59.5 88.15 ms
(the concept of validation and overfitting. And discussed how to )59.5 75.15 ms
(chose)59.5 62.15 ms
(validation strategy based on the properties of data we have. And)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 29 29
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 29)59.5 790.15 ms
F0 sf
( finally we learned to identify)59.5 764.15 ms
(data split made by organizers. After all this work being done,)59.5 751.15 ms
(we honestly expect that the relation will, in a way, substitute )59.5 738.15 ms
(a leaderboard for us. That is the score we see on)59.5 725.15 ms
(the validation will be the same for the private leaderboard. Or )59.5 712.15 ms
(at least, if we improve our model and validation, there will be )59.5 699.15 ms
(improvements)59.5 686.15 ms
(on the private leaderboard. And this is usually true, but)59.5 673.15 ms
(sometimes we encounter some problems here. In most cases these p)59.5 660.15 ms
(roblems can)59.5 647.15 ms
(be divided into two big groups. In the first group are the probl)59.5 634.15 ms
(ems)59.5 621.15 ms
(we encounter during local validation. Usually they are caused by)59.5 608.15 ms
(inconsistency of the data, a widespread example is getting diffe)59.5 595.15 ms
(rent)59.5 582.15 ms
(optimal parameters for different faults. In this case we need to)59.5 569.15 ms
( make)59.5 556.15 ms
(more thorough validation. The problems from the second group, of)59.5 543.15 ms
(ten reveal themselves only when we)59.5 530.15 ms
(send our submissions to the platform. And observe that scores on)59.5 517.15 ms
( the validation)59.5 504.15 ms
(and on the leaderboard don't match. In this case, the problem us)59.5 491.15 ms
(ually occurs because we can't mimic the exact)59.5 478.15 ms
(train test split on our validation. These are tough problems, an)59.5 465.15 ms
(d we)59.5 452.15 ms
(definitely want to be able to handle them. So before we start,)59.5 439.15 ms
(let me provide an overview of this video. For both validation an)59.5 426.15 ms
(d submission)59.5 413.15 ms
(stages we will discuss main problems, their causes, how to handl)59.5 400.15 ms
(e them. And then, we'll talk a bit about when)59.5 387.15 ms
(we can expect a leaderboard shuffle. Let's start with discussion)59.5 374.15 ms
(of validation stage problems. Usually, they attract our)59.5 361.15 ms
(attention during validation. Generally, the main problem is)59.5 348.15 ms
(a significant difference in scores and optimal parameters for)59.5 335.15 ms
(different train validation splits. Let's start with an example. )59.5 322.15 ms
(So we can easily explain this problem. Consider that we need to )59.5 309.15 ms
(predict)59.5 296.15 ms
(sales in a shop in February. Say we have target values for the l)59.5 283.15 ms
(ast year, and, usually,)59.5 270.15 ms
(we will take last month in the validation. This means January, b)59.5 257.15 ms
(ut clearly January)59.5 244.15 ms
(has much more holidays than February. And people tend to buy mor)59.5 231.15 ms
(e, which causes)59.5 218.15 ms
(target values to be higher overall. And that mean squared error)59.5 205.15 ms
(of our predictions for January will be greater than for February)59.5 192.15 ms
(. Does this mean that the module)59.5 179.15 ms
(will perform worse for February? Probably not,)59.5 166.15 ms
(at least not in terms of overfitting. As we can see, sometimes t)59.5 153.15 ms
(his kind)59.5 140.15 ms
(of model behavior can be expected. But what if there is no clear)59.5 127.15 ms
( reason)59.5 114.15 ms
(why scores differ for different folds? Let identify several comm)59.5 101.15 ms
(on reasons for)59.5 88.15 ms
(this and see what we can do about it. The first hypotheses we sh)59.5 75.15 ms
(ould consider)59.5 62.15 ms
(and that we have too little data. For example, consider a case w)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 30 30
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 30)59.5 790.15 ms
F0 sf
(hen we have)59.5 764.15 ms
(a lot of patterns and trends in the data. But we do not have eno)59.5 751.15 ms
(ugh samples)59.5 738.15 ms
(to generalize these patterns well. In that case, a model will ut)59.5 725.15 ms
(ilize)59.5 712.15 ms
(only some general patterns. And for each train validation split,)59.5 699.15 ms
(these patterns will partially differ. This indeed, will lead to)59.5 686.15 ms
(a difference in scores of the model. Furthermore, validation sam)59.5 673.15 ms
(ples)59.5 660.15 ms
(will be different each time only increasing the dispersion of sc)59.5 647.15 ms
(ores for)59.5 634.15 ms
(different folds. The second type of this,)59.5 621.15 ms
(is data is too diverse and inconsistent. For example, if you hav)59.5 608.15 ms
(e very similar)59.5 595.15 ms
(samples with different target variance, a model can confuse them)59.5 582.15 ms
(. Consider two cases, first, if one of such examples is in the t)59.5 569.15 ms
(rain)59.5 556.15 ms
(while another is in the validation. We can get a pretty high err)59.5 543.15 ms
(or for)59.5 530.15 ms
(the second sample. And the second case,)59.5 517.15 ms
(if both samples are in validation, we will get smaller errors fo)59.5 504.15 ms
(r them. Or let's remember another)59.5 491.15 ms
(example of diverse data we have already discussed a bit earlier.)59.5 478.15 ms
( I'm talking about the example of)59.5 465.15 ms
(predicting sales for January and February. Here we have the natu)59.5 452.15 ms
(re or the reason for)59.5 439.15 ms
(the differences in scores. As a quick note, notice that in this)59.5 426.15 ms
(example, we can reduce this diversity a bit if we will validate )59.5 413.15 ms
(on)59.5 400.15 ms
(the February from the previous year. So the main reasons for a d)59.5 387.15 ms
(ifference in)59.5 374.15 ms
(scores and optimal model parameters for different folds are, fir)59.5 361.15 ms
(st,)59.5 348.15 ms
(having too little data, and second, having too diverse and)59.5 335.15 ms
(inconsistent data. Now let's outline our actions here. If we are)59.5 322.15 ms
( facing this kind of problem, it can be useful to make)59.5 309.15 ms
(more thorough validation. You can increase K in KFold,)59.5 296.15 ms
(but usually 5 folds are enough. Make KFold validation several ti)59.5 283.15 ms
(mes)59.5 270.15 ms
(with different random splits. And average scores to get a more)59.5 257.15 ms
(stable estimate of model's quality. The same way we can choose)59.5 244.15 ms
(the best parameters for the model if there is a chance to overfi)59.5 231.15 ms
(t. It is useful to use one set of KFold)59.5 218.15 ms
(splits to select parameters and another set of KFold splits)59.5 205.15 ms
(to check model's quality. Examples of competitions which)59.5 192.15 ms
(required extensive validation include the Liberty Mutual Group P)59.5 179.15 ms
(roperty)59.5 166.15 ms
(Inspection Prediction competition and the Santander Customer Sat)59.5 153.15 ms
(isfaction)59.5 140.15 ms
(competition. In both of them, scores of the competitors)59.5 127.15 ms
(were very close to each other. And thus participants tried to)59.5 114.15 ms
(squeeze more from the data. But do not overfit, so)59.5 101.15 ms
(the thorough validation was crucial. Now, having discussed)59.5 88.15 ms
(validation stage problems, let's move on to)59.5 75.15 ms
(submission stage problems. Sometimes you can diagnose these prob)59.5 62.15 ms
(lems)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 31 31
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 31)59.5 790.15 ms
F0 sf
(in the process of doing careful. But still,)59.5 764.15 ms
(often you encounter these type of problems only when you submit )59.5 751.15 ms
(your)59.5 738.15 ms
(solution to the platform. But then again, is your friend when it)59.5 725.15 ms
( comes down)59.5 712.15 ms
(to finding the root of the problem. Generally speaking,)59.5 699.15 ms
(there are two cases of these issues. In the first case, leaderbo)59.5 686.15 ms
(ard)59.5 673.15 ms
(score is consistently higher or lower than validation score. In )59.5 660.15 ms
(the second, leaderboard score is not)59.5 647.15 ms
(correlated with validation score at all. So in the worst case, w)59.5 634.15 ms
(e can improve)59.5 621.15 ms
(our score on the validation. While, on the contrary,)59.5 608.15 ms
(score on the leaderboard will decrease. As you can imagine,)59.5 595.15 ms
(these problems can be much more trouble. Now remember that the m)59.5 582.15 ms
(ain rule)59.5 569.15 ms
(of making a reliable validation, is to mimic a train tests)59.5 556.15 ms
(pre made by organizers. I won't lie to you,)59.5 543.15 ms
(it can be quite hard to identify and mimic the exact train tests)59.5 530.15 ms
( here. Because of that, I highly you to)59.5 517.15 ms
(start submitting your solutions right after you enter the compet)59.5 504.15 ms
(ition. It's good to start exploring other)59.5 491.15 ms
(possible roots of this problem. Let's first sort out causes we c)59.5 478.15 ms
(ould)59.5 465.15 ms
(observe during validation stage. Recall, we already have differe)59.5 452.15 ms
(nt)59.5 439.15 ms
(model scores on different folds during validation. Here it is us)59.5 426.15 ms
(eful to see a leaderboard)59.5 413.15 ms
(as another validation fold. Then, if we already have)59.5 400.15 ms
(different scores in KFold, getting a not very similar result on)59.5 387.15 ms
(the leaderboard is not suprising. More we can calculate mean and)59.5 374.15 ms
( standard)59.5 361.15 ms
(deviation of the validation scores and estimate if the leaderboa)59.5 348.15 ms
(rd)59.5 335.15 ms
(score is expected. But if this is not the case,)59.5 322.15 ms
(then something is definitely wrong. There could be two more reas)59.5 309.15 ms
(ons for)59.5 296.15 ms
(this problem. The first reason, we have too)59.5 283.15 ms
(little data in public leaderboard, which is pretty self explanat)59.5 270.15 ms
(ory. Just trust your validation,)59.5 257.15 ms
(and everything will be fine. And the second train and test data)59.5 244.15 ms
(are from different distributions. Let me explain what I mean whe)59.5 231.15 ms
(n I)59.5 218.15 ms
(talk about different distributions. Consider a regression test o)59.5 205.15 ms
(f)59.5 192.15 ms
(predicting people's height by their photos on Instagram. The blu)59.5 179.15 ms
(e line represents)59.5 166.15 ms
(the distribution of heights for man, while the red line represen)59.5 153.15 ms
(ts)59.5 140.15 ms
(the distribution of heights for women. As you can see,)59.5 127.15 ms
(these distributions are different. Now let's consider that the t)59.5 114.15 ms
(rain)59.5 101.15 ms
(data consists only of women, while the test data consists only o)59.5 88.15 ms
(f men. Then all model predictions will be)59.5 75.15 ms
(around the average height for women. And the distribution of the)59.5 62.15 ms
(se predictions)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 32 32
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 32)59.5 790.15 ms
F0 sf
(will be very similar to that for the train data. No wonder that )59.5 764.15 ms
(our model will have)59.5 751.15 ms
(a terrible score on the test data. Now, because our course is a )59.5 738.15 ms
(practical)59.5 725.15 ms
(one, let's take a moment and think what you can do if you)59.5 712.15 ms
(encounter these in a competition. Okay, let's start with a gener)59.5 699.15 ms
(al)59.5 686.15 ms
(approach to such problems. At the broadest level, we need to fin)59.5 673.15 ms
(d a way to tackle different)59.5 660.15 ms
(distributions in train and test. Sometimes, these kind of proble)59.5 647.15 ms
(ms)59.5 634.15 ms
(could be solved by adjusting your solution during the training p)59.5 621.15 ms
(rocedure. But sometimes, this problem can be solved only by adju)59.5 608.15 ms
(sting your solution)59.5 595.15 ms
(through the leaderboard. That is through leaderboard probing. Th)59.5 582.15 ms
(e simplest way to solve this particular)59.5 569.15 ms
(situation in a competition is to try to figure out the optimal c)59.5 556.15 ms
(onstant)59.5 543.15 ms
(prediction for train and test data. And shift your predictions)59.5 530.15 ms
(by the difference. Right here we can calculate the average)59.5 517.15 ms
(height of women from the train data. Calculating the average hei)59.5 504.15 ms
(ght)59.5 491.15 ms
(of men is a big trickier. If the competition's metric)59.5 478.15 ms
(is means squared error, we can send two constant submissions,)59.5 465.15 ms
(write down the simple formula. And find out that the average tar)59.5 452.15 ms
(get)59.5 439.15 ms
(value for the test is equal to 70 inches. In general, this techn)59.5 426.15 ms
(ique is)59.5 413.15 ms
(known as leaderboard probing. And we will discuss it in the topi)59.5 400.15 ms
(c about. So now we know the difference between the)59.5 387.15 ms
(average target values for the train and the test data, which is )59.5 374.15 ms
(equal to 7 inches. And as the third step of adjusting)59.5 361.15 ms
(our submission to the leaderboard we could just try to add)59.5 348.15 ms
(7 to all predictions. But from this point it is not validational)59.5 335.15 ms
(it is a leaderboard probing and list. Yes we probably could disc)59.5 322.15 ms
(over this)59.5 309.15 ms
(during exploratory data analysis and try to make a correction)59.5 296.15 ms
(in our validation scheme. But sometimes it is not possible)59.5 283.15 ms
(without leaderboard probing, just like in this example. A compet)59.5 270.15 ms
(ition which has something similar)59.5 257.15 ms
(is the Quora question pairs competition. There, distributions of)59.5 244.15 ms
( the target)59.5 231.15 ms
(from train and test were different. So one could get a good)59.5 218.15 ms
(improvement of a score adjusting his predictions)59.5 205.15 ms
(to the leaderboard. But fortunately, this case is rare enough. M)59.5 192.15 ms
(ore often, we encounter situations)59.5 179.15 ms
(which are more like the following case. Consider that now train)59.5 166.15 ms
(consists not only of women, but mostly of women, and test, vice )59.5 153.15 ms
(versa. Consists not only of men,)59.5 140.15 ms
(but mostly of men. The main strategy to deal with)59.5 127.15 ms
(these kind of situations is simple. Again, remember to mimic)59.5 114.15 ms
(the train test split. If the test consists mostly of Men, force )59.5 101.15 ms
(the validation to)59.5 88.15 ms
(have the same distribution. In that case, you ensure that)59.5 75.15 ms
(your validation will be fair. This is true for getting raw score)59.5 62.15 ms
(s and)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 33 33
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 33)59.5 790.15 ms
F0 sf
(optimal parameters correctly. For example,)59.5 764.15 ms
(we could have quite different scores and optimal parameters for )59.5 751.15 ms
(women's and)59.5 738.15 ms
(men's parts of the data set. Ensuring the same distribution in t)59.5 725.15 ms
(est and)59.5 712.15 ms
(validation helps us get scores and parameters relevant to test. )59.5 699.15 ms
(I want to mention two)59.5 686.15 ms
(examples of this here. First the Data Science Game Qualification)59.5 673.15 ms
(Phase: Music recommendation challenge. And second, competition w)59.5 660.15 ms
(ith CTR prediction which we discussed)59.5 647.15 ms
(earlier in the data topic. Let's start with the second one, do y)59.5 634.15 ms
(ou remember the problem,)59.5 621.15 ms
(we have a test of predicting CTR. So, the train data, which basi)59.5 608.15 ms
(cally)59.5 595.15 ms
(was the history of displayed ads obviously didn't contain)59.5 582.15 ms
(ads which were not shown. On the contrary, the test data)59.5 569.15 ms
(consisted of every possible ad. Notice this is the exact case of)59.5 556.15 ms
( different)59.5 543.15 ms
(distributions in train and test. And again, we need to set up ou)59.5 530.15 ms
(r)59.5 517.15 ms
(validation to mimic test here. So we have this huge bias towards)59.5 504.15 ms
(showing that in the train and to set up a correct validation. We)59.5 491.15 ms
( had to complete the validation)59.5 478.15 ms
(set with rows of not shown ads. Now, let's go back to the first )59.5 465.15 ms
(example. In that competition,)59.5 452.15 ms
(participants had to predict whether a user will listen to)59.5 439.15 ms
(a song recommended by assistant. So, the test contained)59.5 426.15 ms
(only recommended songs. But train, on the contrary,)59.5 413.15 ms
(contained both recommended songs and songs users selected themse)59.5 400.15 ms
(lves. So again, one could adjust his validation)59.5 387.15 ms
(by 50 renowned songs selected by users. And again, if we will no)59.5 374.15 ms
(t account for)59.5 361.15 ms
(that fact, then improving our model on actually selected songs c)59.5 348.15 ms
(an result)59.5 335.15 ms
(in the validation score going up. But it doesn't have to result )59.5 322.15 ms
(and)59.5 309.15 ms
(the same improvements for the leaderboard. Okay let's conclude t)59.5 296.15 ms
(his overview)59.5 283.15 ms
(of handling validation problems for the submission stage. If you)59.5 270.15 ms
( have too little data in public)59.5 257.15 ms
(leaderboard, just trust your validation. If that's not the case,)59.5 244.15 ms
(make sure that you did not overfit. Then check if you made)59.5 231.15 ms
(correct train/test split, as we discussed in the previous video.)59.5 218.15 ms
( And finally, check if you have different)59.5 205.15 ms
(distributions in train and test. Great, let's move on to)59.5 192.15 ms
(the next point of this video. For now,)59.5 179.15 ms
(I hope you did everything all right. First, you did extensive va)59.5 166.15 ms
(lidation. Second, you choose a correct splitter)59.5 153.15 ms
(strategy for train validation split. And finally, you ensured th)59.5 140.15 ms
(e same)59.5 127.15 ms
(distributions in validation and testing. But sometimes you have )59.5 114.15 ms
(to expect)59.5 101.15 ms
(leaderboard shuffle anyway, and not just for you, but for everyo)59.5 88.15 ms
(ne. First, for those who've never heard of it,)59.5 75.15 ms
(a leaderboard shuffle happens when participants position some pu)59.5 62.15 ms
(blic and)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 34 34
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 34)59.5 790.15 ms
F0 sf
(private leaderboard drastically different. Take a look at this s)59.5 764.15 ms
(creenshot from)59.5 751.15 ms
(the two sigma financial model in challenge competition. The gree)59.5 738.15 ms
(n and)59.5 725.15 ms
(the red arrows mean how far a team moved. For example, the parti)59.5 712.15 ms
(cipant who)59.5 699.15 ms
(finished the 3rd on the private leaderboard was the 392nd)59.5 686.15 ms
(on the public leaderboard. Let's discuss three main reasons for)59.5 673.15 ms
(that shuffle, randomness, too little data, and different public,)59.5 660.15 ms
(private distributions. So first, randomness, this is the case wh)59.5 647.15 ms
(en all participants)59.5 634.15 ms
(have very similar scores. This can be either a very good score o)59.5 621.15 ms
(r)59.5 608.15 ms
(a very poor one. But the main point here is)59.5 595.15 ms
(that the main reason for differences in scores is randomness. To)59.5 582.15 ms
( understand this a bit more,)59.5 569.15 ms
(let's go through two quick examples here. The first one is the L)59.5 556.15 ms
(iberty Mutual Group, Property Inspection Prediction)59.5 543.15 ms
(competition. In that competition,)59.5 530.15 ms
(scores of competitors were very close. And though randomness did)59.5 517.15 ms
(n't play)59.5 504.15 ms
(a major role in that competition, still many people overfit)59.5 491.15 ms
(on the public leaderboard. The second example,)59.5 478.15 ms
(which is opposite to the first is the TWO SIGMA Financial Model )59.5 465.15 ms
(and)59.5 452.15 ms
(Challenge competition. Because the financial data in that)59.5 439.15 ms
(competition was highly unpredictable, randomness played a major )59.5 426.15 ms
(role in it. So one could say that the leaderboard)59.5 413.15 ms
(shuffle there was among the biggest shuffles on KFold platform. )59.5 400.15 ms
(Okay, that was randomness, the second)59.5 387.15 ms
(reason to expect leaderboard shuffle is too little data overall,)59.5 374.15 ms
( and)59.5 361.15 ms
(in private test set especially. An example of this is the Restau)59.5 348.15 ms
(rant)59.5 335.15 ms
(Revenue Prediction Competition. In that competition, trained set)59.5 322.15 ms
(consisted of less than 200 gross. And this set consisted)59.5 309.15 ms
(of less than 400 gross. So as you can see shuffle)59.5 296.15 ms
(here was more than expected. Last reason of leaderboard)59.5 283.15 ms
(shuffle could be different distributions between public and)59.5 270.15 ms
(private test sets. This is usually the case)59.5 257.15 ms
(with time series prediction, like the Rossmann Stores Sales)59.5 244.15 ms
(competition. When we have a time based split,)59.5 231.15 ms
(we usually have first few weeks in public leaderboard, and)59.5 218.15 ms
(next few weeks in private leaderboards. As people tend to adjust)59.5 205.15 ms
( their)59.5 192.15 ms
(submission to public leaderboard and overfit, we can expect wors)59.5 179.15 ms
(e)59.5 166.15 ms
(results on private leaderboard. Here again, trust your validatio)59.5 153.15 ms
(n and)59.5 140.15 ms
(everything will be fine. Okay, let's go over reasons for)59.5 127.15 ms
(leaderboard shuffling. Now let's conclude both this video and)59.5 114.15 ms
(the entire validation topic. Let's start with the video. First, )59.5 101.15 ms
(if you have big dispersion)59.5 88.15 ms
(of scores on validation stage we should do extensive validation.)59.5 75.15 ms
( That means every score from)59.5 62.15 ms
(different KFold splits, and team model on one split while)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 35 35
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 35)59.5 790.15 ms
F0 sf
(evaluating score on the other. Second, if submission do not)59.5 764.15 ms
(match local validation score, we should first, check if we have )59.5 751.15 ms
(too)59.5 738.15 ms
(little data in public leaderboard. Second, check if we did not o)59.5 725.15 ms
(verfit, check)59.5 712.15 ms
(if you chose correct splitting strategy. And finally, check if t)59.5 699.15 ms
(rained test)59.5 686.15 ms
(have different distributions. You can expect leaderboard shuffle)59.5 673.15 ms
(because of three key things, randomness, little amount of data, )59.5 660.15 ms
(and different)59.5 647.15 ms
(public/private test distributions. So that's it,)59.5 634.15 ms
(in this topic we defined validation and its connection to overfi)59.5 621.15 ms
(tting. Described common validation strategies. Demonstrated majo)59.5 608.15 ms
(r data)59.5 595.15 ms
(splitting strategies. And finally analyzed and learned how)59.5 582.15 ms
(to tackle main validation problems. Remember this, and it will a)59.5 569.15 ms
(bsolutely)59.5 556.15 ms
(help you out in competitions. Make sure you understand)59.5 543.15 ms
(the main idea of validation well. That is,)59.5 530.15 ms
(you need to mimic the trained test split. [MUSIC]Hi everyone. In)59.5 517.15 ms
( this section, we will talk about a very sensitive topic data le)59.5 504.15 ms
(akage or more simply, leaks. We'll define leakage in a very gene)59.5 491.15 ms
(ral sense as an unexpected information in the data that allows u)59.5 478.15 ms
(s to make unrealistically good predictions. For the time being, )59.5 465.15 ms
(you may have think of it as of directly or indirectly adding gro)59.5 452.15 ms
(und truths into the test data. Data leaks are very, very bad. Th)59.5 439.15 ms
(ey are completely unusable in real world. They usually provide w)59.5 426.15 ms
(ay too much signal and thus make competitions lose its main poin)59.5 413.15 ms
(t, and quickly turn them into a leak hunt increase. People often)59.5 400.15 ms
( are very sensitive about this matter. They tend to overreact. T)59.5 387.15 ms
(hat's completely understandable. After spending a lot of time on)59.5 374.15 ms
( solving the problem, a sudden data leak may render all of that )59.5 361.15 ms
(useless. It is not a pleasant position to be in. I cannot force )59.5 348.15 ms
(you to turn the blind eye but keep in mind, there is no ill inte)59.5 335.15 ms
(nt whatsoever. Data leaks are the result of unintentional errors)59.5 322.15 ms
(, accidents. Even if you find yourself in a competition with an )59.5 309.15 ms
(unexpected data leak close to the deadline, please be more toler)59.5 296.15 ms
(ant. The question of whether to exploit the data leak or not is )59.5 283.15 ms
(exclusive to machine learning competitions. In real world, the a)59.5 270.15 ms
(nswer is obviously a no, nothing to discuss. But in a competitio)59.5 257.15 ms
(n, the ultimate goal is to get a higher leaderboard position. An)59.5 244.15 ms
(d if you truly pursue that goal, then exploit the leak in every )59.5 231.15 ms
(way possible. Further in this section, I will show you the main )59.5 218.15 ms
(types of data leaks that could appear during solving a machine l)59.5 205.15 ms
(earning problem. Also focus on a competition specific leak explo)59.5 192.15 ms
(itation technique leaderboard probing. Finally, you will find sp)59.5 179.15 ms
(ecial videos dedicated to the most interesting and non-trivial d)59.5 166.15 ms
(ata leaks. I will start with the most typical data leaks that ma)59.5 153.15 ms
(y occur in almost every problem. Time series is our first target)59.5 140.15 ms
(. Typically, future picking. It is common sense not to pick into)59.5 127.15 ms
( the future like, can we use stock market's price from day after)59.5 114.15 ms
( tomorrow to predict price for tomorrow? Of course not. However,)59.5 101.15 ms
( direct usage of future information in incorrect time splits sti)59.5 88.15 ms
(ll exist. When you enter a time serious competition at first, ch)59.5 75.15 ms
(eck train, public, and private splits. If even one of them is no)59.5 62.15 ms
(t on time, then you found a data leak. In such case, unrealistic)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 36 36
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 36)59.5 790.15 ms
F0 sf
( features like prices next week will be the most important. But )59.5 764.15 ms
(even when split by time, data still contains information about f)59.5 751.15 ms
(uture. We still can access the rows from the test set. We can ha)59.5 738.15 ms
(ve future user history in CTR task, some fundamental indicators )59.5 725.15 ms
(in stock market predictions tasks, and so on. There are only two)59.5 712.15 ms
( ways to eliminate the possibility of data leakage. It's called )59.5 699.15 ms
(competitions, where one can not access rows from future or a tes)59.5 686.15 ms
(t set with no features at all, only IDs. For example, just the n)59.5 673.15 ms
(umber and instrument ID in stock market prediction, so participa)59.5 660.15 ms
(nts create features based on past and join them themselves. Now,)59.5 647.15 ms
( let's discuss something more unusual. Those types of data leaks)59.5 634.15 ms
( are much harder to find. We often have more than just train and)59.5 621.15 ms
( test files. For example, a lot of images or text in archive. In)59.5 608.15 ms
( such case, we can't access some meta information, file creation)59.5 595.15 ms
( date, image resolution etcetera. It turns out that this meta in)59.5 582.15 ms
(formation may be connected to target variable. Imagine classic c)59.5 569.15 ms
(ats versus dogs classification. What if cat pictures were taken )59.5 556.15 ms
(before dog? Or taken with a different camera? Because of that, a)59.5 543.15 ms
( good practice from organizers is to erase the meta data, resize)59.5 530.15 ms
( the pictures, and change creation date. Unfortunately, sometime)59.5 517.15 ms
(s we will forget about it. A good example is Truly Native compet)59.5 504.15 ms
(ition, where one could get nearly perfect scores using just the )59.5 491.15 ms
(dates from zip archives. Another type of leakage could be found )59.5 478.15 ms
(in IDs. IDs are unique identifiers of every row usually used for)59.5 465.15 ms
( convenience. It makes no sense to include them into the model. )59.5 452.15 ms
(It is assumed that they are automatically generated. In reality,)59.5 439.15 ms
( that's not always true. ID may be a hash of something, probably)59.5 426.15 ms
( not intended for disclosure. It may contain traces of informati)59.5 413.15 ms
(on connected to target variable. It was a case in Caterpillar co)59.5 400.15 ms
(mpetition. A link ID as a feature slightly improve the result. S)59.5 387.15 ms
(o I advise you to pay close attention to IDs and always check wh)59.5 374.15 ms
(ether they are useful or not. Next is row order. In trivial case)59.5 361.15 ms
(, data may be shuffled by target variable. Sometimes simply addi)59.5 348.15 ms
(ng row number or relative number, suddenly improves this course.)59.5 335.15 ms
( Like, in Telstra Network Disruptions competition. It's also pos)59.5 322.15 ms
(sible to find something way more interesting like in TalkingData)59.5 309.15 ms
( Mobile User Demographics competition. There was some kind of ro)59.5 296.15 ms
(w duplication, rows next to each other usually have the same lab)59.5 283.15 ms
(el. This is it with a regular type of leaks. To sum things up, i)59.5 270.15 ms
(n this video, we embrace the concept of data leak and cover data)59.5 257.15 ms
( leaks from future picking, meta data, IDs, and row order.[SOUND)59.5 244.15 ms
(] Now, I will tell you)59.5 231.15 ms
(about a competition-specific technique tightly)59.5 218.15 ms
(connected with data leaks. It's leaderboard probing. There are a)59.5 205.15 ms
(ctually two types)59.5 192.15 ms
(of leaderboard probing. The first one is simply extracting)59.5 179.15 ms
(all ground truth from public part of the leaderboard. It's usual)59.5 166.15 ms
(ly pretty harmless,)59.5 153.15 ms
(only a little more of straining data. It is also a relatively ea)59.5 140.15 ms
(sy to do and I have a submission change on)59.5 127.15 ms
(the small set of rows so that you can unambiguously calculate gr)59.5 114.15 ms
(ound truth for)59.5 101.15 ms
(those rows from leaderboard score. I suggest checking out the li)59.5 88.15 ms
(nk to)59.5 75.15 ms
(Alek Trott's post in additional materials. He thoroughly explain)59.5 62.15 ms
(s how)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 37 37
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 37)59.5 790.15 ms
F0 sf
(to do it very efficiently with minimum amount of submissions. Ou)59.5 764.15 ms
(r main focus will be on another)59.5 751.15 ms
(type of leaderboard probing. Remember the purpose of public,)59.5 738.15 ms
(private split. It's supposed to protect private part of)59.5 725.15 ms
(test set from information extraction. It turns out that it's sti)59.5 712.15 ms
(ll vulnerable. Sometimes, it's possible to)59.5 699.15 ms
(submit predictions in such a way that will give out information)59.5 686.15 ms
(about private data. It's all about consistent categories. Imagin)59.5 673.15 ms
(e, a chunk of data with)59.5 660.15 ms
(the same target for every row. Like in the example, rows with)59.5 647.15 ms
(the same IDs have the same target. Organizers split it into publ)59.5 634.15 ms
(ic and)59.5 621.15 ms
(private parts. But we still know that that particular)59.5 608.15 ms
(chunk has the same label for every role. After setting all the p)59.5 595.15 ms
(redictions)59.5 582.15 ms
(close to 0 in our submission for that particular chunk of data,)59.5 569.15 ms
(we can expect two outcomes. The first one is when score improved)59.5 556.15 ms
(,)59.5 543.15 ms
(it means that ground truth in public is 0. And it also means tha)59.5 530.15 ms
(t ground)59.5 517.15 ms
(truth in private is 0 as well. Remember, our chunk has the same )59.5 504.15 ms
(labels. The second outcome is when)59.5 491.15 ms
(the score became worse. Similarly, it means that ground truth)59.5 478.15 ms
(in both public and private is 1. Some competitions indeed have)59.5 465.15 ms
(that kind of categories. Categories that with high)59.5 452.15 ms
(certainty have the same label. You could have encountered those)59.5 439.15 ms
(type of categories in Red Hat and West Nile competitions. It was)59.5 426.15 ms
( a key for winning. With a lot of submissions, one can)59.5 413.15 ms
(explore a good part of private test set. It's probably the most)59.5 400.15 ms
(annoying type of data leak. It's mostly technical and even if it)59.5 387.15 ms
('s)59.5 374.15 ms
(released close to the competition deadline, you simply won't hav)59.5 361.15 ms
(e enough)59.5 348.15 ms
(submissions to fully exploit it. Furthermore, this is on)59.5 335.15 ms
(the tip of the iceberg. When I say consistent category, I do not)59.5 322.15 ms
( necessarily mean that)59.5 309.15 ms
(this category has the same target. It could be consistent in dif)59.5 296.15 ms
(ferent ways. The definition is quite broad. For example, target )59.5 283.15 ms
(label could simply)59.5 270.15 ms
(have the same distribution for public and private parts of data.)59.5 257.15 ms
( It was the case in)59.5 244.15 ms
(Quora Question Pairs competition. In that competition there)59.5 231.15 ms
(was a binary classification task being evaluated by log loss met)59.5 218.15 ms
(ric. What's important target were able had)59.5 205.15 ms
(different distributions in train and test, but allegedly the sam)59.5 192.15 ms
(e and)59.5 179.15 ms
(private and public parts of these data. And because of that, we )59.5 166.15 ms
(could benefit)59.5 153.15 ms
(a lot via leaderboard probing. Treating the whole test set)59.5 140.15 ms
(as a consistent category. Take a look at the formula on the slid)59.5 127.15 ms
(e. This logarithmic loss for submission)59.5 114.15 ms
(with constant predictions C big. Where N big is the real number )59.5 101.15 ms
(of rows, N1 big is the number of)59.5 88.15 ms
(rows with target one. And L big is the leader board score)59.5 75.15 ms
(given by that constant prediction. From this equation,)59.5 62.15 ms
(we can calculate N1 divided by N or in other words,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 38 38
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 38)59.5 790.15 ms
F0 sf
(the true ratio of once in the test set. That knowledge was very )59.5 764.15 ms
(beneficial. We could use it rebalance)59.5 751.15 ms
(training data points to have the same distribution of)59.5 738.15 ms
(target variable as in the test set. This little trick gave a hug)59.5 725.15 ms
(e)59.5 712.15 ms
(boost in leaderboard score. As you can see, leaderboard)59.5 699.15 ms
(probing is a very serious problem that could occur under a lot)59.5 686.15 ms
(of different circumstances. I hope that someday it will become)59.5 673.15 ms
(complete the eradicated from competitive machine learning. Now, )59.5 660.15 ms
(finally, I like to briefly)59.5 647.15 ms
(walk through the most peculiar and interesting competitions)59.5 634.15 ms
(with data leakage. And first, let's take a look at Truly Native)59.5 621.15 ms
(competition from different point of view. In this competition, p)59.5 608.15 ms
(articipants were)59.5 595.15 ms
(asked to predict whether the content in an HTML file is sponsore)59.5 582.15 ms
(d or not. As was already discussed)59.5 569.15 ms
(in previous video, there was a data leak in archive dates. We ca)59.5 556.15 ms
(n assume that sponsored and non-sponsored HTML files were gotten)59.5 543.15 ms
(during different periods of time. So do we really get rid of dat)59.5 530.15 ms
(a)59.5 517.15 ms
(leak after erasing archive dates? The answer is no. Texts in HTM)59.5 504.15 ms
(L files may be connected)59.5 491.15 ms
(to dates in a lot of ways. From explicit timestamps to much more)59.5 478.15 ms
(subtle things, like news contents. As you've probably already re)59.5 465.15 ms
(alized, the real problem was not metadata leak,)59.5 452.15 ms
(but rather data collection. Even without metainformation, machin)59.5 439.15 ms
(e learning algorithms will)59.5 426.15 ms
(focus on actually useless features. The features that only act a)59.5 413.15 ms
(s proxies for)59.5 400.15 ms
(the date. The next example is)59.5 387.15 ms
(Expedia Hotel Recommendations, and that competitions, participan)59.5 374.15 ms
(ts)59.5 361.15 ms
(worked with logs of customer behavior. These include what custom)59.5 348.15 ms
(ers searched for,)59.5 335.15 ms
(how they interacted with search results, and clicks or books, an)59.5 322.15 ms
(d whether or not)59.5 309.15 ms
(the search result was a travel package. Expedia was interested i)59.5 296.15 ms
(n predicting which)59.5 283.15 ms
(hotel group a user is going to book. Within the logs of customer)59.5 270.15 ms
( behavior,)59.5 257.15 ms
(there was a very tricky feature. At distance from users)59.5 244.15 ms
(seeking their hotel. Turned out, that this feature)59.5 231.15 ms
(is actually a huge data leak. Using this distance, it was possib)59.5 218.15 ms
(le to)59.5 205.15 ms
(reverse engineer two coordinates, and simply map ground truth fr)59.5 192.15 ms
(om)59.5 179.15 ms
(train set to the test set. I strongly suggest you to)59.5 166.15 ms
(check out the special video dedicated to this competition. I hop)59.5 153.15 ms
(e that you will find it very)59.5 140.15 ms
(useful because the approaches and methods of exploiting data lea)59.5 127.15 ms
(k)59.5 114.15 ms
(were extremely nontrivial. And you will find a lot of)59.5 101.15 ms
(interesting tricks in it. The next example is from)59.5 88.15 ms
(Flavours of Physics competition. It was a pretty complicated)59.5 75.15 ms
(problem dealing with physics at Large Hadron Collider. The speci)59.5 62.15 ms
(al thing about)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 39 39
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 39)59.5 790.15 ms
F0 sf
(that competition was that signal was artificially simulated. Org)59.5 764.15 ms
(anizers wanted a machine)59.5 751.15 ms
(learning solution for something that has never been observed. Th)59.5 738.15 ms
(at's why the signal was simulated. But simulation cannot be perf)59.5 725.15 ms
(ect and)59.5 712.15 ms
(it's possible to reverse engineer it. Organizers even created)59.5 699.15 ms
(special statistical tests in order to punish the models)59.5 686.15 ms
(that exploit simulation flaws. However, it was in vain. One coul)59.5 673.15 ms
(d bypass the tests,)59.5 660.15 ms
(fully exploit simulation flaws, and get a perfect score on the l)59.5 647.15 ms
(eaderboard. The last example is going)59.5 634.15 ms
(to cover pairwise tasks. Where one needs to predict)59.5 621.15 ms
(whether the given pair of items are duplicates or not,)59.5 608.15 ms
(like in Quora question pairs competition. There is one thing com)59.5 595.15 ms
(mon to all)59.5 582.15 ms
(the competitions with pairwise tasks. Participants are not asked)59.5 569.15 ms
( to)59.5 556.15 ms
(evaluate all possible pairs. There is always some)59.5 543.15 ms
(nonrandom subsampling, and this subsampling is)59.5 530.15 ms
(the cause of data leakage. Usually, organizers sample mostly)59.5 517.15 ms
(hard-to-distinguish pairs. Because of that, of course,)59.5 504.15 ms
(imbalance in item frequencies. It results in more frequent)59.5 491.15 ms
(items having the higher possibility of being duplicates. But tha)59.5 478.15 ms
(t's not all. We can create a connectivity)59.5 465.15 ms
(matrix N times N, where N is the total number of items. If item )59.5 452.15 ms
(I and item J appeared)59.5 439.15 ms
(in a pair then we place 1 in I, J and J, I positions. Now, we ca)59.5 426.15 ms
(n treat the rows in connectivity)59.5 413.15 ms
(matrix as vector representations for every item. This means that)59.5 400.15 ms
( we can compute)59.5 387.15 ms
(similarities between those vectors. This tricks works for)59.5 374.15 ms
(a very simple reason. When two items have)59.5 361.15 ms
(similar sets of neighbors they have a high possibility)59.5 348.15 ms
(of being duplicates. This is it with data leaks. I hope you got )59.5 335.15 ms
(the concept and)59.5 322.15 ms
(found a lot of interesting examples. Thank you for your attentio)59.5 309.15 ms
(n. [SOUND]Hi, everyone. In this video, I will tell you how I and)59.5 296.15 ms
( my teammates, Stanislav Smirnov solved Kaggle Expedia hotel rec)59.5 283.15 ms
(ommendations competition. Personally, one of my favorites, proba)59.5 270.15 ms
(bly among top five most interesting competitions I've ever parti)59.5 257.15 ms
(cipated in. I'll state the problem now. So, if you came here rig)59.5 244.15 ms
(ht after Data Leaks lesson, it should already be familiar to you)59.5 231.15 ms
(. Anyway, in that competition, we worked with lots of customer b)59.5 218.15 ms
(ehavior. These include what customers searched for, how they int)59.5 205.15 ms
(eracted with search results, clicks or books, and whether or not)59.5 192.15 ms
( the search result was a travel package, and Expedia was interes)59.5 179.15 ms
(ted in predicting which hotel group a user is going to book. Imp)59.5 166.15 ms
(ortant thing here is prediction target the hotel group. In other)59.5 153.15 ms
( words, characteristics of actual hotel, remember it. As it turn)59.5 140.15 ms
(ed out, this competition had a very non-trivial and extremely ha)59.5 127.15 ms
(rd to exploit data leak. From the first glance, data leak was pr)59.5 114.15 ms
(etty straightforward. We had a destination distance among the fe)59.5 101.15 ms
(ature. It's a distance from user city to an actual hotel he clic)59.5 88.15 ms
(ked on booked. And, as I said earlier, our prediction target is )59.5 75.15 ms
(a characteristic of an actual hotel. Furthermore, destination di)59.5 62.15 ms
(stance was very precise so unique user city and destination dist)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 40 40
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 40)59.5 790.15 ms
F0 sf
(ance pairs corresponded to unique hotels. Putting two and two to)59.5 764.15 ms
(gether, we can treat user city and destination distance pair as )59.5 751.15 ms
(a proxy to our target. When in this set, we encountered such pai)59.5 738.15 ms
(r already present in train set, we could simply take a label fro)59.5 725.15 ms
(m there as our prediction. It worked nearly perfect for the pair)59.5 712.15 ms
(s present in both train and test. However, nearly half of test s)59.5 699.15 ms
(et consisted from new pairs without a match from train set. This)59.5 686.15 ms
( way we had to go deeper. But, how exactly can we improve our so)59.5 673.15 ms
(lution? Well, there are two different ways. First, one is to cre)59.5 660.15 ms
(ate current features on corteges similar to user city and destin)59.5 647.15 ms
(ation distance pair. For example, like how many hotels of which )59.5 634.15 ms
(group there are for user city, hotel country, hotel city triplet)59.5 621.15 ms
(. Then, we could train some machine learning model on such featu)59.5 608.15 ms
(res. Another way is to somehow find more matches. For that purpo)59.5 595.15 ms
(se, we need to find true coordinates of users cities and hotel c)59.5 582.15 ms
(ities. From that, to guess it was destination distance feature, )59.5 569.15 ms
(it was possible to find good approximation for the coordinates o)59.5 556.15 ms
(f actual hotels. Let's find out how to do it. First of all, we n)59.5 543.15 ms
(eed to understand how to calculate the distance. Here, we work w)59.5 530.15 ms
(ith geographical coordinates so the distances are geodesic. It's)59.5 517.15 ms
( done via Haversine formula, not a pleasant one. Now, suppose th)59.5 504.15 ms
(at we know true coordinates of three points and distances from f)59.5 491.15 ms
(ourth point with unknown coordinates to each of them, if you wri)59.5 478.15 ms
(te down a system of three equations, one for each distance, we c)59.5 465.15 ms
(an unambiguously solve it and get true coordinates for the fourt)59.5 452.15 ms
(h point. Now, we have four points with known coordinates. I thin)59.5 439.15 ms
(k you get the idea. So, at first, by hook or by crook, we revers)59.5 426.15 ms
(e engineer true coordinate of three big cities. After that, we c)59.5 413.15 ms
(an iteratively find coordinates of more and more cities. But as )59.5 400.15 ms
(you can see from the picture, some cities ended up in oceans. It)59.5 387.15 ms
( means that our algorithm is not very precise. A rounding error )59.5 374.15 ms
(accumulates after every iteration and everything starts to fall )59.5 361.15 ms
(apart. We get some different method and indeed we can do better.)59.5 348.15 ms
( Just compare this picture with the previous one. It's obviously)59.5 335.15 ms
( much more accurate. Remember how in iterative method we solved )59.5 322.15 ms
(a system of three equations to unambiguously find coordinates or)59.5 309.15 ms
( fourth unknown point. But why limit ourselves with three equati)59.5 296.15 ms
(ons? Let's create a giant system of equations from all known dis)59.5 283.15 ms
(tances with true coordinates being done on variables. We end up )59.5 270.15 ms
(with literally hundreds or thousands of equations and tens of th)59.5 257.15 ms
(ousands of unknown variables. Good thing it's very sparse. We ca)59.5 244.15 ms
(n apply special methods from SciPy to efficiently solve such a s)59.5 231.15 ms
(ystem. In the end, after solving that system of equations, we en)59.5 218.15 ms
(d up with a very precise coordinates for both hotel cities and u)59.5 205.15 ms
(ser cities. But as you remember, we're predicting a type of a ho)59.5 192.15 ms
(tel. Using city coordinates and destination distance, it's possi)59.5 179.15 ms
(ble to find an approximation of true coordinates of an actual ho)59.5 166.15 ms
(tel. When we fix user city and draw a circumference around it wi)59.5 153.15 ms
(th the radius of destination distance, it's obvious that true ho)59.5 140.15 ms
(tel location must be somewhere on that circumference. Now, let's)59.5 127.15 ms
( fix some hotel city and draw such circumferences from all users)59.5 114.15 ms
( cities to that fixed hotel cities and draw them for every given)59.5 101.15 ms
( destination distance. After doing so, we end up with pictures l)59.5 88.15 ms
(ike the ones on the slide. A city contains a limited number of h)59.5 75.15 ms
(otels so the intuition here is that hotels actually are on the i)59.5 62.15 ms
(ntersection points and the more circumferences intersect in such)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 41 41
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week2_win_kaggle.txt                                     Page 41)59.5 790.15 ms
F0 sf
( point, the higher the probability of a hotel being in that poin)59.5 764.15 ms
(t. As you can see, the pictures are beautiful but pretty messy. )59.5 751.15 ms
(It's impossible to operate in terms of singular points. However,)59.5 738.15 ms
( there are explicit clusters of points and this information can )59.5 725.15 ms
(be of use. We can do some kind of integration. For every city, l)59.5 712.15 ms
(et's create a grid around its center. Something like 10 kilomete)59.5 699.15 ms
(rs times 10 kilometers with step size of 100 meters. Now, using )59.5 686.15 ms
(training data, for every cell in the grid, we can count how many)59.5 673.15 ms
( hotels of which type are present there. If a circumference goes)59.5 660.15 ms
( through a cell, we give plus one to the hotel type correspondin)59.5 647.15 ms
(g to that circumference. During inference, we also draw a circum)59.5 634.15 ms
(ference based on destination distance feature. We see from what )59.5 621.15 ms
(degree its cells it went through and use information from those )59.5 608.15 ms
(cells to create features like a sum of all counters, average of )59.5 595.15 ms
(all counters, maximum of all counters and so on. Great. We have )59.5 582.15 ms
(covered the part of feature engineering. Note that all the featu)59.5 569.15 ms
(res directly used target label. We cannot use them as is in trai)59.5 556.15 ms
(ning. We should generate them in out-of-fold fashion for train d)59.5 543.15 ms
(ata. So we had training data for years 2013 and 2014. To generat)59.5 530.15 ms
(e features for year 2014, we used labelled data from year 2013 a)59.5 517.15 ms
(nd vice versa, used the year 2014 to generate features for the y)59.5 504.15 ms
(ear 2013. For the test features, which was from year 2015, we na)59.5 491.15 ms
(turally used all training data. In the end, we calculated a lot )59.5 478.15 ms
(of features and serve them into Xgboost model. After 16 hours of)59.5 465.15 ms
( training for the course, we got our results. We ended up on thi)59.5 452.15 ms
(rd position on public leader-boards and forth on private. We did)59.5 439.15 ms
( good, but we still did not fully exploit data leakage. If you c)59.5 426.15 ms
(heck the leaderboard, you'll notice the difference in scores bet)59.5 413.15 ms
(ween first place and the rest. Under speculation, the winner did)59.5 400.15 ms
( extraordinary. Although, in general, his methods were very simi)59.5 387.15 ms
(lar to ours. He was able to extract way more signal. Finally, I )59.5 374.15 ms
(hope you enjoyed my story. As you can see, sometimes working wit)59.5 361.15 ms
(h data leakage could be very interesting and challenging. You ma)59.5 348.15 ms
(y develop some unusual skills and broaden your horizons. Thank y)59.5 335.15 ms
(ou for your attention.)59.5 322.15 ms
re sp
%%PageTrailer
%%Trailer
%%Pages: 41
%%EOF
