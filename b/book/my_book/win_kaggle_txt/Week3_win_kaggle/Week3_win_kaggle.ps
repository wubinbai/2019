%!PS-Adobe-3.0
%%Title: Week3_win_kaggle.txt
%%For: wb
%%Creator: VIM - Vi IMproved 8.0 (2016 Sep 12)
%%CreationDate: Thu Dec 19 08:59:25 2019
%%DocumentData: Clean8Bit
%%Orientation: Portrait
%%Pages: (atend)
%%PageOrder: Ascend
%%BoundingBox: 59 45 559 800
%%DocumentMedia: A4 595 842 0 () ()
%%DocumentNeededResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%DocumentSuppliedResources: procset VIM-Prolog 1.4 1
%%+ encoding VIM-latin1 1.0 0
%%Requirements: duplex collate
%%EndComments
%%BeginDefaults
%%PageResources: font Courier
%%+ font Courier-Bold
%%+ font Courier-Oblique
%%+ font Courier-BoldOblique
%%PageMedia: A4
%%EndDefaults
%%BeginProlog
%%BeginResource: procset VIM-Prolog
%%BeginDocument: /usr/share/vim/vim80/print/prolog.ps
%!PS-Adobe-3.0 Resource-ProcSet
%%Title: VIM-Prolog
%%Version: 1.4 1
%%EndComments
% Editing of this file is NOT RECOMMENDED.  You run a very good risk of causing
% all PostScript printing from VIM failing if you do.  PostScript is not called
% a write-only language for nothing!
/packedarray where not{userdict begin/setpacking/pop load def/currentpacking
false def end}{pop}ifelse/CP currentpacking def true setpacking
/bd{bind def}bind def/ld{load def}bd/ed{exch def}bd/d/def ld
/db{dict begin}bd/cde{currentdict end}bd
/T true d/F false d
/SO null d/sv{/SO save d}bd/re{SO restore}bd
/L2 systemdict/languagelevel 2 copy known{get exec}{pop pop 1}ifelse 2 ge d
/m/moveto ld/s/show ld /ms{m s}bd /g/setgray ld/r/setrgbcolor ld/sp{showpage}bd
/gs/gsave ld/gr/grestore ld/cp/currentpoint ld
/ul{gs UW setlinewidth cp UO add 2 copy newpath m 3 1 roll add exch lineto
stroke gr}bd
/bg{gs r cp BO add 4 -2 roll rectfill gr}bd
/sl{90 rotate 0 exch translate}bd
L2{
/sspd{mark exch{setpagedevice}stopped cleartomark}bd
/nc{1 db/NumCopies ed cde sspd}bd
/sps{3 db/Orientation ed[3 1 roll]/PageSize ed/ImagingBBox null d cde sspd}bd
/dt{2 db/Tumble ed/Duplex ed cde sspd}bd
/c{1 db/Collate ed cde sspd}bd
}{
/nc{/#copies ed}bd
/sps{statusdict/setpage get exec}bd
/dt{statusdict/settumble 2 copy known{get exec}{pop pop pop}ifelse
statusdict/setduplexmode 2 copy known{get exec}{pop pop pop}ifelse}bd
/c{pop}bd
}ifelse
/ffs{findfont exch scalefont d}bd/sf{setfont}bd
/ref{1 db findfont dup maxlength dict/NFD ed{exch dup/FID ne{exch NFD 3 1 roll
put}{pop pop}ifelse}forall/Encoding findresource dup length 256 eq{NFD/Encoding
3 -1 roll put}{pop}ifelse NFD dup/FontType get 3 ne{/CharStrings}{/CharProcs}
ifelse 2 copy known{2 copy get dup maxlength dict copy[/questiondown/space]{2
copy known{2 copy get 2 index/.notdef 3 -1 roll put pop exit}if pop}forall put
}{pop pop}ifelse dup NFD/FontName 3 -1 roll put NFD definefont pop end}bd
CP setpacking
(\004)cvn{}bd
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%BeginResource: encoding VIM-latin1
%%BeginDocument: /usr/share/vim/vim80/print/latin1.ps
%!PS-Adobe-3.0 Resource-Encoding
%%Title: VIM-latin1
%%Version: 1.0 0
%%EndComments
/VIM-latin1[
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclam /quotedbl /numbersign /dollar /percent /ampersand /quotesingle
/parenleft /parenright /asterisk /plus /comma /minus /period /slash
/zero /one /two /three /four /five /six /seven
/eight /nine /colon /semicolon /less /equal /greater /question
/at /A /B /C /D /E /F /G
/H /I /J /K /L /M /N /O
/P /Q /R /S /T /U /V /W
/X /Y /Z /bracketleft /backslash /bracketright /asciicircum /underscore
/grave /a /b /c /d /e /f /g
/h /i /j /k /l /m /n /o
/p /q /r /s /t /u /v /w
/x /y /z /braceleft /bar /braceright /asciitilde /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef /.notdef
/space /exclamdown /cent /sterling /currency /yen /brokenbar /section
/dieresis /copyright /ordfeminine /guillemotleft /logicalnot /hyphen /registered /macron
/degree /plusminus /twosuperior /threesuperior /acute /mu /paragraph /periodcentered
/cedilla /onesuperior /ordmasculine /guillemotright /onequarter /onehalf /threequarters /questiondown
/Agrave /Aacute /Acircumflex /Atilde /Adieresis /Aring /AE /Ccedilla
/Egrave /Eacute /Ecircumflex /Edieresis /Igrave /Iacute /Icircumflex /Idieresis
/Eth /Ntilde /Ograve /Oacute /Ocircumflex /Otilde /Odieresis /multiply
/Oslash /Ugrave /Uacute /Ucircumflex /Udieresis /Yacute /Thorn /germandbls
/agrave /aacute /acircumflex /atilde /adieresis /aring /ae /ccedilla
/egrave /eacute /ecircumflex /edieresis /igrave /iacute /icircumflex /idieresis
/eth /ntilde /ograve /oacute /ocircumflex /otilde /odieresis /divide
/oslash /ugrave /uacute /ucircumflex /udieresis /yacute /thorn /ydieresis]
/Encoding defineresource pop
% vim:ff=unix:
%%EOF
%%EndDocument
%%EndResource
%%EndProlog
%%BeginSetup
595 842 0 sps
1 nc
T F dt
T c
%%IncludeResource: font Courier
/_F0 /VIM-latin1 /Courier ref
/F0 13 /_F0 ffs
%%IncludeResource: font Courier-Bold
/_F1 /VIM-latin1 /Courier-Bold ref
/F1 13 /_F1 ffs
%%IncludeResource: font Courier-Oblique
/_F2 /VIM-latin1 /Courier-Oblique ref
/F2 13 /_F2 ffs
%%IncludeResource: font Courier-BoldOblique
/_F3 /VIM-latin1 /Courier-BoldOblique ref
/F3 13 /_F3 ffs
/UO -1.3 d
/UW 0.65 d
/BO -3.25 d
%%EndSetup
%%Page: 1 1
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                      Page 1)59.5 790.15 ms
F0 sf
([MUSIC] Hi, in this lesson, we will talk about)59.5 764.15 ms
(a major part of any competition. The metrics that are used)59.5 751.15 ms
(to evaluate a solution. In this video, we'll discuss why)59.5 738.15 ms
(there are so many metrics and why it is necessary to know what)59.5 725.15 ms
(metric is used in a competition. In the following videos, we wil)59.5 712.15 ms
(l study what is the difference)59.5 699.15 ms
(between a loss and a metric? And we'll overview and show optimiz)59.5 686.15 ms
(ation)59.5 673.15 ms
(techniques for the most important and common metrics. In the cou)59.5 660.15 ms
(rse, we focus on regression and)59.5 647.15 ms
(classification. So we only discuss metric for these tasks. For b)59.5 634.15 ms
(etter understanding, we will also)59.5 621.15 ms
(build a simple baseline for each metric. That is what the best c)59.5 608.15 ms
(onstant to)59.5 595.15 ms
(predict for that particular method. So metrics are an essential)59.5 582.15 ms
(part of any competition. They are used to evaluate our submissio)59.5 569.15 ms
(ns. Okay, but why do we have a different)59.5 556.15 ms
(evolution metric on each competition? That is because there are )59.5 543.15 ms
(plenty of ways)59.5 530.15 ms
(to measure equality of an algorithm and each company decides for)59.5 517.15 ms
( themselves what is the most appropriate)59.5 504.15 ms
(way for their particular problem. For example, let's say an onli)59.5 491.15 ms
(ne shop is trying to)59.5 478.15 ms
(maximize effectiveness of their website. The thing is you need t)59.5 465.15 ms
(o)59.5 452.15 ms
(formalize what is effectiveness. You need to define a metric)59.5 439.15 ms
(how effectiveness is measured. It can be a number of times)59.5 426.15 ms
(a website was visited, or the number of times something)59.5 413.15 ms
(was ordered using this website. So the company usually decides f)59.5 400.15 ms
(or)59.5 387.15 ms
(itself what quantity is most important for it and)59.5 374.15 ms
(then tries to optimize it. In the competitions, the metrics)59.5 361.15 ms
(is fixed for us and the models and competitors are ranked using )59.5 348.15 ms
(it. In order to get higher leader board score)59.5 335.15 ms
(you need to get a better metric score. That's basically the only)59.5 322.15 ms
( thing in the)59.5 309.15 ms
(competition that we need to care about, how to get a better scor)59.5 296.15 ms
(e. And so it is very important to)59.5 283.15 ms
(understand how metric works and how to optimize it efficiently. )59.5 270.15 ms
(I want to stress out that)59.5 257.15 ms
(it is really important to optimize exactly the metric we're give)59.5 244.15 ms
(n in)59.5 231.15 ms
(the competition and not any other metric. Consider an example, b)59.5 218.15 ms
(lue and red lines represent objects of)59.5 205.15 ms
(a class zero and one respectively. And say we decided to use)59.5 192.15 ms
(a linear classifier,and came up with two matrix to optimize, M1 )59.5 179.15 ms
(and M2. The question is, how much different)59.5 166.15 ms
(the resulting classifiers would be? Actually by a lot. The two l)59.5 153.15 ms
(ines here, the solid and the dashed one show the best line)59.5 140.15 ms
(your boundaries for the two cases. For the dashed, M1 score is t)59.5 127.15 ms
(he highest)59.5 114.15 ms
(among all possible hyperplanes. But M2 score for the hyperplane )59.5 101.15 ms
(is low. And we have an opposite situation for)59.5 88.15 ms
(the solid boundary. M2 score is the highest,)59.5 75.15 ms
(whereas M1 score is low. Now, if we know that in this particular)59.5 62.15 ms
(competition, the ranking is based on M1 score, then we need to o)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 2 2
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                      Page 2)59.5 790.15 ms
F0 sf
(ptimize M1 score)59.5 764.15 ms
(and so we should submit the prediction. Predictions of the model)59.5 751.15 ms
(with dash boundary. Once again,)59.5 738.15 ms
(if your model is scored with some metric, you get best results b)59.5 725.15 ms
(y)59.5 712.15 ms
(optimizing exactly that metric. Now, the biggest problem is that)59.5 699.15 ms
( some)59.5 686.15 ms
(metrics cannot be optimized efficiently. That is there is no sim)59.5 673.15 ms
(ple enough way)59.5 660.15 ms
(to find, say, the optimal hyperplane. That is why sometimes we n)59.5 647.15 ms
(eed to)59.5 634.15 ms
(train our model to optimize something different than competition)59.5 621.15 ms
( metric. But in this case we will need to)59.5 608.15 ms
(apply various heuristics to improve competition metric score. An)59.5 595.15 ms
(d there's another case where we)59.5 582.15 ms
(need to be smart about the metrics. It is one that train and)59.5 569.15 ms
(the test sets are different. In the lesson about leaks,)59.5 556.15 ms
(we'll discuss leader board probing. That is, we can check, for e)59.5 543.15 ms
(xample, if the mean target value on public part)59.5 530.15 ms
(of test set is the same as on train. If it's not, we would need )59.5 517.15 ms
(to adapt our)59.5 504.15 ms
(predictions to suit rest set better. This is basically a specifi)59.5 491.15 ms
(c metric)59.5 478.15 ms
(optimization technique we apply, because train and test are diff)59.5 465.15 ms
(erent. Or there can be more severe cases)59.5 452.15 ms
(where improved metric validation set could possibly not result i)59.5 439.15 ms
(nto)59.5 426.15 ms
(improved metric on the test set. In these situations,)59.5 413.15 ms
(it's a good idea to stop and think maybe there is a different)59.5 400.15 ms
(way to approach the problem. In particular, time series can)59.5 387.15 ms
(be very challenging to forecast. Even if you did a validation ju)59.5 374.15 ms
(st right. [INAUDIBLE] by time, rolling windows,)59.5 361.15 ms
(fill the distribution in the future can be much different)59.5 348.15 ms
(from what we had in the train set. Or sometimes,)59.5 335.15 ms
(there's just not enough training data, so a model cannot capture)59.5 322.15 ms
( the patterns. In one of the compositions I took part, I had to )59.5 309.15 ms
(use some tricks to boost)59.5 296.15 ms
(my score after the modeling. And the trick was as a consequence)59.5 283.15 ms
(of a particular metric used in that competition. The metric was )59.5 270.15 ms
(quite unusual actually,)59.5 257.15 ms
(but it is intuitive. If a trend is guessed correctly, then the)59.5 244.15 ms
(absolute difference between the prediction and the target is con)59.5 231.15 ms
(sidered as an error. If for instance, model predict end)59.5 218.15 ms
(value in the prediction horizon to be higher than the last value)59.5 205.15 ms
( from the train)59.5 192.15 ms
(side but in reality it is lower, then the trend is predicted inc)59.5 179.15 ms
(orrectly,)59.5 166.15 ms
(and the error was set to)59.5 153.15 ms
(absolute difference squared. So if we predict a value to be)59.5 140.15 ms
(above the dashline, but it turns out to be below or vice versa, )59.5 127.15 ms
(the trend)59.5 114.15 ms
([INAUDIBLE] to be predicted incorrectly. So this metric carries )59.5 101.15 ms
(a lot more about correct trend to be predicted than)59.5 88.15 ms
(about actual value you predict. And that is something it)59.5 75.15 ms
(was possible to exploit. There were several times)59.5 62.15 ms
(series was to forecast, the horizon to predict was wrong, and)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 3 3
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                      Page 3)59.5 790.15 ms
F0 sf
(the model's predictions were unreliable. Moreover, it was not po)59.5 764.15 ms
(ssible)59.5 751.15 ms
(to optimize this metric exactly. So I realized that it would be )59.5 738.15 ms
(much better)59.5 725.15 ms
(to set all the predictions to either last value plus a very tiny)59.5 712.15 ms
( constant,)59.5 699.15 ms
(or last value minus very tiny constant. The same value for all t)59.5 686.15 ms
(he points in)59.5 673.15 ms
(the time interval, we are to predict for each time series. And d)59.5 660.15 ms
(esign depends on the estimation. What is more likely the values)59.5 647.15 ms
(in the horizon to be lower than the last known value,)59.5 634.15 ms
(or to be higher? This trick actually took me to)59.5 621.15 ms
(the first place in that competition. So finding a nice way to op)59.5 608.15 ms
(timize)59.5 595.15 ms
(a metric can give you an advantage over other participants,)59.5 582.15 ms
(especially if the metric is peculiar. So maybe I should formulat)59.5 569.15 ms
(e it like that. We should not forget to do kind of)59.5 556.15 ms
(exploratory metric analysis along with exploratory data analysis)59.5 543.15 ms
(. At least when the metric)59.5 530.15 ms
(is an unusual one. So in this video we've understood)59.5 517.15 ms
(that each business has its own way to measure ineffectiveness)59.5 504.15 ms
(of an algorithm based on its needs, and therefore, there are so)59.5 491.15 ms
(many different metrics. And we saw two motivational examples. Wh)59.5 478.15 ms
(y should we care about the metrics? Well, basically because it i)59.5 465.15 ms
(s how)59.5 452.15 ms
(competitors are compared to each other. In the following videos )59.5 439.15 ms
(we'll)59.5 426.15 ms
(talk about concrete metrics. We'll first discuss high level)59.5 413.15 ms
(intuition for each metric and then talk about optimization techn)59.5 400.15 ms
(iques. [MUSIC]In this video, we will review the most common rank)59.5 387.15 ms
(ing metrics and establish an intuition about them. Although in a)59.5 374.15 ms
( competition, the metric is fixed for us, it is still useful to )59.5 361.15 ms
(understand in what cases one metric could be preferred to anothe)59.5 348.15 ms
(r. In this course, we concentrate on regression and classificati)59.5 335.15 ms
(on, so we will only discuss related metrics. For a better unders)59.5 322.15 ms
(tanding, for each metric, we will also build the most simple bas)59.5 309.15 ms
(eline we could imagine, the constant model. That is, if we are o)59.5 296.15 ms
(nly allowed to predict the same value for every object, what val)59.5 283.15 ms
(ue is optimal to predict according to the chosen metric? Let's s)59.5 270.15 ms
(tart with regression task and related metrics. In the following )59.5 257.15 ms
(videos, we'll talk about metrics for classification. First, let )59.5 244.15 ms
(us clarify the notation we're going to use throughout the lesson)59.5 231.15 ms
(. N will be the number of samples in our training data set, y is)59.5 218.15 ms
( that the target, and y-hat is our model's predictions. And y-ha)59.5 205.15 ms
(t and y with index i are the predictions, and target value respe)59.5 192.15 ms
(ctively for i-th object. The first metric we will discuss is Mea)59.5 179.15 ms
(n Square Error. It is for sure the most common metric for regres)59.5 166.15 ms
(sion type of problems. In data science, people use it when they )59.5 153.15 ms
(don't have any specific preferences for the solution to their pr)59.5 140.15 ms
(oblem, or when they don't know other metric. MSE basically measu)59.5 127.15 ms
(res average squared error of our predictions. For each point, we)59.5 114.15 ms
( calculate square difference between the predictions of the targ)59.5 101.15 ms
(et and then average those values over the objects. Let's introdu)59.5 88.15 ms
(ce a simple data set now. Say, we have five objects, and each ob)59.5 75.15 ms
(ject has some features, X, and the target is shown in the column)59.5 62.15 ms
( Y. Let's ask ourselves a question. How will the error change if)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 4 4
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                      Page 4)59.5 790.15 ms
F0 sf
( we fix all the predictions but want to be perfect, and we'll de)59.5 764.15 ms
(rive the value of the remaining one? To answer this question, ta)59.5 751.15 ms
(ke a look at this plot. On the horizontal line, we will first pu)59.5 738.15 ms
(t points to the positions of the target values. The points are c)59.5 725.15 ms
(olored according to the corresponding rows in our data table. An)59.5 712.15 ms
(d on the Y-axis, we will show the mean square error. So, let's n)59.5 699.15 ms
(ow assume that our predictions for the first four objects are pe)59.5 686.15 ms
(rfect, and let's draw a curve. How the metric value will change )59.5 673.15 ms
(if we change the prediction for the last object? For MSE metric,)59.5 660.15 ms
( it looks like that. In fact, if we predict 25, the error is zer)59.5 647.15 ms
(o, and if we predict something else, then it is greater than zer)59.5 634.15 ms
(o. And the error curve looks like parabola. Let's now draw analo)59.5 621.15 ms
(gous curves for other objects. Well, right now it's hard to make)59.5 608.15 ms
( any conclusions but we will build the same kind of plot for eve)59.5 595.15 ms
(ry metric and we will note the difference between them. Now, let)59.5 582.15 ms
('s build the simplest baseline model. We'll not use the features)59.5 569.15 ms
( X at all and we will always predict a constant value Alpha. But)59.5 556.15 ms
(, what is the optimal constant? What constant minimizes the mean)59.5 543.15 ms
( square error for our data set? In fact, it is easier to set the)59.5 530.15 ms
( derivative of our total error with respect to that constant to )59.5 517.15 ms
(zero, and find it from this equation. What we'll find is that th)59.5 504.15 ms
(e best constant is the mean value of the target column. If you t)59.5 491.15 ms
(hink you don't know how to derive it, take a look at the reading)59.5 478.15 ms
( materials. There is a fine explanation and links to related boo)59.5 465.15 ms
(ks. But let us constructively check it. Once again, on the horiz)59.5 452.15 ms
(ontal axis, let's denote our target values with dot and draw a f)59.5 439.15 ms
(unction. How the error changes is if we change the value of that)59.5 426.15 ms
( constant Alpha? We can do it with a simple grid search over a g)59.5 413.15 ms
(iven range by changing Alpha intuitively and recomputing an erro)59.5 400.15 ms
(r. Now, the green square shows a minimum value for our metric. T)59.5 387.15 ms
(he constant we found is 10.99, and it's quite close to the true )59.5 374.15 ms
(mean of the target which is 11. In fact, the value we got deviat)59.5 361.15 ms
(es from the true mean value only because with the grid search, w)59.5 348.15 ms
(e get only approximate answer. Also note that the red curve on t)59.5 335.15 ms
(he second plot is uniformly same and average of the curves from )59.5 322.15 ms
(the first plot. We are finished discussing MSE metric itself, bu)59.5 309.15 ms
(t there are two more related metrics used frequently, RMSE and R)59.5 296.15 ms
(_squared. And we will briefly study them now. RMSE, Root Mean Sq)59.5 283.15 ms
(uare Error, is a very similar metric to MSE. In fact, it is calc)59.5 270.15 ms
(ulated in two steps. First, we calculate regular mean square err)59.5 257.15 ms
(or and then, we take a square root of it. The square root is int)59.5 244.15 ms
(roduced to make scale of the errors to be the same as the scale )59.5 231.15 ms
(of the targets. For MSE, the error is squared, so taking a root )59.5 218.15 ms
(out of it makes total error a little bit easier to comprehend be)59.5 205.15 ms
(cause it is linear now. Now, it is very important to understand )59.5 192.15 ms
(in what sense RMSE is similar to MSE, and what is the difference)59.5 179.15 ms
(. First, they are similar in terms of their minimizers. Every mi)59.5 166.15 ms
(nimizer of MSE is a minimizer of RMSE and vice versa. But genera)59.5 153.15 ms
(lly, if we have two sets of predictions, A and B, and say MSE of)59.5 140.15 ms
( A is greater than MSE of B, then we can be sure that RMSE of A )59.5 127.15 ms
(is greater RMSE of B. And it also works in the opposite directio)59.5 114.15 ms
(n. This is actually true only because square root function is no)59.5 101.15 ms
(n-decreasing. What does it mean for us? It means that, if our ta)59.5 88.15 ms
(rget the metric is RMSE, we still can compare our models using M)59.5 75.15 ms
(SE, since MSE will order the models in the same way as RMSE. And)59.5 62.15 ms
( we can optimize MSE instead of RMSE. In fact, MSE is a little b)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 5 5
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                      Page 5)59.5 790.15 ms
F0 sf
(it easier to work with, so everybody uses MSE instead of RMSE. B)59.5 764.15 ms
(ut there is a little bit of difference between the two for gradi)59.5 751.15 ms
(ent-based models. Take a look at the gradient of RMSE with respe)59.5 738.15 ms
(ct to i-th prediction. It is basically equal to gradient of MSE )59.5 725.15 ms
(multiplied by some value. The value doesn't depend on the index )59.5 712.15 ms
(I. It means that travelling along MSE gradient is equivalent to )59.5 699.15 ms
(traveling along RMSE gradient but with a different flowing rate )59.5 686.15 ms
(and the flowing rate depends on MSE score itself. So, it is kind)59.5 673.15 ms
( of dynamic. So even though RMSE and MSE are really similar in t)59.5 660.15 ms
(erms of models scoring, they can be not immediately interchangea)59.5 647.15 ms
(ble for gradient based methods. We will probably need to adjust )59.5 634.15 ms
(some parameters like the learning rate. Now, what if I told you )59.5 621.15 ms
(that MSE for my models predictions is 32? Should I improve my mo)59.5 608.15 ms
(del or is it good enough? Or what if my MSE was 0.4? Actually, i)59.5 595.15 ms
(t's hard to realize if our model is good or not by looking at th)59.5 582.15 ms
(e absolute values of MSE or RMSE. It really depends on the prope)59.5 569.15 ms
(rties of the dataset and their target vector. How much variation)59.5 556.15 ms
( is there in the target vector. We would probably want to measur)59.5 543.15 ms
(e how much our model is better than the constant baseline. And s)59.5 530.15 ms
(ay, the desired metrics should give us zero if we are no better )59.5 517.15 ms
(than the baseline and one if the predictions are perfect. For th)59.5 504.15 ms
(at purpose, R_squared metric is usually used. Take a look. When )59.5 491.15 ms
(MSE of our predictions is zero, the R_squared is 1, and when our)59.5 478.15 ms
( MSE is equal to MSE over constant model, then R_squared is zero)59.5 465.15 ms
(. Well, because the values in numerator and denominator are the )59.5 452.15 ms
(same. And all reasonable models will score between 0 and 1. The )59.5 439.15 ms
(most important thing for us is that to optimize R_squared, we ca)59.5 426.15 ms
(n optimize MSE. It will be absolutely equivalent since R_squared)59.5 413.15 ms
( is basically MSE score divided by a constant and subtracted fro)59.5 400.15 ms
(m another constant. These constants doesn't matter for optimizat)59.5 387.15 ms
(ion. Lets move on and discuss another metric called Mean Absolut)59.5 374.15 ms
(e Error, or MAE in short. The error is calculated as an average )59.5 361.15 ms
(of absolute differences between the target values and the predic)59.5 348.15 ms
(tions. What is important about this metric is that it penalizes )59.5 335.15 ms
(huge errors that not as that badly as MSE does. Thus, it's not t)59.5 322.15 ms
(hat sensitive to outliers as mean square error. It also has a li)59.5 309.15 ms
(ttle bit different applications than MSE. MAE is widely used in )59.5 296.15 ms
(finance, where $10 error is usually exactly two times worse than)59.5 283.15 ms
( $5 error. On the other hand, MSE metric thinks that $10 error i)59.5 270.15 ms
(s four times worse than $5 error. MAE is easier to justify. And )59.5 257.15 ms
(if you used RMSE, it would become really hard to explain to your)59.5 244.15 ms
( boss how you evaluated your model. What constant is optimal for)59.5 231.15 ms
( MAE? It's quite easy to find that its a median of the target va)59.5 218.15 ms
(lues. In this case, it is eight. See reading materials for a pro)59.5 205.15 ms
(of. Just to verify that everything is correct, we again can try )59.5 192.15 ms
(to Greek search for an optimal value with a simple loop. And in )59.5 179.15 ms
(fact, the value we found is 7.98, which indicates we were right.)59.5 166.15 ms
( Here, we see that MAE is more robust than MSE, that is, it is n)59.5 153.15 ms
(ot that influenced by the outliers. In fact, recall that the opt)59.5 140.15 ms
(imal constant for MSE was about 11 while for MAE it is eight. An)59.5 127.15 ms
(d eight looks like a much better prediction for the points on th)59.5 114.15 ms
(e left side. If we assume that point with a target 27 is an outl)59.5 101.15 ms
(ier and we should not care about the prediction for it. Another )59.5 88.15 ms
(important thing about MAE is its gradients with respect to the p)59.5 75.15 ms
(redictions. The grid end is a step function and it takes -1 when)59.5 62.15 ms
( Y_hat is smaller than the target and +1 when it is larger. Now,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 6 6
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                      Page 6)59.5 790.15 ms
F0 sf
( the gradient is not defined when the prediction is perfect, bec)59.5 764.15 ms
(ause when Y_hat is equal to Y, we can not evaluate gradient. It )59.5 751.15 ms
(is not defined. So formally, MAE is not differentiable, but in f)59.5 738.15 ms
(act, how often your predictions perfectly measure the target. Ev)59.5 725.15 ms
(en if they do, we can write a simple IF condition and return zer)59.5 712.15 ms
(o when it is the case and through gradient otherwise. Also know )59.5 699.15 ms
(that second derivative is zero everywhere and not defined in the)59.5 686.15 ms
( point zero. I want to end the discussion with the last note. We)59.5 673.15 ms
(ll, it has nothing to do with competitions but every data scient)59.5 660.15 ms
(ists should understand this. We said that MAE is more robust tha)59.5 647.15 ms
(n MSE. That is, it is less sensitive to outliers, but it doesnt )59.5 634.15 ms
(mean it is always better to use MAE. No, it does not. It is basi)59.5 621.15 ms
(cally a question. Are there any real outliers in the dataset or )59.5 608.15 ms
(there are just, let's say, unexpectedly high values that we shou)59.5 595.15 ms
(ld treat just as others? Outliers have usually mistakes, measure)59.5 582.15 ms
(ment errors, and so on, but at the same time, similarly looking )59.5 569.15 ms
(objects can be of natural kind. So, if you think these unusual o)59.5 556.15 ms
(bjects are normal in the sense that they're just rare, you shoul)59.5 543.15 ms
(d not use a metric which will ignore them. And it is better to u)59.5 530.15 ms
(se MSE. Otherwise, if you think that they are really outliers, l)59.5 517.15 ms
(ike mistakes, you should use MAE. So in this video, we have disc)59.5 504.15 ms
(ussed several important metrics. We first discussed, mean square)59.5 491.15 ms
( error and realized that the best constant for it is the mean ta)59.5 478.15 ms
(rgeted value. Root Mean Square Error, RMSE, and R_squared are ve)59.5 465.15 ms
(ry similar to MSE from optimization perspective. We then discuss)59.5 452.15 ms
(ed Mean Absolute Error and when people prefer to use MAE over MS)59.5 439.15 ms
(E. In the next video, we will continue to study regression metri)59.5 426.15 ms
(cs and then we'll get to classification ones.[SOUND] In the prev)59.5 413.15 ms
(ious video,)59.5 400.15 ms
(we started to discuss regression metrics. In this video,)59.5 387.15 ms
(we'll talk about three more metrics, \(R\)MSPE, MAPE, and \(R\)MSLE.)59.5 374.15 ms
( Think about the following problem. We need to predict,)59.5 361.15 ms
(how many laptops two shops will sell? And in the train set for)59.5 348.15 ms
(a particular date, we see that the first shop sold 10 items, and)59.5 335.15 ms
(the second sold 1,000 items. Now suppose our model predicts)59.5 322.15 ms
(9 items instead of 10 for the first shop, and)59.5 309.15 ms
(999 instead of 1,000 for the second. It could happen that off by)59.5 296.15 ms
(one error in the first case, is much more critical)59.5 283.15 ms
(than in the second case. But MSE and MAE are equal to one for)59.5 270.15 ms
(both shops predictions, and thus according to those metrics, the)59.5 257.15 ms
(se)59.5 244.15 ms
(off by one errors are indistinguishable. This is basically becau)59.5 231.15 ms
(se MSE and)59.5 218.15 ms
(MAE work with absolute errors while relative error can)59.5 205.15 ms
(be more important for us. Off by one error for)59.5 192.15 ms
(the shops that sell ten items is equal to mistaking by 100 items)59.5 179.15 ms
( for)59.5 166.15 ms
(shops that sell 1,000 items. On the plot for MSE and MAE, we can)59.5 153.15 ms
( see that all the error curves have)59.5 140.15 ms
(the same shape for every target value. The curves are kind of sh)59.5 127.15 ms
(ifted)59.5 114.15 ms
(version of each other. That is an indicator that metric)59.5 101.15 ms
(works with absolute errors. The relative error preference)59.5 88.15 ms
(can be expressed with Mean Square Percentage Error, MSPE in shor)59.5 75.15 ms
(t, or)59.5 62.15 ms
(Mean Absolute Percentage Error, MAPE. If you compare them to MSE)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 7 7
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                      Page 7)59.5 790.15 ms
F0 sf
( and MAE,)59.5 764.15 ms
(you will notice the difference. For each object, the absolute er)59.5 751.15 ms
(ror)59.5 738.15 ms
(is divided by the target value, giving relative error. MSPE and )59.5 725.15 ms
(MAPE can also be thought)59.5 712.15 ms
(as weighted versions of MSE and MAE, respectively. For the MAPE,)59.5 699.15 ms
( the weight of its sample is)59.5 686.15 ms
(inversely proportional to it's target. While for MSPE, it is inv)59.5 673.15 ms
(ersely)59.5 660.15 ms
(proportional to a target square. Know that the weight do)59.5 647.15 ms
(not sum up to one here. You can take a look at this)59.5 634.15 ms
(individual error plus for our individual sample dataset. Now, we)59.5 621.15 ms
( see the course became more)59.5 608.15 ms
(flat as the target value increases. It means that, the cost we p)59.5 595.15 ms
(ay for)59.5 582.15 ms
(a fixed absolute error, depends on the target value. And as the )59.5 569.15 ms
(target increases, we pay less. So having talk about definition a)59.5 556.15 ms
(nd)59.5 543.15 ms
(motivation behind MSPE and MAPE. Let's now think, what are the o)59.5 530.15 ms
(ptimal)59.5 517.15 ms
(constant predictions for these matrix? Recall that for MSE, the )59.5 504.15 ms
(optimal)59.5 491.15 ms
(constant is the mean over target values. Now, for MSPE, the weig)59.5 478.15 ms
(hted)59.5 465.15 ms
(version of MSE, in turns out that the optimal constant is weight)59.5 452.15 ms
(ed)59.5 439.15 ms
(mean of the target values. For our dataset,)59.5 426.15 ms
(the optimal value is about 6.6, and we see that it's biased)59.5 413.15 ms
(towards small targets. Since the absolute error for)59.5 400.15 ms
(them is weighted with the highest weight, and thus inputs metric)59.5 387.15 ms
( the most. Now the MAPE, this is a question for you. What do you)59.5 374.15 ms
( think is)59.5 361.15 ms
(an optimal constant for it? Just use your intuition here and)59.5 348.15 ms
(knowledge from the previous slides. Especially recall that MAPE)59.5 335.15 ms
(is weighted version of MAE. The right answer is,)59.5 322.15 ms
(the best constant is weighted median. It is not a very commonly )59.5 309.15 ms
(used)59.5 296.15 ms
(quantity actually, so take a look for a bit of explanation in)59.5 283.15 ms
(the reading materials. The optimal value here is 6, and it is)59.5 270.15 ms
(even smaller than the constant for MSPE. But do not try to expla)59.5 257.15 ms
(in)59.5 244.15 ms
(it using outliers. If an outlier had a very,)59.5 231.15 ms
(very small value, MAPE would be very biased towards it, since th)59.5 218.15 ms
(is)59.5 205.15 ms
(outlier will have the highest weight. All right, now let's move )59.5 192.15 ms
(on to)59.5 179.15 ms
(the last metric in this video, Root Mean Square Logarithmic Erro)59.5 166.15 ms
(r,)59.5 153.15 ms
(or RMSLE in short. What is RMSLE? It is just an RMSE calculated)59.5 140.15 ms
(in logarithmic scale. In fact, to calculate it,)59.5 127.15 ms
(we take a logarithm of our predictions and the target values, an)59.5 114.15 ms
(d)59.5 101.15 ms
(compute RMSE between them. The targets are usually non-negative )59.5 88.15 ms
(but)59.5 75.15 ms
(can equal to 0, and the logarithm of 0 is not defined. That is w)59.5 62.15 ms
(hy a constant is usually)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 8 8
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                      Page 8)59.5 790.15 ms
F0 sf
(added to the predictions and the targets before applying)59.5 764.15 ms
(the logarithmic operation. This constant can also be)59.5 751.15 ms
(chosen to be different to one. It can be for example 300)59.5 738.15 ms
(depending on organizer's needs. But for us, it will not change m)59.5 725.15 ms
(uch. So, this metric is usually used)59.5 712.15 ms
(in the same situation as MSPE and MAPE, as it also carries about)59.5 699.15 ms
( relative)59.5 686.15 ms
(errors more than about absolute ones. But note the asymmetry)59.5 673.15 ms
(of the error curves. From the perspective of RMSLE, it is always)59.5 660.15 ms
( better to predict more)59.5 647.15 ms
(than the same amount less than target. Same as root mean square )59.5 634.15 ms
(error doesn't)59.5 621.15 ms
(differ much from mean square error, RMSLE can be calculated)59.5 608.15 ms
(without root operation. But the rooted version)59.5 595.15 ms
(is more widely used. It is important to know that the plot)59.5 582.15 ms
(we see here on the slide is built for a version without the root)59.5 569.15 ms
(. And for a root version,)59.5 556.15 ms
(an analogous plot would be misleading. Now let's move on to the )59.5 543.15 ms
(question)59.5 530.15 ms
(about the best constant. I will let you guess the answer again. )59.5 517.15 ms
(Just recall that, Just recall what)59.5 504.15 ms
(is the best constant prediction for RMSE and)59.5 491.15 ms
(use the connection between RMSLE and RMSE. To find the constant,)59.5 478.15 ms
( we should realize)59.5 465.15 ms
(that we can first find the best constant for RMSE in the log spa)59.5 452.15 ms
(ce, will)59.5 439.15 ms
(be the weighted mean in the log space. And after it, we need to )59.5 426.15 ms
(get back from log space to)59.5 413.15 ms
(the usual one with an inverse transform. The optimal constant tu)59.5 400.15 ms
(rns out to be 9.1. It is higher than constants for)59.5 387.15 ms
(both MAPE and MSPE. Here we see the optimal constants for)59.5 374.15 ms
(the metrics we've broken down. MSE is quite biased towards)59.5 361.15 ms
(the huge value from our dataset, while MAE is much less biased. )59.5 348.15 ms
(MSPE and MAPE are biased)59.5 335.15 ms
(towards smaller targets because they assign higher weight to)59.5 322.15 ms
(the object with small targets. And RMSLE is frequently considere)59.5 309.15 ms
(d)59.5 296.15 ms
(as better metrics than MAPE, since it is less biased towards sma)59.5 283.15 ms
(ll)59.5 270.15 ms
(targets, yet works with relative errors. I strongly encourage yo)59.5 257.15 ms
(u to)59.5 244.15 ms
(think about the baseline for metrics that you can face for first)59.5 231.15 ms
( time. It truly helps to build an intuition and)59.5 218.15 ms
(to find a way to optimize the metrics. So, in this video, we wil)59.5 205.15 ms
(l discuss different metrics)59.5 192.15 ms
(that works with relative errors. MSPE, means square percentage e)59.5 179.15 ms
(rror,)59.5 166.15 ms
(MAPE, mean absolute percentage error, and RMSLE,)59.5 153.15 ms
(root mean squared logarithmic error. We'll discussed the definit)59.5 140.15 ms
(ions and)59.5 127.15 ms
(the baseline solutions for them. In the next video, we will stud)59.5 114.15 ms
(y)59.5 101.15 ms
(several classification matrix. [MUSIC][MUSIC] In the previous vi)59.5 88.15 ms
(deos, we discussed)59.5 75.15 ms
(metrics for regression problems. And here,)59.5 62.15 ms
(we'll review classification metrics. We will first talk about ac)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 9 9
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                      Page 9)59.5 790.15 ms
F0 sf
(curacy,)59.5 764.15 ms
(logarithmic loss, and then get to area under a receiver)59.5 751.15 ms
(operating curve, and Cohen's Kappa. And specifically Quadratic w)59.5 738.15 ms
(eighted Kappa. Let's start by fixing the notation. N will be the)59.5 725.15 ms
( number of objects in our)59.5 712.15 ms
(dataset, L, the number of classes. As before, y will stand for t)59.5 699.15 ms
(he target,)59.5 686.15 ms
(and y hat, for predictions. If you see an expression in square)59.5 673.15 ms
(brackets, that is an indicator function. It fields one if the ex)59.5 660.15 ms
(pression)59.5 647.15 ms
(is true and zero if it's false. Throughout the video,)59.5 634.15 ms
(we'll use two more terms hard labels or hard predictions, and)59.5 621.15 ms
(soft labels or soft predictions. Usually models output some kind)59.5 608.15 ms
( of scores. For example, probabilities for)59.5 595.15 ms
(an objects to belong to each class. The scores can be written)59.5 582.15 ms
(as a vector of size L, and I will refer to this vector)59.5 569.15 ms
(as to soft predictions. Now in classification we are usually)59.5 556.15 ms
(asked to predict a label for the object, do a hard prediction. T)59.5 543.15 ms
(o do it, we usually find a maximum)59.5 530.15 ms
(value in the soft predictions, and set class that corresponds to)59.5 517.15 ms
( this)59.5 504.15 ms
(maximum score as our predicted label. So hard label is)59.5 491.15 ms
(a function of soft labels, it's usually arg max for)59.5 478.15 ms
(multi class tasks, but for binary classification it can be)59.5 465.15 ms
(thought of as a thresholding function. So we output label 1)59.5 452.15 ms
(when the soft score for the class 1 is higher than the threshold)59.5 439.15 ms
(,)59.5 426.15 ms
(and we output class 0 otherwise. Let's start our journey)59.5 413.15 ms
(with the accuracy score. Accuracy is the most straightforward)59.5 400.15 ms
(measure of classifiers quality. It's a value between 0 and 1. Th)59.5 387.15 ms
(e higher, the better. And it is equal to the fraction)59.5 374.15 ms
(of correctly classified objects. To compute accuracy,)59.5 361.15 ms
(we need hard predictions. We need to assign each)59.5 348.15 ms
(object a specific table. Now, what is the best constant)59.5 335.15 ms
(to predict in case of accuracy? Actually, there are a small)59.5 322.15 ms
(number of constants to try. We can only assign a class label)59.5 309.15 ms
(to all the objects at once. So what class should we assign? Obvi)59.5 296.15 ms
(ously, the most frequent one. Then the number of correctly guess)59.5 283.15 ms
(ed)59.5 270.15 ms
(objects will be the highest. But exactly because of that reason,)59.5 257.15 ms
( there is a caveat in interpreting)59.5 244.15 ms
(the values of the accuracy score. Take a look at this example. S)59.5 231.15 ms
(ay we have 10 cats and)59.5 218.15 ms
(90 dogs in our train set. If we always predicted dog for)59.5 205.15 ms
(every object, then the accuracy would be already 0.9. And imagin)59.5 192.15 ms
(e you tell someone that your)59.5 179.15 ms
(classifier is correct 9 times out of 10. The person would probab)59.5 166.15 ms
(ly)59.5 153.15 ms
(think you have a nice model. But in fact, your model just predic)59.5 140.15 ms
(ts)59.5 127.15 ms
(dog class no matter what input is. So the problem is, that the b)59.5 114.15 ms
(ase)59.5 101.15 ms
(line accuracy can be very high for a data set, even 99%, and tha)59.5 88.15 ms
(t makes)59.5 75.15 ms
(it hard to interpret the results. Although accuracy score is ver)59.5 62.15 ms
(y clean and)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 10 10
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 10)59.5 790.15 ms
F0 sf
(intuitive, it turns out to be quite hard to optimize. Accuracy a)59.5 764.15 ms
(lso doesn't care how confident)59.5 751.15 ms
(the classifier is in the predictions, and what soft predictions )59.5 738.15 ms
(are. It cares only about arg)59.5 725.15 ms
(max of soft predictions. And thus, people sometimes prefer to)59.5 712.15 ms
(use different metrics that are first, easier to optimize. And se)59.5 699.15 ms
(cond, these metrics work with)59.5 686.15 ms
(soft predictions, not hard ones. One of such metrics is logarith)59.5 673.15 ms
(mic loss. It tries to make the classifier to)59.5 660.15 ms
(output two posterior probabilities for their objects to be of a )59.5 647.15 ms
(certain kind,)59.5 634.15 ms
(of a certain class. A log loss is usually the reason)59.5 621.15 ms
(a little bit differently for binary and multi class tasks. For b)59.5 608.15 ms
(inary, it is assumed that y)59.5 595.15 ms
(hat is a number from 01 range, and it is a probability of)59.5 582.15 ms
(an object to belong to class one. So 1 minus y hat is the probab)59.5 569.15 ms
(ility for)59.5 556.15 ms
(this object to be of class 0. For multiclass tasks,)59.5 543.15 ms
(LogLoss is written in this form. Here y hat ith is a vector of s)59.5 530.15 ms
(ize L,)59.5 517.15 ms
(and its sum is exactly 1. The elements are the probabilities)59.5 504.15 ms
(to belong to each of the classes. Try to write this formula down)59.5 491.15 ms
( for)59.5 478.15 ms
(L equals 2, and you will see it is exactly)59.5 465.15 ms
(binary loss from above. And finally, it should be mentioned)59.5 452.15 ms
(that to avoid in practice, predictions are clipped to)59.5 439.15 ms
(be not from 0 to 1, but from some small positive number to)59.5 426.15 ms
(1 minus some small positive number. Okay, now let us analyze it )59.5 413.15 ms
(a little bit. Assume a target for an object is 0,)59.5 400.15 ms
(and here on the plot, we see how the error will change if we)59.5 387.15 ms
(change our predictions from 0 to 1. For comparison, we'll plot)59.5 374.15 ms
(absolute error with another color. Logloss usually penalizes)59.5 361.15 ms
(completely wrong answers and prefers to make a lot of small)59.5 348.15 ms
(mistakes to one but severer mistake. Now, what is the best const)59.5 335.15 ms
(ant for)59.5 322.15 ms
(logarithmic loss? It turns out that you need to set)59.5 309.15 ms
(predictions to the frequencies of each class in the data set. In)59.5 296.15 ms
( our case, the frequencies for the cat class is 0.1, and)59.5 283.15 ms
(it is 0.9 for class dog. Then the best constant is)59.5 270.15 ms
(vector of those two values. How do I, well how do I know that is)59.5 257.15 ms
( so? To prove it we should take a derivative)59.5 244.15 ms
(with the respect to constant alpha, set it to 0, and)59.5 231.15 ms
(find alpha from this equation. Okay, we've discussed accuracy an)59.5 218.15 ms
(d)59.5 205.15 ms
(log loss, now let's move on. Take a look at the example. We show)59.5 192.15 ms
( ground truth target)59.5 179.15 ms
(value with color, and the position of the point)59.5 166.15 ms
(shows the classifier score. Recall that to compute accuracy scor)59.5 153.15 ms
(e for)59.5 140.15 ms
(a binary task, we usually take soft predictions)59.5 127.15 ms
(from our model and apply threshold. We can see the prediction to)59.5 114.15 ms
( be green)59.5 101.15 ms
(if the score is higher than 0.5 and red if it's lower. For this )59.5 88.15 ms
(example the accuracy is 6 or)59.5 75.15 ms
(7, as we misclassified one red object. But look, if the threshol)59.5 62.15 ms
(d was 0.7, then all the objects would)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 11 11
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 11)59.5 790.15 ms
F0 sf
(be classified correctly. So this is kind of motivation for)59.5 764.15 ms
(our next metric, Area Under Curve. We shouldn't fix the threshol)59.5 751.15 ms
(d for it, but this metric kind of tries all possible)59.5 738.15 ms
(ones and aggregates those scores. So this metric doesn't really )59.5 725.15 ms
(cares about)59.5 712.15 ms
(absolute values of the predictions. But it depends only on)59.5 699.15 ms
(the order of the objects. Actually, there are several ways AUC, )59.5 686.15 ms
(or)59.5 673.15 ms
(this area under curve, can be explained. The first one explains )59.5 660.15 ms
(under what)59.5 647.15 ms
(curve we should compute area. And the second explains)59.5 634.15 ms
(AUC as the probability of object pairs to be correctly)59.5 621.15 ms
(ordered by our model. We will see both)59.5 608.15 ms
(explanations in the moment. So let's start with the first one. S)59.5 595.15 ms
(o we need to calculate)59.5 582.15 ms
(an area under a curve. What curve? Let's construct it right now.)59.5 569.15 ms
( Once again, say we have six objects, and)59.5 556.15 ms
(their true label is shown with a color. And the position of the )59.5 543.15 ms
(dot shows)59.5 530.15 ms
(the classifier's predictions. And for now we will use word posit)59.5 517.15 ms
(ive)59.5 504.15 ms
(as synonym to belongs to the red class. So positive side is on t)59.5 491.15 ms
(he left. What we will do now, we'll go from left to)59.5 478.15 ms
(right, jump from one object to another. And for)59.5 465.15 ms
(each we will calculate how many red and green dots are there to )59.5 452.15 ms
(the left,)59.5 439.15 ms
(to this object that we stand on. The red dots we'll have a name )59.5 426.15 ms
(for)59.5 413.15 ms
(them, true positives. And for the green ones we'll)59.5 400.15 ms
(have name false positives. So we will kind of compute)59.5 387.15 ms
(how many true positives and false positives we see to the left)59.5 374.15 ms
(of the object we stand on. Actually it's very simple,)59.5 361.15 ms
(we start from bottom left corner and go up every time we see red)59.5 348.15 ms
( point. And right when we see a green one. Let's see. So we stan)59.5 335.15 ms
(d on the leftmost point first. And it is red, or positive. So we)59.5 322.15 ms
( increase the number of)59.5 309.15 ms
(true positives and move up. Next, we jump on the green point. It)59.5 296.15 ms
( is false positive, and so we go right. Then two times up for tw)59.5 283.15 ms
(o red points. And finally two times right for)59.5 270.15 ms
(the last green point. We finished in the top right corner. And i)59.5 257.15 ms
(t always works like that. We start from bottom left and end up i)59.5 244.15 ms
(n top right corner when)59.5 231.15 ms
(we jump on the right most point. By the way, the curve we've jus)59.5 218.15 ms
(t built)59.5 205.15 ms
(is called Receiver Operating Curve or ROC Curve. And now we are )59.5 192.15 ms
(ready to calculate)59.5 179.15 ms
(an area under this curve. The area is seven and we need to norma)59.5 166.15 ms
(lize)59.5 153.15 ms
(it by the total plural area of the square. So AUC is 7/9, cool. )59.5 140.15 ms
(Now what AUC will be for)59.5 127.15 ms
(the data set that can be separated with a threshold,)59.5 114.15 ms
(like in our initial example? Actually AUC will be 1,)59.5 101.15 ms
(maximum value of AUC. So it works. It doesn't need a threshold)59.5 88.15 ms
(to be specified and it doesn't depend on absolute values. Recall)59.5 75.15 ms
( that we've never used absolute)59.5 62.15 ms
(values while constructing the curve. Now in practice,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 12 12
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 12)59.5 790.15 ms
F0 sf
(if you build such curve for a huge data set in real classifier,)59.5 764.15 ms
(you would observe a picture like that. Here curves for different)59.5 751.15 ms
( classifiers)59.5 738.15 ms
(are shown with different colors. The curves usually lie above)59.5 725.15 ms
(the dashed line which shows how would the curve look like if)59.5 712.15 ms
(we made predictions at random. So it kind of shows us a baseline)59.5 699.15 ms
(. And note that the area under)59.5 686.15 ms
(the dashed line is 0.5. All right, we've seen that we can build)59.5 673.15 ms
(a curve and compute area under it. There is another total differ)59.5 660.15 ms
(ent)59.5 647.15 ms
(explanation for the AUC. Consider all pairs of objects, such tha)59.5 634.15 ms
(t one object is from red class and)59.5 621.15 ms
(another one is from green. AUC is a probability that score for t)59.5 608.15 ms
(he green one will be higher)59.5 595.15 ms
(than the score for the red one. In other words, AUC is a fractio)59.5 582.15 ms
(n)59.5 569.15 ms
(of correctly ordered pairs. You see in our example we have)59.5 556.15 ms
(two incorrectly ordered pairs and nine pairs in total. And then )59.5 543.15 ms
(there are 7 correctly)59.5 530.15 ms
(ordered pairs and thus AUC is 7/9. Exactly as we got before,)59.5 517.15 ms
(while computing area under the curve. All right,)59.5 504.15 ms
(we've discussed how to compute AUC. Now let's think what is the )59.5 491.15 ms
(best)59.5 478.15 ms
(constant prediction for it. In fact, AUC doesn't depend on)59.5 465.15 ms
(the exact values of the predictions. So all constants will lead)59.5 452.15 ms
(to the same score and this score will be around 0.5,)59.5 439.15 ms
(the baseline. This is actually something)59.5 426.15 ms
(that people love about AUC. It is clear what the baseline is. Of)59.5 413.15 ms
( course there are flaws in AUC,)59.5 400.15 ms
(every metric has some. But still AUC is metric I usually use)59.5 387.15 ms
(when no one sets up another one for me. All right, finally let's)59.5 374.15 ms
( get)59.5 361.15 ms
(to the last metric to discuss, Cohen's Kappa and it's derivative)59.5 348.15 ms
(s. Recall that if we always predict)59.5 335.15 ms
(the label of the most frequent class, we can already get pretty )59.5 322.15 ms
(high accuracy)59.5 309.15 ms
(score, and that can be misleading. Actually in our example)59.5 296.15 ms
(all the models will fit, will have a score somewhere)59.5 283.15 ms
(between 0.9 and 1. So we can introduce a new metric such that)59.5 270.15 ms
(for an accuracy of 1 it would give us 1, and for)59.5 257.15 ms
(the baseline accuracy it would output 0. And of course,)59.5 244.15 ms
(baselines are going to be different for every data,)59.5 231.15 ms
(not necessarily 0.9 or whatever. It is also very similar to)59.5 218.15 ms
(what r squared does with MSE. It informally saying is)59.5 205.15 ms
(kind of normalizes it. So we do the same here. And this is actua)59.5 192.15 ms
(lly already)59.5 179.15 ms
(almost Cohen's Kappa. In Cohen's Kappa we take)59.5 166.15 ms
(another value as the baseline. We take the higher predictions fo)59.5 153.15 ms
(r)59.5 140.15 ms
(the data set and shuffle them, like randomly permute. And then w)59.5 127.15 ms
(e calculate an accuracy for)59.5 114.15 ms
(these shuffled predictions. And that will be our baseline. Well )59.5 101.15 ms
(to be precise, we permute and)59.5 88.15 ms
(calculate accuracies many times and take, as the baseline, an av)59.5 75.15 ms
(erage for)59.5 62.15 ms
(those computed accuracies. In practice, of course,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 13 13
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 13)59.5 790.15 ms
F0 sf
(we do not need to do any permutations. This baseline score can)59.5 764.15 ms
(be computed analytically. We need, first, to multiply the empiri)59.5 751.15 ms
(cal)59.5 738.15 ms
(frequencies of our predictions and grant those labels for)59.5 725.15 ms
(each class, and then sum them up. For example,)59.5 712.15 ms
(if we assign 20 cat labels and 80 dog labels at random,)59.5 699.15 ms
(then the baseline accuracy will be 0.2*0.1 + 0.8*0.9 = 0.74. You)59.5 686.15 ms
( can find more examples in actually. Here I wanted to explain a )59.5 673.15 ms
(nice way of)59.5 660.15 ms
(thinking about eliminator as a baseline. We can also recall that)59.5 647.15 ms
( error)59.5 634.15 ms
(is equal to 1 minus accuracy. We could rewrite the formula as 1)59.5 621.15 ms
(minus model's error/baseline error. It will still be Cohen's Kap)59.5 608.15 ms
(pa, but now, it would be easier to)59.5 595.15 ms
(derive weighted Cohen's Kappa. To explain weighted Kappa,)59.5 582.15 ms
(we first need to do a step aside, and introduce weighted error. )59.5 569.15 ms
(See now we have cats,)59.5 556.15 ms
(dogs and tigers to classify. And we are more or less okay if)59.5 543.15 ms
(we predict dog instead of cat. But it's undesirable to predict c)59.5 530.15 ms
(at or)59.5 517.15 ms
(dog if it's really a tiger. So we're going to form)59.5 504.15 ms
(a weight matrix where each cell contains The weight for)59.5 491.15 ms
(the mistake we might do. In our case, we set error weight to be)59.5 478.15 ms
(ten times larger if we predict cat or dog, but the ground truth )59.5 465.15 ms
(label is tiger. So with error weight matrix, we can express our )59.5 452.15 ms
(preference on)59.5 439.15 ms
(the errors that the classifier would make. Now, to calculate wei)59.5 426.15 ms
(ght and error we need another matrix, confusion)59.5 413.15 ms
(matrix, for the classifier's prediction. This matrix shows how o)59.5 400.15 ms
(ur classifier)59.5 387.15 ms
(distributes the predictions over the objects. For example, the f)59.5 374.15 ms
(irst column indicates)59.5 361.15 ms
(that four cats out of ten were recognized correctly, two were cl)59.5 348.15 ms
(assified as dogs and)59.5 335.15 ms
(four as tigers. So to get a weighted error score, we need to mul)59.5 322.15 ms
(tiply these two matrices)59.5 309.15 ms
(element-wise and sum their results. This formula needs a proper )59.5 296.15 ms
(normalization to make sure the quantity is between 0 and)59.5 283.15 ms
(1, but it doesn't matter for our purposes, as the normalization)59.5 270.15 ms
(constant will anyway cancel. And finally,)59.5 257.15 ms
(weighted kappa is calculated as 1- weighted error / weighted bas)59.5 244.15 ms
(eline error. In many cases, the weight matrices)59.5 231.15 ms
(are defined in a very simple way. For example, for classificatio)59.5 218.15 ms
(n)59.5 205.15 ms
(problems with ordered labels. Say you need to assign each)59.5 192.15 ms
(object a value from 1 to 3. It can be, for instance,)59.5 179.15 ms
(a rating of how severe the disease is. And it is not regression,)59.5 166.15 ms
( since you do not)59.5 153.15 ms
(allow to output values to be somewhere between the ratings and t)59.5 140.15 ms
(he ground truth)59.5 127.15 ms
(values also look more like labels, not as numeric values to pred)59.5 114.15 ms
(ict. So such problems are usually treated)59.5 101.15 ms
(as classification problems, but weight matrix is introduced to a)59.5 88.15 ms
(ccount for)59.5 75.15 ms
(order of the labels. For example, weights can be linear, if we)59.5 62.15 ms
(predict two instead of one, we pay one. If we predict three inst)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 14 14
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 14)59.5 790.15 ms
F0 sf
(ead of of one,)59.5 764.15 ms
(we pay two. Or the weights can be quadratic,)59.5 751.15 ms
(if we'll predict two instead of one, we still pay one, but if we)59.5 738.15 ms
( predict)59.5 725.15 ms
(three instead of one, we now pay for. Depending on what weight m)59.5 712.15 ms
(atrix is used, we get either linear weighted kappa or)59.5 699.15 ms
(quadratic weighted kappa. The quadratic weighted kappa has been)59.5 686.15 ms
(used in several competitions on Kaggle. It is usually explained )59.5 673.15 ms
(as)59.5 660.15 ms
(inter-rater agreement coefficient, how much the predictions of t)59.5 647.15 ms
(he model)59.5 634.15 ms
(agree with ground-truth raters. Which is quite intuitive for)59.5 621.15 ms
(medicine applications, how much the model agrees)59.5 608.15 ms
(with professional doctors. Finally, in this video,)59.5 595.15 ms
(we've discussed classification matrix. The accuracy, it is an es)59.5 582.15 ms
(sential)59.5 569.15 ms
(metric for classification. But a simple model that predicts alwa)59.5 556.15 ms
(ys)59.5 543.15 ms
(the same value can possibly have a very high accuracy that makes)59.5 530.15 ms
(it hard to interpret this metric. The score also depends on the )59.5 517.15 ms
(threshold)59.5 504.15 ms
(we choose to convert soft predictions to hard labels. Logloss is)59.5 491.15 ms
( another metric, as opposed to accuracy it depends on soft)59.5 478.15 ms
(predictions rather than on hard labels. And it forces the model )59.5 465.15 ms
(to predict)59.5 452.15 ms
(probabilities of an object to belong to each class. AUC, area un)59.5 439.15 ms
(der receiver operating curve,)59.5 426.15 ms
(doesn't depend on the absolute values predicted by the classifie)59.5 413.15 ms
(r, but)59.5 400.15 ms
(only considers the ordering of the object. It also implicitly tr)59.5 387.15 ms
(ies all the)59.5 374.15 ms
(thresholds to converge soft predictions to hard labels, and thus)59.5 361.15 ms
( removes the)59.5 348.15 ms
(dependence of the score on the threshold. Finally, Cohen's Kappa)59.5 335.15 ms
( fixes the baseline)59.5 322.15 ms
(for accuracy score to be zero. In spirit it is very)59.5 309.15 ms
(similar to how R-squared beta scales MSE value)59.5 296.15 ms
(to be easier explained. If instead of accuracy we used weighted)59.5 283.15 ms
(accuracy, we would get weighted kappa. Weighted kappa with quadr)59.5 270.15 ms
(atic weights)59.5 257.15 ms
(is called quadratic weighted kappa and commonly used on Kaggle. )59.5 244.15 ms
([MUSIC]In this video, we will discuss what is the loss and what )59.5 231.15 ms
(is a metric, and what is the difference between them. And then w)59.5 218.15 ms
(e'll overview what are the general approaches to metric optimiza)59.5 205.15 ms
(tion. Let's start with a comparison between two notions, loss an)59.5 192.15 ms
(d metric. The metric or target metric is a function which we wan)59.5 179.15 ms
(t to use to evaluate the quality of our model. For example, for )59.5 166.15 ms
(a classification task, we may want to maximize accuracy of our p)59.5 153.15 ms
(redictions, how frequently the model outputs the correct label. )59.5 140.15 ms
(But the problem is that no one really knows how to optimize accu)59.5 127.15 ms
(racy efficiently. Instead, people come up with the proxy loss fu)59.5 114.15 ms
(nctions. They are such evaluation functions that are easy to opt)59.5 101.15 ms
(imize for a given model. For example, logarithmic loss is widely)59.5 88.15 ms
( used as an optimization loss, while the accuracy score is how t)59.5 75.15 ms
(he solution is eventually evaluated. So, once again, the loss fu)59.5 62.15 ms
(nction is a function that our model optimizes and uses to evalua)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 15 15
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 15)59.5 790.15 ms
F0 sf
(te the solution, and the target metric is how we want the soluti)59.5 764.15 ms
(on to be evaluated. This is kind of expectation versus reality t)59.5 751.15 ms
(hing. Sometimes we are lucky and the model can optimize our targ)59.5 738.15 ms
(et metric directly. For example, for mean square error metric, m)59.5 725.15 ms
(ost libraries can optimize it from the outset, from the box. So )59.5 712.15 ms
(the loss function is the same as the target metric. And sometime)59.5 699.15 ms
(s we want to optimize metrics that are really hard or even impos)59.5 686.15 ms
(sible to optimize directly. In this case, we usually set the mod)59.5 673.15 ms
(el to optimize a loss that is different to a target metric, but )59.5 660.15 ms
(after a model is trained, we use hacks and heuristics to negate )59.5 647.15 ms
(the discrepancy and adjust the model to better fit the target me)59.5 634.15 ms
(tric. We will see the examples for both cases in the following v)59.5 621.15 ms
(ideos. And the last thing to mention is that loss metric, cost o)59.5 608.15 ms
(bjective and other notions are more or less used as synonyms. It)59.5 595.15 ms
( is completely okay to say target loss and optimization metric, )59.5 582.15 ms
(but we will fix the wording for the clarity now. Okay, so far, w)59.5 569.15 ms
(e've understood why it's important to optimize a metric given in)59.5 556.15 ms
( a competition. And we have discussed the difference between opt)59.5 543.15 ms
(imization loss and target metric. Now, let's overview the approa)59.5 530.15 ms
(ches to target metrics optimization in general. The approaches c)59.5 517.15 ms
(an be broadly divided into several categories, depending on the )59.5 504.15 ms
(metric we need to optimize. Some metrics can be optimized direct)59.5 491.15 ms
(ly. That is, we should just find a model that optimizes this met)59.5 478.15 ms
(ric and run it. In fact, all we need to do is to set the model's)59.5 465.15 ms
( loss function to these metric. The most common metrics like MSE)59.5 452.15 ms
(, Logloss are implemented as loss functions in almost every libr)59.5 439.15 ms
(ary. For some of the metrics that cannot be optimized directly, )59.5 426.15 ms
(we can somehow pre-process the train set and use a model with a )59.5 413.15 ms
(metric or loss function which is easy to optimize. For example, )59.5 400.15 ms
(while MSPE metric cannot be optimized directly with XGBoost, we )59.5 387.15 ms
(will see later that we can resample the train set and optimize M)59.5 374.15 ms
(SE loss instead, which XGBoost can optimize. Sometimes, we'll op)59.5 361.15 ms
(timize incorrect metric, but we'll post-process the predictions )59.5 348.15 ms
(to fit classification, to fit the communication metric better. F)59.5 335.15 ms
(or some models and frameworks, it's possible to define a custom )59.5 322.15 ms
(loss function, and sometimes it's possible to implement a loss f)59.5 309.15 ms
(unction which will serve as a nice proxy for the desired metric.)59.5 296.15 ms
( For example, it can be done for quadratic-weighted Kappa, as we)59.5 283.15 ms
( will see later. It's actually quite easy to define a custom los)59.5 270.15 ms
(s function for XGBoost. We only need to implement a single funct)59.5 257.15 ms
(ion that takes predictions and the target values and computes fi)59.5 244.15 ms
(rst and second-order derivatives of the loss function with respe)59.5 231.15 ms
(ct to the model's predictions. For example, here you see one for)59.5 218.15 ms
( the Logloss. Of course, the loss function should be smooth enou)59.5 205.15 ms
(gh and have well-behaved derivatives, otherwise XGBoost will dri)59.5 192.15 ms
(ve crazy. In this course, we consider only a small set of metric)59.5 179.15 ms
(s, but there are plenty of them in fact. And for some of them, i)59.5 166.15 ms
(t is really hard to come up with a neat optimization procedure o)59.5 153.15 ms
(r write a custom loss function. Thankfully, there is a method th)59.5 140.15 ms
(at always works. It is called early stopping, and it is very sim)59.5 127.15 ms
(ple. You set a model to optimize any loss function it can optimi)59.5 114.15 ms
(ze and you monitor the desired metric on a validation set. And y)59.5 101.15 ms
(ou stop the training when the model starts to fit according to t)59.5 88.15 ms
(he desired metric and not according to the metric the model is t)59.5 75.15 ms
(ruly optimizing. That is important. Of course, some metrics cann)59.5 62.15 ms
(ot be even easily evaluated. For example, if the metric is based)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 16 16
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 16)59.5 790.15 ms
F0 sf
( on a human assessor's opinions, you cannot evaluate it on every)59.5 764.15 ms
( iteration. For such metrics, we cannot use early stopping, but )59.5 751.15 ms
(we will never find such metrics in a competition. So, in this vi)59.5 738.15 ms
(deo, we have discussed the discrepancy between our target metric)59.5 725.15 ms
( and the loss function that our model optimizes. We've reviewed )59.5 712.15 ms
(several approaches to target metric optimization and, in particu)59.5 699.15 ms
(lar, discussed early stopping. In the following videos, we will )59.5 686.15 ms
(go through the regression and classification metrics and see the)59.5 673.15 ms
( hacks we can use to optimize them.[SOUND] So)59.5 660.15 ms
(far we've discussed different metrics, their definitions, and in)59.5 647.15 ms
(tuition for them. We've studied the difference between)59.5 634.15 ms
(optimization loss and target metric. In this video, we'll see ho)59.5 621.15 ms
(w we can)59.5 608.15 ms
(efficiently optimize metrics used for regression problems. We've)59.5 595.15 ms
( discussed,)59.5 582.15 ms
(we always can use earl stopping. So I won't mention it for ever )59.5 569.15 ms
(metrics. But keep it in mind. Let's start with mean squared erro)59.5 556.15 ms
(r. It's the most commonly used metric for)59.5 543.15 ms
(regression tasks. So we should expect it)59.5 530.15 ms
(to be easy to work with. In fact, almost every modelling softwar)59.5 517.15 ms
(e)59.5 504.15 ms
(will implement MSE as a loss function. So all you need to do to )59.5 491.15 ms
(optimize it is)59.5 478.15 ms
(to turn this on in your favorite library. And here are some of t)59.5 465.15 ms
(he library that)59.5 452.15 ms
(support mean square error optimization. Both XGBoost and)59.5 439.15 ms
(LightGBM will do it easily. A RandomForestRegresor from a scaler)59.5 426.15 ms
( and)59.5 413.15 ms
(also can split based on MSE, thus optimizing individually. A lot)59.5 400.15 ms
( of linear models)59.5 387.15 ms
(implemented in siclicar, and most of them are designed to optimi)59.5 374.15 ms
(ze MSE. For example, ordinarily squares,)59.5 361.15 ms
(reach regression, regression and so on. There's also SGRegressor)59.5 348.15 ms
( class and)59.5 335.15 ms
(Sklearn. It also implements a linear model but differently to ot)59.5 322.15 ms
(her)59.5 309.15 ms
(linear models in Sklearn. It uses [INAUDIBLE] gradient decent)59.5 296.15 ms
(to train it, and thus very versatile. Well and of course MSE was)59.5 283.15 ms
( built in. The library for)59.5 270.15 ms
(online learning of linear models, also accepts MSC as lost funct)59.5 257.15 ms
(ion. But every neural net package like PyTorch,)59.5 244.15 ms
(Keras, Flow, has MSE loss implemented. You just need to find an )59.5 231.15 ms
(example)59.5 218.15 ms
(on GitHub or wherever, and see what name MSE loss has)59.5 205.15 ms
(in that particular library. For example,)59.5 192.15 ms
(it is sometimes called L two loss, as L to distance in Matt Luke)59.5 179.15 ms
('s using. But basically for all the metrics)59.5 166.15 ms
(we consider in this lesson, you may find plaintal flames)59.5 153.15 ms
(since they were used and discovered independently)59.5 140.15 ms
(in different communities. Now, what about mean absolute error. M)59.5 127.15 ms
(AE is popular too, so it is easy to)59.5 114.15 ms
(find a model that will optimize it. Unfortunately, the extra boo)59.5 101.15 ms
(st)59.5 88.15 ms
(cannot optimize MAE because MAE has zero as a second)59.5 75.15 ms
(derivative while LightGBM can. So you still can use gradient boo)59.5 62.15 ms
(sting)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 17 17
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 17)59.5 790.15 ms
F0 sf
(decision trees to this metric. MAE criteria was implemented for)59.5 764.15 ms
(RandomForestRegressor from Sklearn. But note that running time w)59.5 751.15 ms
(ill be)59.5 738.15 ms
(quite high compared with MSE Corte. Unfortunately, linear models)59.5 725.15 ms
(from SKLearn including SG Regressor can not)59.5 712.15 ms
(optimize MAE negatively. But, there is a loss called Huber Loss,)59.5 699.15 ms
(it is implemented in some of the models. Basically, it is very s)59.5 686.15 ms
(imilar to MAE,)59.5 673.15 ms
(especially when the errors are large. We will discuss it in the )59.5 660.15 ms
(next slide. In [INAUDIBLE], MAE loss is implemented, but under a)59.5 647.15 ms
( different name)59.5 634.15 ms
(that's called quantile loss. In fact, MAE is just a special)59.5 621.15 ms
(case of quantile loss. Although I will not go into the details)59.5 608.15 ms
(here, but just recall that MAE is somehow connected to median va)59.5 595.15 ms
(lues and)59.5 582.15 ms
(median is a particular quantile. What about neural networks? As )59.5 569.15 ms
(we've discussed MAE is not)59.5 556.15 ms
(differentiable only when the predictions are equal to target. An)59.5 543.15 ms
(d it is of a rare case. That is why we may use any model)59.5 530.15 ms
(train to put to optimize MAE. It may be that you will not find M)59.5 517.15 ms
(AE)59.5 504.15 ms
(implemented in a neural library, but it is very easy to implemen)59.5 491.15 ms
(t it. In fact, all the models need is a loss function gradient)59.5 478.15 ms
(with respect to predictions. And in this case,)59.5 465.15 ms
(this is just a set function. Different names you may encounter f)59.5 452.15 ms
(or)59.5 439.15 ms
(MAE is, L1 that fit and a one loss, and sometimes people)59.5 426.15 ms
(refer to that special case of quintile regression as)59.5 413.15 ms
(to median regression. A lot, a lot of,)59.5 400.15 ms
(a lot of ways to make MAE smooth. You can actually make up your )59.5 387.15 ms
(own smooth)59.5 374.15 ms
(function that have upload that loops like MAE error. The most fa)59.5 361.15 ms
(mous one is Huber loss. It's basically a mix between MSE and MAE)59.5 348.15 ms
(. MSE is computed when the error is small,)59.5 335.15 ms
(so we can safely approach zero error. And MAE is computed for)59.5 322.15 ms
(large errors given robustness. So, to this end, we discuss the l)59.5 309.15 ms
(ibraries)59.5 296.15 ms
(that can optimize mean square error and mean absolute error. Now)59.5 283.15 ms
(, let's get to not ask)59.5 270.15 ms
(common relative metrics. MSPE and MAPE. It's much harder to find)59.5 257.15 ms
( the model)59.5 244.15 ms
(which can optimize them out of the box. Of course we can always )59.5 231.15 ms
(can use,)59.5 218.15 ms
(either, of course we can always either implement a custom loss f)59.5 205.15 ms
(or)59.5 192.15 ms
(an integer boost or a neural net. It is really easy to do there.)59.5 179.15 ms
( Or we can optimize different metric and)59.5 166.15 ms
(do early stopping. But there are several specific)59.5 153.15 ms
(approaches that I want to mention. This approach is based on the)59.5 140.15 ms
( fact that)59.5 127.15 ms
(MSP is a weighted version of MSE and MAP is a weighted version o)59.5 114.15 ms
(f MAE. On the right side,)59.5 101.15 ms
(we've sen expression for MSP and MAP. The summon denominator jus)59.5 88.15 ms
(t)59.5 75.15 ms
(ensures that the weights are summed up to 1, but it's not requir)59.5 62.15 ms
(ed. Intuitively, the sample weights are)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 18 18
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 18)59.5 790.15 ms
F0 sf
(indicating how important the object is for us while training the)59.5 764.15 ms
( model. The smaller the target,)59.5 751.15 ms
(is the more important the object. So, how do we use this knowled)59.5 738.15 ms
(ge? In fact,)59.5 725.15 ms
(many libraries accept sample weights. Say we want to optimize MS)59.5 712.15 ms
(P. So if we can set sample weights to)59.5 699.15 ms
(the ones from the previous slide, we can use MSE laws with it. A)59.5 686.15 ms
(nd, the model will actually)59.5 673.15 ms
(optimize desired MSPE loss. Although most important libraries li)59.5 660.15 ms
(ke)59.5 647.15 ms
(XGBoost, LightGBM, most neural net packages support sample weigh)59.5 634.15 ms
(ting,)59.5 621.15 ms
(not every library implements it. But there is another method whi)59.5 608.15 ms
(ch works)59.5 595.15 ms
(whenever a library can optimize MSE or MAE. Nothing else is need)59.5 582.15 ms
(ed. All we need to do is to create a new)59.5 569.15 ms
(training set by sampling it from the original set that we have a)59.5 556.15 ms
(nd)59.5 543.15 ms
(fit a model with, for example, I'm a secretarian if you)59.5 530.15 ms
(want to optimize MSPE. It is important to set)59.5 517.15 ms
(the probabilities for each object to be sampled to)59.5 504.15 ms
(the weights we've calculated. The size of the new data set is up)59.5 491.15 ms
( to you. You can sample for example, twice as many)59.5 478.15 ms
(objects as it was in original train set. And note that we do not)59.5 465.15 ms
( need to)59.5 452.15 ms
(do anything with the test set. It stays as is. I would also advi)59.5 439.15 ms
(se you to)59.5 426.15 ms
(re-sample train set several times. Each time fitting a model. An)59.5 413.15 ms
(d then average models predictions,)59.5 400.15 ms
(if we'll get the score much better and more stable. The results )59.5 387.15 ms
(will,)59.5 374.15 ms
(another way we can optimize MSPE, this approach was widely used )59.5 361.15 ms
(during)59.5 348.15 ms
(Rossmund Competition on Kagle. It can be proved that if)59.5 335.15 ms
(the errors are small, we can optimize the predictions)59.5 322.15 ms
(in logarithmic scale. Where it is similar to what we will)59.5 309.15 ms
(do on the next slide actually. We will not go into details but y)59.5 296.15 ms
(ou can find a link to explanation)59.5 283.15 ms
(in the reading materials. And finally, let's get to the last)59.5 270.15 ms
(regression metric we have to discuss. Root, mean, square, logari)59.5 257.15 ms
(thmic error. It turns out quite easy to optimize,)59.5 244.15 ms
(because of the connection with MSE loss. All we need to do is fi)59.5 231.15 ms
(rst to apply and)59.5 218.15 ms
(transform to our target variables. In this case,)59.5 205.15 ms
(logarithm of the target plus one. Let's denote the transformed t)59.5 192.15 ms
(arget)59.5 179.15 ms
(with a z variable right now. And then, we need to fit a model)59.5 166.15 ms
(with MSE loss to transform target. To get a prediction for a tes)59.5 153.15 ms
(t subject,)59.5 140.15 ms
(we first obtain the prediction, z hat, in the logarithmic scale )59.5 127.15 ms
(just by calling)59.5 114.15 ms
(model.predict or something like that. And next, we do an inverse)59.5 101.15 ms
( transform from)59.5 88.15 ms
(logarithmic scale back to the original by expatiating z hat and)59.5 75.15 ms
(subtracting one, and this is how we obtain the predictions)59.5 62.15 ms
(y hat for the test set. In this video, we run through regression)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 19 19
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 19)59.5 790.15 ms
F0 sf
(matrix and tools to optimize them. MSE and MAE are very common a)59.5 764.15 ms
(nd)59.5 751.15 ms
(implemented in many packages. RMSPE and MAPE can be optimized by)59.5 738.15 ms
(either resampling the data set or setting proper sample weights.)59.5 725.15 ms
( RMSLE is optimized by)59.5 712.15 ms
(optimizing MSE in log space. In the next video,)59.5 699.15 ms
(we will see optimization techniques for classification matrix. [)59.5 686.15 ms
(MUSIC][MUSIC] In this and the next video,)59.5 673.15 ms
(we will discuss, what are the ways to optimize)59.5 660.15 ms
(classification metrics? In this video,)59.5 647.15 ms
(we will discuss logloss and accuracy, and in the next one, AUC a)59.5 634.15 ms
(nd)59.5 621.15 ms
(quadratic-weighted kappa. Let's start with logloss, logloss for )59.5 608.15 ms
(classification is like MSE for)59.5 595.15 ms
(aggression, it is implemented everywhere. All we need to do is t)59.5 582.15 ms
(o find out what)59.5 569.15 ms
(arguments should be passed to a library to make it use logloss f)59.5 556.15 ms
(or training. There are a huge number of libraries)59.5 543.15 ms
(to try, like XGBoost, LightGBM, Logistic Regression, and [INAUDI)59.5 530.15 ms
(BLE])59.5 517.15 ms
(classifier from sklearn, Vowpal Wabbit. All neural nets, by defa)59.5 504.15 ms
(ult,)59.5 491.15 ms
(optimize logloss for classification. Random forest classifier pr)59.5 478.15 ms
(edictions turn)59.5 465.15 ms
(out to be quite bad in terms of logloss. But there is a way to m)59.5 452.15 ms
(ake them better, we can calibrate the predictions)59.5 439.15 ms
(to better fit logloss. We've mentioned several times that)59.5 426.15 ms
(logloss requires model to output exterior probabilities,)59.5 413.15 ms
(but what does it mean? It actually means that if we take all the)59.5 400.15 ms
(points that have a score of, for example, 0.8, then there will b)59.5 387.15 ms
(e exactly four times)59.5 374.15 ms
(more positive objects than negatives. That is, 80% of the points)59.5 361.15 ms
( will be)59.5 348.15 ms
(from class 1, and 20% from class 0. If the classifier doesn't)59.5 335.15 ms
(directly optimize logloss, its predictions should be calibrated.)59.5 322.15 ms
( Take a look at this plot, the blue line)59.5 309.15 ms
(shows sorted by value predictions for the validation set. And th)59.5 296.15 ms
(e red line shows correspondent)59.5 283.15 ms
(target values smoothed with rolling window. We clearly see that )59.5 270.15 ms
(our predictions)59.5 257.15 ms
(are kind of conservative. Theyre much greater than two)59.5 244.15 ms
(target mean on the left side, and much lower than they should)59.5 231.15 ms
(be on the right side. So this classifier is not calibrated, and )59.5 218.15 ms
(the green curve shows)59.5 205.15 ms
(the predictions after calibration. But if we plot sorted predict)59.5 192.15 ms
(ions for calibrated classifier, the curve will)59.5 179.15 ms
(be very similar to target rolling mean. And in fact, the calibra)59.5 166.15 ms
(tor)59.5 153.15 ms
(predictions will have lower log loss. Now, there are several way)59.5 140.15 ms
(s to)59.5 127.15 ms
(calibrate predictions, for example, we can use so-called Platt s)59.5 114.15 ms
(caling. Basically, we just need to fit a logistic)59.5 101.15 ms
(regression to our predictions. I will not go into the details ho)59.5 88.15 ms
(w to do)59.5 75.15 ms
(that, but it's very similar to how we stack models, and we will )59.5 62.15 ms
(discuss)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 20 20
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 20)59.5 790.15 ms
F0 sf
(stacking in detail in a different video. Second, we can fit isot)59.5 764.15 ms
(onic)59.5 751.15 ms
(regression to our predictions, and again, it is done very simila)59.5 738.15 ms
(r)59.5 725.15 ms
(to stacking, just another model. While finally, we can use stack)59.5 712.15 ms
(ing, so the idea is, we can fit any classifier. It doesn't need )59.5 699.15 ms
(to optimize logloss,)59.5 686.15 ms
(it just needs to be good, for example, in terms of AUC. And then)59.5 673.15 ms
( we can fit another model on top that will take the predictions )59.5 660.15 ms
(of our)59.5 647.15 ms
(model, and calibrate them properly. And that model on top will u)59.5 634.15 ms
(se)59.5 621.15 ms
(logloss as its optimization loss. So it will be optimizing indir)59.5 608.15 ms
(ectly,)59.5 595.15 ms
(and its predictions will be calibrated. Logloss was the only met)59.5 582.15 ms
(ric that)59.5 569.15 ms
(is easy to optimize directly. With accuracy, there is no easy)59.5 556.15 ms
(recipe how to directly optimize it. In general, the recipe is fo)59.5 543.15 ms
(llowing,)59.5 530.15 ms
(actually, if it is a binary classification task, fit any metric,)59.5 517.15 ms
( and)59.5 504.15 ms
(tune with the binarization threshold. For multi-class tasks, fit)59.5 491.15 ms
( any metric and tune parameters comparing)59.5 478.15 ms
(the models by their accuracy score, not by the metric that the m)59.5 465.15 ms
(odels)59.5 452.15 ms
(were really optimizing. So this is kind of early stopping and th)59.5 439.15 ms
(e cross validation,)59.5 426.15 ms
(where you look at the accuracy score. Just to get an intuition w)59.5 413.15 ms
(hy accuracy is)59.5 400.15 ms
(hard to optimize, let's look at this plot. So on the vertical ax)59.5 387.15 ms
(is we)59.5 374.15 ms
(will show the loss, and the horizontal axis shows signed distanc)59.5 361.15 ms
(e)59.5 348.15 ms
(to the decision boundary, for example, to a hyper plane or for a)59.5 335.15 ms
( linear model. The distance is considered to be positive)59.5 322.15 ms
(if the class is predicted correctly. And negative if the object )59.5 309.15 ms
(is located at)59.5 296.15 ms
(the wrong side of the decision boundary. The blue line here show)59.5 283.15 ms
(s zero-one loss, this is the loss that)59.5 270.15 ms
(corresponds to accuracy score. We pay 1 if the object is misclas)59.5 257.15 ms
(sified,)59.5 244.15 ms
(that is, the object has negative distance,)59.5 231.15 ms
(and we pay nothing otherwise. The problem is that, this loss has)59.5 218.15 ms
(zero almost everywhere gradient, with respect to the predictions)59.5 205.15 ms
(. And most learning algorithms require)59.5 192.15 ms
(a nonzero gradient to fit, otherwise it's not clear how we need )59.5 179.15 ms
(to change the)59.5 166.15 ms
(predictions such that loss is decreased. And so people came up w)59.5 153.15 ms
(ith proxy losses that are upper bounds for)59.5 140.15 ms
(these zero-one loss. So if you perfectly fit the proxy loss,)59.5 127.15 ms
(the accuracy will be perfect too, but differently to zero-one lo)59.5 114.15 ms
(ss,)59.5 101.15 ms
(they are differentiable. For example, you see here logistic loss)59.5 88.15 ms
(,)59.5 75.15 ms
(the red curve used in logistic regression, and)59.5 62.15 ms
(hinge loss, loss used in SVM. Now recall that to obtain hard lab)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 21 21
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 21)59.5 790.15 ms
F0 sf
(els for)59.5 764.15 ms
(a test object, we usually take argmax of our soft predictions,)59.5 751.15 ms
(picking the class with a maximum score. If our task is binary an)59.5 738.15 ms
(d)59.5 725.15 ms
(soft predictions sum up to 1, argmax is equivalent)59.5 712.15 ms
(to threshold function. Output 1 when the predictions for)59.5 699.15 ms
(the class one is higher than 0.5, and output 0 when the predicti)59.5 686.15 ms
(on's lower. So we've already seen this example)59.5 673.15 ms
(where threshold 0.5 is not optimal, so what can we do? We can tu)59.5 660.15 ms
(ne the threshold we apply, we can do it with a simple grid)59.5 647.15 ms
(search implemented with a for loop. Well, it means that we can b)59.5 634.15 ms
(asically)59.5 621.15 ms
(fit any sufficiently powerful model. It will not matter much wha)59.5 608.15 ms
(t loss exactly,)59.5 595.15 ms
(say, hinge or log loss the model will optimize. All we want from)59.5 582.15 ms
( our)59.5 569.15 ms
(model's predictions is the existence of a good threshold)59.5 556.15 ms
(that will separate the classes. Also, if our classifier)59.5 543.15 ms
(is ideally calibrated, then it is really returning)59.5 530.15 ms
(posterior probabilities. And for such a classifier,)59.5 517.15 ms
(threshold 0.5 would be optimal, but such classifiers are rarely )59.5 504.15 ms
(the case,)59.5 491.15 ms
(and threshold tuning helps often. So in this video, we discussed)59.5 478.15 ms
( logloss and accuracy, in the next video)59.5 465.15 ms
(we will discuss AUC and quadratic weighted kappa. [MUSIC]So in t)59.5 452.15 ms
(he previous video, we've discussed Logloss and Accuracy. In this)59.5 439.15 ms
( video we'll discuss Area Under Curve, AUC, and \(Quadratic weigh)59.5 426.15 ms
(ted\) Kappa. Let's start with AUC. Although the loss function of )59.5 413.15 ms
(AUC has zero gradients almost everywhere, exactly as accuracy lo)59.5 400.15 ms
(ss, there exists an algorithm to optimize AUC with gradient-base)59.5 387.15 ms
(d methods, and some models implement this algorithm. So we can u)59.5 374.15 ms
(se it by setting the right parameters. I will give you an idea a)59.5 361.15 ms
(bout this method without much details as there is more than one )59.5 348.15 ms
(way to implement it. Recall that originally, classification task)59.5 335.15 ms
( is usually solved at the level of objects. We want to assign 0 )59.5 322.15 ms
(to red objects, and 1 to the green ones. But we do it independen)59.5 309.15 ms
(tly for each object, and so our loss is pointwise. We compute it)59.5 296.15 ms
( for each object individually, and sum or average the losses for)59.5 283.15 ms
( all the objects to get a total loss. Now, recall that AUC is th)59.5 270.15 ms
(e probability of a pair of the objects to be ordered in the righ)59.5 257.15 ms
(t way. So ideally, we want predictions Y^ for the green objects )59.5 244.15 ms
(to be larger than for the red ones. So, instead of working with )59.5 231.15 ms
(single objects, we should work with pairs of objects. And instea)59.5 218.15 ms
(d of using pointwise loss, we should use pairwise loss. A pairwi)59.5 205.15 ms
(se loss takes predictions and labels for a pair of objects and c)59.5 192.15 ms
(omputes their loss. Ideally, the loss would be zero when the ord)59.5 179.15 ms
(ering is correct, and greater than zero when the ordering is not)59.5 166.15 ms
( correct, incorrect. But in practice, different loss functions c)59.5 153.15 ms
(an be used. For example, we can use logloss. We may think that t)59.5 140.15 ms
(he target for this pairwise loss is always one, red minus green )59.5 127.15 ms
(should be one. That is why there is only one term in logloss obj)59.5 114.15 ms
(ective instead of two. The prob function in the formula is neede)59.5 101.15 ms
(d to make sure that the difference between the predictions is st)59.5 88.15 ms
(ill in the 0,1 range, and I use it here just for the sake of sim)59.5 75.15 ms
(plicity. Well, basically, XGBoost, LightGBM have pairwise loss w)59.5 62.15 ms
(e've discussed implemented. It is straightforward to implement i)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 22 22
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 22)59.5 790.15 ms
F0 sf
(n any neural net library, and for sure, you can find implementat)59.5 764.15 ms
(ions on GitHub. I should say that in practice, most people still)59.5 751.15 ms
( use logloss as an optimization loss without any more post proce)59.5 738.15 ms
(ssing. I personally observed XGBoost learned with loglosst to gi)59.5 725.15 ms
(ve comparable AUC score to the one learned with pairwise loss. A)59.5 712.15 ms
(ll right. Now, let's move to the last topic to discuss. It is Qu)59.5 699.15 ms
(adratic weighted Kappa metric. There are two methods. One is ver)59.5 686.15 ms
(y common and very easy, the second is not that common and will r)59.5 673.15 ms
(equire you to implement a custom loss function for either XGBoos)59.5 660.15 ms
(t or neural net. But we've already implemented it for XGBoost, s)59.5 647.15 ms
(o you will be able to find the implementation in the reading mat)59.5 634.15 ms
(erials. But let's start with the simple one. Recall that we're s)59.5 621.15 ms
(olving an ordered classification problem and our labels can be f)59.5 608.15 ms
(ound of us integer ratings, say from one to five. The task is cl)59.5 595.15 ms
(assification as we cannot output, for example, 4.5 as an answer.)59.5 582.15 ms
( But anyway, we can treat it as a regression problem, and then s)59.5 569.15 ms
(omehow, post-process the predictions and convert them to integer)59.5 556.15 ms
( ratings. And actually quadratic weights make Kappa as somehow s)59.5 543.15 ms
(imilar to regression with MSE loss. If we allow our predictions )59.5 530.15 ms
(to take values between the labels, that is relax the predictions)59.5 517.15 ms
(. But in fact, it is different to MSE. So if relaxed, Kappa woul)59.5 504.15 ms
(d be one minus MSE divided by something that really depends on t)59.5 491.15 ms
(he predictions. And it looks like everyone's logic is, well, the)59.5 478.15 ms
(re is MSE in the denominator, we can optimize it, and let's don')59.5 465.15 ms
(t care about denominator. Well, of course it's not correct way t)59.5 452.15 ms
(o do it, but it turns out to be useful in practice. But anyway, )59.5 439.15 ms
(MSE gives us flat values instead of integers. So now, we need so)59.5 426.15 ms
(mehow to convert them into integers. And the straightforward way)59.5 413.15 ms
( would be to do rounding all the predictions. But we can think a)59.5 400.15 ms
(bout rounding as of applying a threshold. Like if the value is g)59.5 387.15 ms
(reater than 3.5 and less than 4.5, then output 3. But then we ca)59.5 374.15 ms
(n ask ourselves a question, why do we use exactly those threshol)59.5 361.15 ms
(ds? Let's tune them. And again, it's just straightforward, it ca)59.5 348.15 ms
(n be easily done with grid search. So to summarize, we need to f)59.5 335.15 ms
(it MSE loss to our data and then find appropriate thresholds. Fi)59.5 322.15 ms
(nally, there is a paper which suggests a way to relax classifica)59.5 309.15 ms
(tion problem to regression, but it deals with this- hard to deal)59.5 296.15 ms
( with part in denominator that we had. I will not get into the d)59.5 283.15 ms
(etails here, but it's clearly written and easy to understand pap)59.5 270.15 ms
(er, so I really encourage you to read it. And more, you can find)59.5 257.15 ms
( loss implementation in the reading materials, and just use it i)59.5 244.15 ms
(f you don't want to read the paper. Finally, we finished this le)59.5 231.15 ms
(sson. We've discussed that evaluation or target metric is how al)59.5 218.15 ms
(l submissions are scored. We've discussed the difference between)59.5 205.15 ms
( target metric and optimization loss. Optimization loss is what )59.5 192.15 ms
(our model optimizes, and it is not always the same as target met)59.5 179.15 ms
(ric that we want to optimize. Sometimes, we only can set our mod)59.5 166.15 ms
(el to optimize completely different to target metric. But later,)59.5 153.15 ms
( we usually try to post-process the predictions to make them bet)59.5 140.15 ms
(ter fit target metric. We've discussed intuition behind differen)59.5 127.15 ms
(t metrics for regression and classification tasks, and saw how t)59.5 114.15 ms
(o efficiently optimize different metrics. I hope you've enjoyed )59.5 101.15 ms
(this lesson, and see you later.[MUSIC] Hi, everyone. In this sec)59.5 88.15 ms
(tion, we'll cover a very)59.5 75.15 ms
(powerful technique, mean encoding. It actually has a number of n)59.5 62.15 ms
(ames. Some call it likelihood encoding,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 23 23
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 23)59.5 790.15 ms
F0 sf
(some target encoding, but in this course,)59.5 764.15 ms
(we'll stick with plain mean encoding. The general idea of this)59.5 751.15 ms
(technique is to add new variables based on some)59.5 738.15 ms
(feature to get where we started,. In simplest case, we encode ea)59.5 725.15 ms
(ch)59.5 712.15 ms
(level of categorical variable with corresponding target mean. Le)59.5 699.15 ms
(t's take a look at)59.5 686.15 ms
(the following example. Here, we have some binary)59.5 673.15 ms
(classification task in which we have a categorical variable, som)59.5 660.15 ms
(e city. And of course,)59.5 647.15 ms
(we want to numerically encode it. The most obvious way and)59.5 634.15 ms
(what people usually use is label encoding. It's what we have in )59.5 621.15 ms
(second column. Mean encoding is done differently, via encoding e)59.5 608.15 ms
(very city with)59.5 595.15 ms
(corresponding mean target. For example, for Moscow, we have)59.5 582.15 ms
(five rows with three 0s and two 1s. So we encode it with 2 divid)59.5 569.15 ms
(ed by 5 or)59.5 556.15 ms
(0.4. Similarly, we deal with the rest)59.5 543.15 ms
(of cities, pretty straightforward. What I've described here)59.5 530.15 ms
(is a very high level idea. There are a huge number of pitfalls o)59.5 517.15 ms
(ne)59.5 504.15 ms
(should overcome in actual competition. We went deep into details)59.5 491.15 ms
( for)59.5 478.15 ms
(now, just keep it in mind. At first, let me explain. Why does it)59.5 465.15 ms
( even work? Imagine, that our dataset is much bigger)59.5 452.15 ms
(and contains hundreds of different cities. Well, let's try to co)59.5 439.15 ms
(mpare,)59.5 426.15 ms
(of course, very abstractly, mean encoding with label encoding. W)59.5 413.15 ms
(e plot future histograms for)59.5 400.15 ms
(class 0 and class 1. In case of label encoding,)59.5 387.15 ms
(we'll always get total and random picture because)59.5 374.15 ms
(there's no logical order, but when we use mean target to encode )59.5 361.15 ms
(the)59.5 348.15 ms
(feature, classes look way more separable. The plot looks kind of)59.5 335.15 ms
( sorted. It turns out that this sorting quality)59.5 322.15 ms
(of mean encoding is quite helpful. Remember, what is the most po)59.5 309.15 ms
(pular and effective way to solve)59.5 296.15 ms
(machine learning problem? Is grading using trees, [INAUDIBLE] OI)59.5 283.15 ms
(GBM. One of the few downsides is)59.5 270.15 ms
(an inability to handle high cardinality categorical variables. T)59.5 257.15 ms
(rees have limited depth,)59.5 244.15 ms
(with mean encoding, we can compensate it, we can reach better lo)59.5 231.15 ms
(ss)59.5 218.15 ms
(with shorter trees. Cross validation loss)59.5 205.15 ms
(might even look like this. In general, the more complicated and)59.5 192.15 ms
(non linear feature target dependency, the more effective is mean)59.5 179.15 ms
( encoding, okay. Further in this section, you will)59.5 166.15 ms
(learn how to construct mean encodings. There are actually a lot )59.5 153.15 ms
(of ways. Also keep in mind that we use)59.5 140.15 ms
(classification tests only as an example. We can use mathematics)59.5 127.15 ms
(on other tests as well. The main idea remains the same. Despite )59.5 114.15 ms
(the simplicity of the idea, you)59.5 101.15 ms
(need to be very careful with validation. It's got to be impeccab)59.5 88.15 ms
(le. It's probably the most important part. Understanding the cor)59.5 75.15 ms
(rect linkless)59.5 62.15 ms
(validation is also a basis for staking. The last, but not least,)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 24 24
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 24)59.5 790.15 ms
F0 sf
( are extensions. There are countless possibilities to)59.5 764.15 ms
(derive new features from target variable. Sometimes, they produc)59.5 751.15 ms
(e significant)59.5 738.15 ms
(improvement for your models. Let's start with some)59.5 725.15 ms
(characteristics of data sets, that indicate the usefulness)59.5 712.15 ms
(of main encoding. The presence of categorical)59.5 699.15 ms
(variables with a lot of levels is already a good indicator, but)59.5 686.15 ms
(we need to go a little deeper. Let's take a look at each of thes)59.5 673.15 ms
(e)59.5 660.15 ms
(learning logs from Springleaf competition. I ran three models wi)59.5 647.15 ms
(th different depths,)59.5 634.15 ms
(7, 9, and 11. Train logs are on the top plot. Validation logs ar)59.5 621.15 ms
(e on the bottom one. As you can see, with increasing the depths)59.5 608.15 ms
(of trees, our training care becomes better and better, nearly pe)59.5 595.15 ms
(rfect and)59.5 582.15 ms
(that's a normal part. But we don't actually over feed and)59.5 569.15 ms
(that's weird. Our validation score also increase,)59.5 556.15 ms
(it's a sign that trees need a huge number of splits to)59.5 543.15 ms
(extract information from some variables. And we can check it for)59.5 530.15 ms
( mortal dump. It turns out that some features have)59.5 517.15 ms
(a tremendous amount of split points, like 1200 or 1600 and that')59.5 504.15 ms
(s a lot. Our model tries to treat all)59.5 491.15 ms
(those categories differently and they are also very important fo)59.5 478.15 ms
(r)59.5 465.15 ms
(predicting the target. We can help our model via mean encodings.)59.5 452.15 ms
( There is a number of ways)59.5 439.15 ms
(to calculate encodings. The first one is the one)59.5 426.15 ms
(we've been discussing so far. Simply taking mean of target varia)59.5 413.15 ms
(ble. Another popular option is to take)59.5 400.15 ms
(initial logarithm of this value, it's called weight of evidence.)59.5 387.15 ms
( Or you can calculate all)59.5 374.15 ms
(of the numbers of ones. Or the difference between number)59.5 361.15 ms
(of ones and the number of zeros. All of these are variable optio)59.5 348.15 ms
(ns. Now, let's actually)59.5 335.15 ms
(construct the features. We will do it on sprinkled data set, sup)59.5 322.15 ms
(pose we've already separated)59.5 309.15 ms
(the data for train and validation, X_tr and X val data frames. T)59.5 296.15 ms
(hese called snippet shows how)59.5 283.15 ms
(to construct mean encoding for an arbitrary column and map it in)59.5 270.15 ms
(to)59.5 257.15 ms
(a new data frame, train new and val new. We simply do group by o)59.5 244.15 ms
(n that column and)59.5 231.15 ms
(use target as a map. Resulting commands were able [INAUDIBLE]. I)59.5 218.15 ms
(t is then mapped to tree and)59.5 205.15 ms
(validation data sets by a map operator. After we've repeated thi)59.5 192.15 ms
(s process for)59.5 179.15 ms
(every call, we can fit each of those)59.5 166.15 ms
(model on this new data. But something's definitely not right, af)59.5 153.15 ms
(ter several efforts training AOC)59.5 140.15 ms
(is nearly 1, while on validation, the score set rates around 0.5)59.5 127.15 ms
(5,)59.5 114.15 ms
(which is practically noise. It's a clear sign of terrible overfi)59.5 101.15 ms
(tting. I'll explain what happened)59.5 88.15 ms
(in a few moments. Right now, I want to point out that)59.5 75.15 ms
(at least we validated correctly. We separated train and validati)59.5 62.15 ms
(on, and used all the train)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 25 25
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 25)59.5 790.15 ms
F0 sf
(data to estimate mean encodings. If, for instance, we would have)59.5 764.15 ms
(estimated mean encodings before train validation split, then we )59.5 751.15 ms
(would)59.5 738.15 ms
(not notice such an overfitting. Now, let's figure out)59.5 725.15 ms
(the reason of overfitting. When they are categorized, it's prett)59.5 712.15 ms
(y)59.5 699.15 ms
(common to get results like in an example, target 0 in train and)59.5 686.15 ms
(target 1 in validation. Mean encodings turns into a perfect)59.5 673.15 ms
(feature for such categories. That's why we immediately get)59.5 660.15 ms
(very good scores on train and fail hardly on validation. So far,)59.5 647.15 ms
( we've grasped the concept of mean)59.5 634.15 ms
(encodings and walked through some trivial examples, that obvious)59.5 621.15 ms
(ly can not use)59.5 608.15 ms
(mean encodings like this in practice. We need to deal with overf)59.5 595.15 ms
(itting first,)59.5 582.15 ms
(we need some kind of regularization. And I will tell you about d)59.5 569.15 ms
(ifferent methods in the next video. [MUSIC][MUSIC] In previous v)59.5 556.15 ms
(ideo, we realized that)59.5 543.15 ms
(mean encodings cannot be used as is and requires some kind of re)59.5 530.15 ms
(gularization)59.5 517.15 ms
(on training part of data. Now, we'll carry out four different)59.5 504.15 ms
(methods of regularization, namely, doing a cross-validation loop)59.5 491.15 ms
(to construct mean encodings. Then, smoothing based on)59.5 478.15 ms
(the size of category. Then, adding random noise. And finally, ca)59.5 465.15 ms
(lculating expanding)59.5 452.15 ms
(mean on some parametrization of data. We will go through all of)59.5 439.15 ms
(these methods one by one. Let's start with CV loop regularizatio)59.5 426.15 ms
(n. It's a very intuitive and robust method. For a given data poi)59.5 413.15 ms
(nt, we don't want to)59.5 400.15 ms
(use target variable of that data point. So we separate the data )59.5 387.15 ms
(into)59.5 374.15 ms
(K-node intersecting subsets, or in other words, folds. To get me)59.5 361.15 ms
(an encoding value for)59.5 348.15 ms
(some subset, we don't use data points from that subset and estim)59.5 335.15 ms
(ate)59.5 322.15 ms
(the encoding only on the rest of subset. We iteratively walk thr)59.5 309.15 ms
(ough)59.5 296.15 ms
(all the data subsets. Usually, four or five folds)59.5 283.15 ms
(are enough to get decent results. You don't need to tune this nu)59.5 270.15 ms
(mber. It may seem that we have completely)59.5 257.15 ms
(avoided leakage from target variable. Unfortunately, it's not tr)59.5 244.15 ms
(ue. It will become apparent if we perform)59.5 231.15 ms
(leave one out scheme to separate the data. I'll return to it a l)59.5 218.15 ms
(ittle later, but first let's learn how to)59.5 205.15 ms
(apply this method in practice. Suppose that our training)59.5 192.15 ms
(data is in a DFTR data frame. We will add mean encoded features)59.5 179.15 ms
(into another train new data frame. In the outer loop,)59.5 166.15 ms
(we iterate through stratified K-fold iterator in order to separa)59.5 153.15 ms
(te)59.5 140.15 ms
(training data into chunks. X_tr is used to estimate the encoding)59.5 127.15 ms
(. X_val is used to apply)59.5 114.15 ms
(estimating encoding. After that,)59.5 101.15 ms
(we iterate through all the columns and map estimated encodings)59.5 88.15 ms
(to X_val data frame. At the end of the outer loop we fill)59.5 75.15 ms
(train new data frame with the result. Finally, some rare categor)59.5 62.15 ms
(ies may)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 26 26
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 26)59.5 790.15 ms
F0 sf
(be present only in a single fold. So we don't have the data to)59.5 764.15 ms
(estimate target mean for them. That's why we end up with some na)59.5 751.15 ms
(ns. We can fill them with global mean. As you can see,)59.5 738.15 ms
(the whole process is very simple. Now, let's return to)59.5 725.15 ms
(the question of whether we leak information about)59.5 712.15 ms
(target variable or not. Consider the following example. Here we )59.5 699.15 ms
(want to encode Moscow)59.5 686.15 ms
(via leave-one-out scheme. For the first row, we get 0.5,)59.5 673.15 ms
(because there are two 1s and two 0s in the rest of rows. Similar)59.5 660.15 ms
(ly, for)59.5 647.15 ms
(the second row we get 0.25 and so on. But look closely, all the )59.5 634.15 ms
(resulting and)59.5 621.15 ms
(the resulting features. It perfect splits the data,)59.5 608.15 ms
(rows with feature mean equal or greater than 0.5 have target 0 a)59.5 595.15 ms
(nd)59.5 582.15 ms
(the rest of rows has target 1. We didn't explicitly use target v)59.5 569.15 ms
(ariable,)59.5 556.15 ms
(but our encoding is biased. Furthermore, this effect remains val)59.5 543.15 ms
(id)59.5 530.15 ms
(even for the KFold scheme, just milder. So is this type of regul)59.5 517.15 ms
(arization useless? Definitely not. In practice,)59.5 504.15 ms
(if you have enough data and use four or five folds, the encoding)59.5 491.15 ms
(s will work)59.5 478.15 ms
(fine with this regularization strategy. Just be careful and)59.5 465.15 ms
(use correct validation. Another regularization)59.5 452.15 ms
(method is smoothing. It's based on the following idea. If catego)59.5 439.15 ms
(ry is big,)59.5 426.15 ms
(has a lot of data points, then we can trust this to [INAUDIBLE] )59.5 413.15 ms
(encoding, but)59.5 400.15 ms
(if category is rare it's the opposite. Formula on the slide uses)59.5 387.15 ms
( this idea. It has hyper parameter alpha that)59.5 374.15 ms
(controls the amount of regularization. When alpha is zero,)59.5 361.15 ms
(we have no regularization, and when alpha approaches infinity)59.5 348.15 ms
(everything turns into globalmean. In some sense alpha is equal t)59.5 335.15 ms
(o)59.5 322.15 ms
(the category size we can trust. It's also possible to use some o)59.5 309.15 ms
(ther)59.5 296.15 ms
(formula, basically anything that punishes encoding software cate)59.5 283.15 ms
(gories)59.5 270.15 ms
(can be considered smoothing. Smoothing obviously won't)59.5 257.15 ms
(work on its own but we can combine it with for)59.5 244.15 ms
(example, CD loop regularization. Another way to regularize encod)59.5 231.15 ms
(ence is to)59.5 218.15 ms
(add some noise without regularization. Meaning codings have bett)59.5 205.15 ms
(er quality for the [INAUDIBLE] data than for)59.5 192.15 ms
(the test data. And by adding noise, we simply degrade)59.5 179.15 ms
(the quality of encoding on training data. This method is pretty )59.5 166.15 ms
(unstable,)59.5 153.15 ms
(it's hard to make it work. The main problem is the amount)59.5 140.15 ms
(of noise we need to add. Too much noise will turn)59.5 127.15 ms
(the feature into garbage, while too little noise)59.5 114.15 ms
(means worse regularization. This method is usually used together)59.5 101.15 ms
(with leave one out regularization. You need to diligently fine t)59.5 88.15 ms
(une it. So, it's probably not the best option)59.5 75.15 ms
(if you don't have a lot of time. The last regularization method )59.5 62.15 ms
(I'm going)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 27 27
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 27)59.5 790.15 ms
F0 sf
(to cover is based on expanding mean. The idea is very simple. We)59.5 764.15 ms
( fix some sorting order of our data and use only rows from zero )59.5 751.15 ms
(to n minus)59.5 738.15 ms
(one to calculate encoding for row n. You can check simple implem)59.5 725.15 ms
(entation)59.5 712.15 ms
(in the code snippet. Cumsum stores cumulative sum)59.5 699.15 ms
(of target variable up to the given row and)59.5 686.15 ms
(cumcnt stores cumulative count. This method introduces the least)59.5 673.15 ms
( amount)59.5 660.15 ms
(of leakage from target variable and it requires no hyper paramet)59.5 647.15 ms
(er tuning. The only downside is that)59.5 634.15 ms
(feature quality is not uniform. But it's not a big deal. We can )59.5 621.15 ms
(average models on encodings calculated from)59.5 608.15 ms
(different data permutations. It's also worth noting that)59.5 595.15 ms
(it is expanding mean method that is used in CatBoost grading,)59.5 582.15 ms
(boosting to it's library, which proves to perform magnificently)59.5 569.15 ms
(on data sets with categorical features. Okay, let's summarize wh)59.5 556.15 ms
(at)59.5 543.15 ms
(we've discussed in this video. We covered four different)59.5 530.15 ms
(types of regularization. Each of them has its own advantages and)59.5 517.15 ms
(disadvantages. Sometimes unintuitively we)59.5 504.15 ms
(introduce target variable leakage. But in practice, we can bear )59.5 491.15 ms
(with it. Personally, I recommend CV loop or)59.5 478.15 ms
(expanding mean methods for practical tasks. They are the most ro)59.5 465.15 ms
(bust and easy to tune. This is was regularization. In the next v)59.5 452.15 ms
(ideo, I will tell)59.5 439.15 ms
(you about various extensions and practical applications of mean )59.5 426.15 ms
(encodings. Thank you. [MUSIC][SOUND] In the final video,)59.5 413.15 ms
(we will cover various generalizations and extensions of mean enc)59.5 400.15 ms
(odings. Namely how to do meaning coding in)59.5 387.15 ms
(regression and multiclass tasks. How can we apply encoding to do)59.5 374.15 ms
(mains)59.5 361.15 ms
(with many-to-many relations. What features can we build based on)59.5 348.15 ms
(target we're able in time series. And finally, how to encode num)59.5 335.15 ms
(erical)59.5 322.15 ms
(features and interactions of features. Let's start with regressi)59.5 309.15 ms
(on tasks. They are actually more flexible for)59.5 296.15 ms
(feature encoding. Unlike binary classification where)59.5 283.15 ms
(a mean is frankly the only meaningful statistic we can extract)59.5 270.15 ms
(from target variable. In regression tasks, we can try)59.5 257.15 ms
(a variety of statistics, like medium, percentile, standard)59.5 244.15 ms
(deviation of target variable. We can even calculate)59.5 231.15 ms
(some distribution bins. For example, if target variable)59.5 218.15 ms
(is distributed between 1 and 100, we can create 10 bin features.)59.5 205.15 ms
( In the first feature, we'll count how many)59.5 192.15 ms
(data points have targeted between 1 and 10, in the second betwee)59.5 179.15 ms
(n 10 and)59.5 166.15 ms
(20 and so on. Of course,)59.5 153.15 ms
(we need to realize all of these features. In a nutshell,)59.5 140.15 ms
(regression tasks are like classification. Just more flexible in )59.5 127.15 ms
(terms)59.5 114.15 ms
(of feature engineering. Men encoding for multi-class tasks)59.5 101.15 ms
(is also pretty straightforward. For every feature we want to enc)59.5 88.15 ms
(ode, we will have n different encodings)59.5 75.15 ms
(where n is the number of classes. It actually has non obvious ad)59.5 62.15 ms
(vantage. Three models for example, usually solve multi-class)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 28 28
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 28)59.5 790.15 ms
F0 sf
(task in one versus old fashion. So every class had a different m)59.5 764.15 ms
(odel, and when we feed that model, it doesn't)59.5 751.15 ms
(have any information about structure of other classes because th)59.5 738.15 ms
(ey)59.5 725.15 ms
(are merge into one entity. Therefore, together with mean encodin)59.5 712.15 ms
(gs, we introduce some additional information)59.5 699.15 ms
(about the structure of other classes. The domains with many-to-m)59.5 686.15 ms
(any)59.5 673.15 ms
(relations are usually very complex and require special approache)59.5 660.15 ms
(s)59.5 647.15 ms
(to create mean encodings. I will give you only a very high)59.5 634.15 ms
(level idea, consider an example. Binary classification task for )59.5 621.15 ms
(users based)59.5 608.15 ms
(on apps installed on their smartphones. Each user may have multi)59.5 595.15 ms
(ple apps and)59.5 582.15 ms
(each app is used by multiple users. Hence, many-to-many relation)59.5 569.15 ms
(. We want to mean encode apps. The hard part we need to deal wit)59.5 556.15 ms
(h is)59.5 543.15 ms
(that the user may have a lot of apps. So let's take a cross prod)59.5 530.15 ms
(uct of user and)59.5 517.15 ms
(app entities. It will result in a so)59.5 504.15 ms
(called long representation of data. We will have a role for)59.5 491.15 ms
(each user app pair. Using this table, we can naturally)59.5 478.15 ms
(calculate mean encoding for apps. So now every app is encoded wi)59.5 465.15 ms
(th target)59.5 452.15 ms
(mean, but how to map it back to users. Every user has a number o)59.5 439.15 ms
(f apps, so instead of app1, app2, app3, we will now have a vecto)59.5 426.15 ms
(r like 0.1,)59.5 413.15 ms
(0.2, 0.1. That was pretty simple. We can collect various statist)59.5 400.15 ms
(ics)59.5 387.15 ms
(from those vectors, like mean, minimal, maximum,)59.5 374.15 ms
(standard deviation, and so on. So far we assume that our data)59.5 361.15 ms
(has no inner structure, but with time series we can obviously)59.5 348.15 ms
(use future information. On one hand, it's a limitation, on the o)59.5 335.15 ms
(ther hand, it actually allows)59.5 322.15 ms
(us to make some complicated features. In data sets without time )59.5 309.15 ms
(component)59.5 296.15 ms
(when encoding the category, we are forced to use all the rules)59.5 283.15 ms
(to calculate the statistic. It makes no sense to choose)59.5 270.15 ms
(some subset of rules. Presence of time changes it. For a given c)59.5 257.15 ms
(ategory, we can't. For example, calculate the mean from)59.5 244.15 ms
(previous day, previous two days, previous week, etc. Consider an)59.5 231.15 ms
( example. We need to predict which)59.5 218.15 ms
(categories users spends money. In these two example we have)59.5 205.15 ms
(a period of two days, two users, and three spending categories. )59.5 192.15 ms
(Some good features would be)59.5 179.15 ms
(the total amount of money users spent in previous day. An averag)59.5 166.15 ms
(e amount of money spent)59.5 153.15 ms
(by all users in given category. So, in day 1, user 101 spends $6)59.5 140.15 ms
(, user 102, $3. Therefore, we feel those numbers)59.5 127.15 ms
(as future values for day 2. Similarly, with the average)59.5 114.15 ms
(amount by category. The more data we have, the more)59.5 101.15 ms
(complicated features we can create. In practice, it is often bee)59.5 88.15 ms
(n official)59.5 75.15 ms
(to mean encode numeric features and some combination of features)59.5 62.15 ms
(. To encode a numeric feature, we only need)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 29 29
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 29)59.5 790.15 ms
F0 sf
(to bin it and then treat as categorical. Now, we need to answer )59.5 764.15 ms
(two questions. First, how to bin numeric feature, and second how)59.5 751.15 ms
( to select useful)59.5 738.15 ms
(combination of features. Well, we can find it out from a model)59.5 725.15 ms
(structure by analyzing the trees. So at first, we take for)59.5 712.15 ms
(example, [INAUDIBLE] model and raw features without any encoding)59.5 699.15 ms
(s. Let's start with numeric features. If numeric feature has a l)59.5 686.15 ms
(ot of)59.5 673.15 ms
([INAUDIBLE] points, it means that it has some complicated depend)59.5 660.15 ms
(ency with target)59.5 647.15 ms
(and its was trying to mean encode it. Furthermore, these exact s)59.5 634.15 ms
(plit points)59.5 621.15 ms
(may be used to bin the feature. So by analyzing model structure,)59.5 608.15 ms
( we both identify suspicious numeric)59.5 595.15 ms
(feature and found a good way to bin it. It's going to be a littl)59.5 582.15 ms
(e harder)59.5 569.15 ms
(with selecting interactions, but nothing extraordinary. First, l)59.5 556.15 ms
(et's define how to extract to)59.5 543.15 ms
(way interaction from decision tree. The process will be similar )59.5 530.15 ms
(for three way,)59.5 517.15 ms
(four way arbitrary way interactions. So two features interact in)59.5 504.15 ms
( a tree if)59.5 491.15 ms
(they are in two neighbouring notes. With that in mind, we can it)59.5 478.15 ms
(erate)59.5 465.15 ms
(through all the trees in the model and calculate how many times )59.5 452.15 ms
(each)59.5 439.15 ms
(feature interaction appeared. The most frequent interactions)59.5 426.15 ms
(are probably worthy of mean encoding. For example, if we found t)59.5 413.15 ms
(hat feature one)59.5 400.15 ms
(and feature two pair is most frequent, then we can concatenate t)59.5 387.15 ms
(hat)59.5 374.15 ms
(those feature values in our data. And mean encode resulting inte)59.5 361.15 ms
(raction. Now let me illustrate how important)59.5 348.15 ms
(interaction encoding may be. Amazon Employee Access Challenge)59.5 335.15 ms
(Competition has a very specific data set. There are only nine ca)59.5 322.15 ms
(tegorical features. If we blindly fit say like GBM)59.5 309.15 ms
(model on the raw features, then no matter how we)59.5 296.15 ms
(return the parameters, we'll score in a 0.87 AUC range. Which wi)59.5 283.15 ms
(ll place roughly on 700)59.5 270.15 ms
(position on the leaderboard. Furthermore, even if we mean encode)59.5 257.15 ms
( all)59.5 244.15 ms
(the labels, we won't have any progress. But if we fit cat boost )59.5 231.15 ms
(model,)59.5 218.15 ms
(which internally mean encodes some feature interactions,)59.5 205.15 ms
(we will immediately score in 0.91 range, which will place us)59.5 192.15 ms
(onto win this position. The difference in both)59.5 179.15 ms
(absolute AUC values and relative leaderboard)59.5 166.15 ms
(positions is tremendous. Also note that cat boost)59.5 153.15 ms
(is no silver bullet. In order to get even higher)59.5 140.15 ms
(on the leader board, would still need to manually add)59.5 127.15 ms
(more mean encoded interactions. In general, if you participate i)59.5 114.15 ms
(n)59.5 101.15 ms
(a competition with a lot of categorical variables, it's always w)59.5 88.15 ms
(orth trying to)59.5 75.15 ms
(work with interactions and mean encodings. I also want to remind)59.5 62.15 ms
( you about)59.5 49.15 ms
re sp
%%PageTrailer
%%Page: 30 30
%%BeginPageSetup
sv
0 g
F0 sf
%%EndPageSetup
F1 sf
(Week3_win_kaggle.txt                                     Page 30)59.5 790.15 ms
F0 sf
(correct validation process. During all local experiments, you sh)59.5 764.15 ms
(ould at first split data in X_tr and)59.5 751.15 ms
(X_val parts. Estimate encodings on X_tr,)59.5 738.15 ms
(map them to X_tr and X_val, and)59.5 725.15 ms
(then regularize them on X_tr and only after that validate your)59.5 712.15 ms
(model on X_tr / X_val split. Don't even think about estimating)59.5 699.15 ms
(encodings before splitting the data. And at submission stage, yo)59.5 686.15 ms
(u can)59.5 673.15 ms
(estimate encodings on whole train data. Map it to train and test)59.5 660.15 ms
(, then apply regularization on training)59.5 647.15 ms
(data and finally fit a model. And note that you should have alre)59.5 634.15 ms
(ady)59.5 621.15 ms
(decided on regularization method and its strength in local exper)59.5 608.15 ms
(iments. At the end of this section,)59.5 595.15 ms
(let's summarize main advantages and disadvantages of mean encodi)59.5 582.15 ms
(ngs. First of all, mean encoding allows us to make a compact)59.5 569.15 ms
(transformation of categorical variables. It is also a powerful b)59.5 556.15 ms
(asis for)59.5 543.15 ms
(feature engineering. Then the main disadvantage)59.5 530.15 ms
(is target rebel leakage. We need to be very careful with)59.5 517.15 ms
(validation and irregularization. It also works only on specific )59.5 504.15 ms
(data sets. It definitely won't help)59.5 491.15 ms
(in every competition. But keep in mind, when this method works,)59.5 478.15 ms
(it may produce significant improvements. Thank you for your atte)59.5 465.15 ms
(ntion. [MUSIC])59.5 452.15 ms
re sp
%%PageTrailer
%%Trailer
%%Pages: 30
%%EOF
